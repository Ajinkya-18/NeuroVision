{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ajinkya-18/NeuroVision/blob/main/eeg_text_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0tL4qnGzyo4"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/content/final_lightweight_17k', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k.tar.gz /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf /content/alljoined_lightweight_17k.tar.gz -C /content/final_lightweight_17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9157c041cbe4b6a9a9d5bf5413653d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Spectrogram Stats:   0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats calculated and saved to /content/final_lightweight_17k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Path to your lightweight dataset on the local Colab disk\n",
    "DATA_ROOT = '/content/final_lightweight_17k'\n",
    "SPECTROGRAM_TRAIN_DIR = Path(DATA_ROOT) / 'spectrograms' / 'train'\n",
    "\n",
    "# Initialize accumulators\n",
    "channel_sum = torch.zeros(64, dtype=torch.float64)\n",
    "channel_sum_sq = torch.zeros(64, dtype=torch.float64)\n",
    "pixel_count = 0\n",
    "\n",
    "# Calculate stats only on the training set\n",
    "files = list(SPECTROGRAM_TRAIN_DIR.glob('*.pt'))\n",
    "for path in tqdm(files, desc=\"Calculating Spectrogram Stats\"):\n",
    "    data = torch.load(path)\n",
    "    channel_sum += data.sum(dim=[1, 2]).to(torch.float64)\n",
    "    channel_sum_sq += (data.to(torch.float64) ** 2).sum(dim=[1, 2])\n",
    "    pixel_count += data.shape[1] * data.shape[2]\n",
    "\n",
    "mean = (channel_sum / pixel_count).to(torch.float32)\n",
    "std = torch.sqrt((channel_sum_sq / pixel_count) - mean.to(torch.float64)**2).to(torch.float32)\n",
    "\n",
    "# Save the stats to the dataset folder\n",
    "torch.save(mean, Path(DATA_ROOT) / 'spec_mean.pt')\n",
    "torch.save(std, Path(DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "print(f\"\\nStats calculated and saved to {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Installing and upgrading libraries...\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
      "Collecting lpips\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
      "Collecting open-clip-torch\n",
      "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.2)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
      "Collecting ftfy (from open-clip-torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.2.14)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ftfy, lpips, open-clip-torch\n",
      "Successfully installed ftfy-6.3.1 lpips-0.1.4 open-clip-torch-3.2.0\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1ve8co_3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1ve8co_3\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=24258a5ebe3e29aef5975dc239e1f2c73271d6a0161be392172b0bc515879946\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vqi60pji/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"â³ Installing and upgrading libraries...\")\n",
    "# The -q flag makes the output cleaner\n",
    "!pip install torch torchvision timm lpips transformers accelerate diffusers open-clip-torch\n",
    "!pip install peft ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMHcgjRIz1tD"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "def run_caption_generation():\n",
    "    \"\"\"A self-contained script to run only once.\"\"\"\n",
    "    config = CONFIG()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Loading BLIP model for caption generation...\")\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    caption_blip = Blip2ForConditionalGeneration.from_pretrained(config.BLIP_MODEL_NAME).to(device)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        captions_path = Path(config.PROCESSED_DATA_ROOT) / f'{split}_gt_captions.json'\n",
    "        if captions_path.exists():\n",
    "            print(f\"Captions for '{split}' split already exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating captions for '{split}' split...\")\n",
    "        df = pd.read_csv(config.METADATA_CSV)\n",
    "        split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "\n",
    "        captions = {}\n",
    "        batch_size = 16\n",
    "        caption_blip.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(split_df), batch_size), desc=f\"Generating {split} captions\"):\n",
    "                batch_info = split_df.iloc[i:i+batch_size]\n",
    "                batch_images = [\n",
    "                    Image.open(Path(config.PROCESSED_DATA_ROOT) / info['image_path']).convert(\"RGB\")\n",
    "                    for _, info in batch_info.iterrows()\n",
    "                ]\n",
    "\n",
    "                inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "                generated_ids = caption_blip.generate(**inputs, max_length=50)\n",
    "                batch_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                for j, caption in enumerate(batch_captions):\n",
    "                    captions[str(i + j)] = caption.strip()\n",
    "\n",
    "        with open(captions_path, 'w') as f:\n",
    "            json.dump(captions, f, indent=2)\n",
    "        print(f\"âœ… Saved {len(captions)} captions for '{split}' split.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_caption_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y fonts-liberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment'\n",
    "\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    BATCH_SIZE = 14\n",
    "    GRAD_ACCUMULATION_STEPS = 2\n",
    "    NUM_EPOCHS = 30\n",
    "    ADAPTER_LR = 1e-5  # Higher LR for the new adapter\n",
    "    BLIP_LR = 1e-6     # Lower LR for fine-tuning the pre-trained layers\n",
    "\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "    GRADIENT_CLIP_NORM = 1.0\n",
    "\n",
    "    VALIDATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 400\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 2000\n",
    "    VIS_GRID_SIZE = 5\n",
    "\n",
    "    DIFFUSION_MODEL_ID =\"runwayml/stable-diffusion-v1-5\"\n",
    "    EVAL_IMAGE_GENERATION_INTERVAL = 1\n",
    "    WARMUP_STEPS = 500  # Number of steps to gradually increase the LR\n",
    "    TOTAL_TRAIN_STEPS = (TRAIN_SAMPLES_PER_EPOCH/BATCH_SIZE) * NUM_EPOCHS\n",
    "    STAGE1_EPOCHS = 12\n",
    "    # STAGE2_EPOCHS = 18\n",
    "    ALIGN_WEIGHT = 1.0\n",
    "    SEMANTIC_WEIGHT = 2.0\n",
    "    CE_WEIGHT = 3.0\n",
    "    BEST_MODEL_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_103040/best_model_epoch_27_loss_5.0528.pth'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER MODULE ---\n",
    "# ==============================================================================\n",
    "\n",
    "class EEGTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A powerful hybrid CNN-Transformer encoder to extract rich features\n",
    "    directly from 64-channel EEG spectrograms.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=64, embed_dim=768, nhead=8, num_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Convolutional Stem: Extracts local spatio-temporal features\n",
    "        self.conv_stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim // 4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim // 4),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "        )\n",
    "\n",
    "        # 2. Transformer Encoder: Learns global relationships between features\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: [Batch, 64, Freq, Time]\n",
    "\n",
    "        # Pass through convolutional stem\n",
    "        x = self.conv_stem(x)\n",
    "        # Output x: [Batch, embed_dim, F/4, T/4]\n",
    "\n",
    "        # Prepare for transformer: flatten spatial dims and permute\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.flatten(2).permute(0, 2, 1) # -> [Batch, H*W, embed_dim]\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.output_layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE HYBRID EEG-BLIP2 MODEL ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    The final, refactored model that directly connects a powerful EEG encoder\n",
    "    to the BLIP-2 Q-Former via a projection layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, device, eeg_embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 1. Instantiate the powerful, from-scratch EEG encoder\n",
    "        self.eeg_encoder = EEGTransformerEncoder(embed_dim=eeg_embed_dim)\n",
    "\n",
    "        # 2. Load the pre-trained BLIP model\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # 3. Create the crucial projection layer\n",
    "        # This layer maps the EEG encoder's output dimension (e.g., 768) to the\n",
    "        # dimension the BLIP vision encoder's features, which the Q-Former expects (1408).\n",
    "        blip_vision_hidden_size = self.blip.vision_model.config.hidden_size\n",
    "        self.eeg_projection = nn.Linear(eeg_embed_dim, blip_vision_hidden_size)\n",
    "\n",
    "        # The __init__ method no longer handles freezing. This is now controlled by the training loop.\n",
    "\n",
    "        self.eeg_encoder.to(device)\n",
    "        self.eeg_projection.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        \"\"\" The direct EEG-to-Q-Former forward pass. \"\"\"\n",
    "        # 1. Get rich features from our custom EEG encoder\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "        # Output shape: [Batch, SeqLen, 768]\n",
    "\n",
    "        # 2. Project EEG features to match the Q-Former's expected input dimension\n",
    "        projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "        # Output shape: [Batch, SeqLen, 1408]\n",
    "\n",
    "        # 3. Pass projected features directly to the Q-Former\n",
    "        query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=projected_eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        # This function remains the same, using the standard image pathway\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # Follow the same direct pathway for generation\n",
    "            eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "            projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "\n",
    "            query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=projected_eeg_features,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs,\n",
    "                **kwargs\n",
    "            )\n",
    "            return generated_ids\n",
    "\n",
    "    def forward(self, eeg_spectrograms):\n",
    "        \"\"\"\n",
    "        Returns the EEG embedding and the raw logits from the language model.\n",
    "        \"\"\"\n",
    "        # 1. Get EEG features and project them\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "        projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "\n",
    "        # 2. Get the 32 query embeddings from the Q-Former\n",
    "        query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=projected_eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # 3. Project queries for the language model\n",
    "        language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "\n",
    "        # 4. Get the logits from the language model\n",
    "        outputs = self.blip.language_model(inputs_embeds=language_model_inputs, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the final EEG embedding for our alignment loss\n",
    "        eeg_embedding = query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return eeg_embedding, logits\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & COLLATING ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch):\n",
    "    spectrograms, pil_images, text_captions = zip(*[(item[0], item[1], item[2]) for item in batch])\n",
    "    labels = [item[3] for item in batch]\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return torch.stack(spectrograms), list(pil_images), list(text_captions), padded_labels\n",
    "\n",
    "\n",
    "class EEGDatasetWithCaptions(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, processor, transform, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "        # --- NEW: Define a dedicated augmentation pipeline ---\n",
    "        if self.augment:\n",
    "            self.augmentation_transform = transforms.Compose([\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "                # Randomly mask out frequency bands\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(0.1, 0.5))], p=0.5),\n",
    "                # Randomly mask out time steps\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(2.0, 5.0))], p=0.5),\n",
    "            ])\n",
    "\n",
    "        # This is your existing normalization transform\n",
    "        self.normalization_transform = transform\n",
    "\n",
    "        # Load pre-generated captions\n",
    "        captions_path = Path(root_dir) / f'{split}_gt_captions.json'\n",
    "        print(f\"Loading pre-generated captions from {captions_path}\")\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "\n",
    "        spectrogram = torch.load(self.root_dir / info['spectrogram_path'])\n",
    "\n",
    "        # --- APPLY AUGMENTATIONS (for training set only) ---\n",
    "        if self.augment:\n",
    "            spectrogram = self.augmentation_transform(spectrogram)\n",
    "\n",
    "        # Apply normalization after augmentation\n",
    "        spectrogram = self.normalization_transform(spectrogram)\n",
    "\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        gt_caption = self.captions.get(str(idx), \"an image\")\n",
    "        labels = self.processor(text=gt_caption, return_tensors=\"pt\", padding=True).input_ids.squeeze()\n",
    "\n",
    "        return spectrogram, image, gt_caption, labels\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN TRAINING FUNCTION ---\n",
    "# ==============================================================================\n",
    "# InfoNCE loss function\n",
    "\n",
    "def info_nce_loss(query, positive_key, temperature=0.07):\n",
    "    # Ensure inputs are normalized\n",
    "    query = F.normalize(query, dim=-1)\n",
    "    positive_key = F.normalize(positive_key, dim=-1)\n",
    "\n",
    "    # Calculate the similarity matrix of every query with every key\n",
    "    # The diagonal of this matrix contains the positive pairs\n",
    "    logits = query @ positive_key.T\n",
    "\n",
    "    # The labels are the indices of the positive pairs (the diagonal)\n",
    "    labels = torch.arange(len(query), device=query.device)\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    return F.cross_entropy(logits / temperature, labels)\n",
    "\n",
    "\n",
    "class SemanticLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A loss function that measures the semantic similarity between two sets of captions\n",
    "    using a pre-trained SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Load a pre-trained model optimized for semantic similarity\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "        # Freeze the model's weights as we only use it for inference\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, predicted_captions, ground_truth_captions):\n",
    "        # Convert the text captions into semantic embedding vectors\n",
    "        pred_embeddings = self.model.encode(predicted_captions, convert_to_tensor=True)\n",
    "        gt_embeddings = self.model.encode(ground_truth_captions, convert_to_tensor=True)\n",
    "\n",
    "        # Calculate the cosine similarity. The loss is 1.0 minus the similarity.\n",
    "        # A higher similarity (closer to 1.0) results in a lower loss (closer to 0.0).\n",
    "        cosine_sim = F.cosine_similarity(pred_embeddings, gt_embeddings, dim=-1)\n",
    "        loss = 1.0 - cosine_sim.mean()\n",
    "        return loss\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "def create_and_save_reconstruction_grid(eval_samples, diffusion_pipe, epoch, val_loss, output_dir):\n",
    "    \"\"\"\n",
    "    Generates a 2x2 image grid comparing original and reconstructed images.\n",
    "    Top row: Original images with ground truth captions.\n",
    "    Bottom row: Reconstructed images with predicted captions.\n",
    "    \"\"\"\n",
    "    if len(eval_samples) < 2:\n",
    "        print(\"Not enough samples for a 2x2 grid, skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ–¼ï¸  Generating reconstruction grid for epoch {epoch}...\")\n",
    "\n",
    "    # Setup canvas and font\n",
    "    w, h = 512, 512  # Standard Stable Diffusion size\n",
    "    title_h = 60    # Space for titles\n",
    "    grid = Image.new('RGB', (w * 2, h * 2 + title_h * 2), 'black')\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 20)\n",
    "    except IOError:\n",
    "        print(\"Default font not found, using fallback.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Process two samples\n",
    "    for i in range(2):\n",
    "        predicted_cap, gt_cap, original_image = eval_samples[i]\n",
    "\n",
    "        # --- Generate the reconstructed image from the predicted caption ---\n",
    "        generator = torch.Generator(device=diffusion_pipe.device).manual_seed(42 + i)\n",
    "        reconstructed_image = diffusion_pipe(\n",
    "            prompt=predicted_cap, generator=generator, num_inference_steps=20\n",
    "        ).images[0]\n",
    "\n",
    "        # --- Paste images and draw titles ---\n",
    "        col_offset = i * w\n",
    "\n",
    "        # Top Row (Original)\n",
    "        draw.text((col_offset + 5, 5), f\"GT: {gt_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(original_image.resize((w, h)), (col_offset, title_h))\n",
    "\n",
    "        # Bottom Row (Reconstructed)\n",
    "        draw.text((col_offset + 5, h + title_h + 5), f\"Pred: {predicted_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(reconstructed_image.resize((w, h)), (col_offset, h + title_h * 2))\n",
    "\n",
    "    # --- Save the final grid ---\n",
    "    # Sanitize loss value for filename\n",
    "    val_loss_str = f\"{val_loss:.4f}\".replace('.', '_')\n",
    "    save_path = Path(output_dir) / 'reconstructions'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    filename = save_path / f\"reconstruction_epoch_{epoch}_loss_{val_loss_str}.png\"\n",
    "    grid.save(filename)\n",
    "    print(f\"  Saved reconstruction grid to {filename}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "def train_model(config):\n",
    "    # --- Setup (Device, Output Dir, etc.) ---\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Models and Data ---\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    model = EEG_BLIP2_Model(config, device) # Assumes the refactored, cleaner version\n",
    "\n",
    "    for p in model.blip.parameters():\n",
    "        p.requires_grad=False\n",
    "\n",
    "    print(\"Starting training from scratch.\")\n",
    "    # try:\n",
    "    #   print('Loading best model weights')\n",
    "    #   model.load_state_dict(torch.load(config.BEST_MODEL_DIR, map_location=device))\n",
    "    #   print('Successfully loaded best model weights!')\n",
    "    # except:\n",
    "    #   print('Could not load the best model weights.')\n",
    "\n",
    "    print(\"Loading Stable Diffusion pipeline for visualizations...\")\n",
    "    diffusion_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        config.DIFFUSION_MODEL_ID,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        safety_checker=None, requires_safety_checker=False\n",
    "    ).to(device)\n",
    "    diffusion_pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    semantic_loss_fn = SemanticLoss(device)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    spec_std += 1e-6\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "\n",
    "    train_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', processor, transform, augment=True)\n",
    "    val_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', processor, transform, augment=False)\n",
    "\n",
    "    # --- Training Components ---\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 15, \"do_sample\": True, \"min_new_tokens\": 5,\n",
    "        \"repetition_penalty\": 1.4, \"top_p\": 0.9, \"temperature\": 0.85\n",
    "    }\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    print(\"ðŸŽ¯ Starting Decoupled 3-Stage Training Loop\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "\n",
    "        # --- STAGE 1 & 2 Freezing Logic ---\n",
    "        if epoch < config.STAGE1_EPOCHS:\n",
    "            if epoch == 0:\n",
    "                print(f\"\\n--- STAGE 1: Training EEG Encoder & Projection ONLY (Epochs 0-{config.STAGE1_EPOCHS-1}) ---\")\n",
    "\n",
    "            # Set requires_grad for Stage 1\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'eeg_encoder' in name or 'eeg_projection' in name:\n",
    "                    param.requires_grad = True\n",
    "                    # print('Unfreeing Encoder and eeg-projection layers')\n",
    "\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        elif epoch == config.STAGE1_EPOCHS:\n",
    "            print(f\"\\n--- STAGE 2: Fine-Tuning Q-Former and language-projection (Epochs {config.STAGE1_EPOCHS}+) ---\")\n",
    "\n",
    "            # Set requires_grad for Stage 2\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'qformer' in name or 'language_projection' in name:\n",
    "                    param.requires_grad = True\n",
    "                    # print('Unfreezing Q-Former layer')\n",
    "\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # elif epoch == config.STAGE2_EPOCHS:\n",
    "        #     print(f\"\\n---STAGE 3: Fine-tuning Language Projection (Epochs {config.STAGE2_EPOCHS}+)---\")\n",
    "\n",
    "        #     for name, param in model.named_parameters():\n",
    "        #         if 'qformer' in name or 'language_projection' in name:\n",
    "        #             param.requires_grad = True\n",
    "        #             # print('Unfreezing language-projection layer')\n",
    "\n",
    "        #         else:\n",
    "        #             param.requires_grad = False\n",
    "\n",
    "\n",
    "        # --- Create/Re-create Optimizer & Scheduler at stage transitions ---\n",
    "        if epoch == 0 or epoch == config.STAGE1_EPOCHS:\n",
    "            print(f\"Creating optimizer for new stage...\")\n",
    "\n",
    "            # Use differential learning rates for Stage 2 and 3\n",
    "            if epoch < config.STAGE1_EPOCHS:\n",
    "                trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "                optimizer = optim.AdamW(trainable_params, lr=config.ADAPTER_LR, weight_decay=config.WEIGHT_DECAY)\n",
    "            else: # For both Stage 2 and Stage 3\n",
    "                qformer_params = [p for n, p in model.named_parameters() if p.requires_grad and 'qformer' in n]\n",
    "                lang_proj_params = [p for n, p in model.named_parameters() if p.requires_grad and 'language_projection' in n]\n",
    "\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {\"params\": qformer_params, \"lr\": config.BLIP_LR},\n",
    "                    {\"params\": lang_proj_params, \"lr\": config.BLIP_LR * 0.5}, # Even smaller LR for the final layer\n",
    "                ]\n",
    "                optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "            num_training_steps = (len(train_dataset) // config.BATCH_SIZE) * config.NUM_EPOCHS\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=config.WARMUP_STEPS, num_training_steps=num_training_steps\n",
    "            )\n",
    "            print(\"Optimizer and scheduler created.\")\n",
    "\n",
    "        # --- Training Batch Loop ---\n",
    "        model.train()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
    "            sampler=SubsetRandomSampler(np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)),\n",
    "            drop_last=True, collate_fn=collate_fn)\n",
    "        train_bar = tqdm(train_loader, desc=f\"ðŸ”„ Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "\n",
    "        for batch_idx, (spectrograms, pil_images, gt_text_captions, labels) in enumerate(train_bar):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings, logits = model(spectrograms)\n",
    "\n",
    "                # For semantic loss, we need to generate captions (this is slower but necessary)\n",
    "                with torch.no_grad():\n",
    "                    image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "                    generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                    eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_text_captions)\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "\n",
    "                logits_len = logits.size(1)\n",
    "                labels_len = labels.size(1)\n",
    "\n",
    "                if labels_len < logits_len:\n",
    "                    padded_labels = F.pad(labels, (0, logits_len - labels_len), 'constant', -100)\n",
    "                else:\n",
    "                    # In case labels are longer (unlikely), truncate them\n",
    "                    padded_labels = labels[:, :logits_len]\n",
    "\n",
    "                loss_ce = ce_loss_fn(logits.view(-1, logits.size(-1)), padded_labels.view(-1))\n",
    "\n",
    "                # Use consistent loss weights defined in config\n",
    "                loss = (config.ALIGN_WEIGHT * loss_align) + (config.SEMANTIC_WEIGHT * loss_semantic) + (config.CE_WEIGHT * loss_ce)\n",
    "                loss = loss / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), config.GRADIENT_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            train_bar.set_postfix({\n",
    "                \"ce_loss\": f\"{loss_ce.item():.4f}\",\n",
    "                \"align_loss\": f\"{loss_align.item():.4f}\",\n",
    "                \"sem_loss\": f\"{loss_semantic.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        print(f\"\\nðŸ” Running Validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "\n",
    "        val_align_losses, val_semantic_losses, val_ce_losses, eval_samples = [], [], [], []\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
    "            sampler=SubsetRandomSampler(np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)),\n",
    "            collate_fn=collate_fn)\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "            for spectrograms, pil_images, gt_text_captions, labels in tqdm(val_loader, desc=\"Validation Loop\"):\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                eeg_embeddings, logits = model(spectrograms)\n",
    "                image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_text_captions)\n",
    "\n",
    "                logits_len = logits.size(1)\n",
    "                labels_len = labels.size(1)\n",
    "                if labels_len < logits_len:\n",
    "                    padded_labels = F.pad(labels, (0, logits_len - labels_len), 'constant', -100)\n",
    "                else:\n",
    "                    padded_labels = labels[:, :logits_len]\n",
    "\n",
    "                loss_ce = ce_loss_fn(logits.view(-1, logits.size(-1)), padded_labels.view(-1))\n",
    "\n",
    "                val_align_losses.append(loss_align.item())\n",
    "                val_semantic_losses.append(loss_semantic.item())\n",
    "                val_ce_losses.append(loss_ce.item())\n",
    "\n",
    "                if len(eval_samples) < config.VIS_GRID_SIZE:\n",
    "                    for i in range(min(config.VIS_GRID_SIZE - len(eval_samples), len(eeg_captions))):\n",
    "                        eval_samples.append((eeg_captions[i], gt_text_captions[i], pil_images[i]))\n",
    "\n",
    "        avg_val_align_loss = np.mean(val_align_losses)\n",
    "        avg_val_semantic_loss = np.mean(val_semantic_losses)\n",
    "        avg_val_ce_loss = np.mean(val_ce_losses)\n",
    "\n",
    "        total_val_loss = (config.ALIGN_WEIGHT * avg_val_align_loss) + \\\n",
    "                         (config.SEMANTIC_WEIGHT * avg_val_semantic_loss) + \\\n",
    "                         (config.CE_WEIGHT * avg_val_ce_loss)\n",
    "\n",
    "        print(f\"  Validation Results:\")\n",
    "        print(f\"    - Avg Alignment Loss: {avg_val_align_loss:.4f}\")\n",
    "        print(f\"    - Avg Semantic Loss:  {avg_val_semantic_loss:.4f}\")\n",
    "        print(f\"    - Avg CE Loss:        {avg_val_ce_loss:.4f}\")\n",
    "        print(f\"    - Total Weighted Loss: {total_val_loss:.4f}\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            save_path = output_dir / f'best_model_epoch_{epoch+1}_loss_{total_val_loss:.4f}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"ðŸ† New best model saved to {save_path}\")\n",
    "\n",
    "        # --- Print caption comparisons ---\n",
    "        if eval_samples:\n",
    "            print(\"\\n--- âœï¸  Caption Generation Samples ---\")\n",
    "            for i, (predicted_cap, gt_cap, _) in enumerate(eval_samples):\n",
    "                print(f\"Sample {i+1}:\")\n",
    "                print(f\"  - Ground Truth: {gt_cap.strip()}\")\n",
    "                print(f\"  - Predicted:    {predicted_cap.strip()}\")\n",
    "            print(\"------------------------------------\")\n",
    "\n",
    "        if (epoch + 1) % config.EVAL_IMAGE_GENERATION_INTERVAL == 0 and eval_samples:\n",
    "            create_and_save_reconstruction_grid(\n",
    "                eval_samples,\n",
    "                diffusion_pipe,\n",
    "                epoch + 1,\n",
    "                total_val_loss,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "    return output_dir\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     config = TRAIN_CONFIG()\n",
    "#     try:\n",
    "#         output_dir = train_model(config)\n",
    "#         if output_dir:\n",
    "#             print(f\"âœ… Training completed successfully! Results saved to: {output_dir}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Training failed with error: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGlkqnklOiRY"
   },
   "source": [
    "# Correct Scripts for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be31aefb6114f359ff827d012c2f44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61e78603ca34d00b906c0c8fbdeea0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model weights\n",
      "Successfully loaded best model weights!\n",
      "Loading Stable Diffusion pipeline for visualizations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008598fdadcc4da9a3b0739bd3707e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-generated captions from /content/final_lightweight_17k/train_gt_captions.json\n",
      "Loading pre-generated captions from /content/final_lightweight_17k/val_gt_captions.json\n",
      "ðŸŽ¯ Starting Decoupled 2-Stage Training Loop\n",
      "\n",
      "--- STAGE 2: Fine-Tuning Q-Former ONLY (Epochs 15+) ---\n",
      "Creating optimizer for Stage 2...\n",
      "Optimizer and scheduler created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-909854676.py:372: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10334930d9c34d3590da0643d5221127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 16/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:543: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
      "  return F.conv2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc428d1048584afebaad04417fa1e5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1269\n",
      "    - Avg Semantic Loss:  0.9073\n",
      "    - Total Weighted Loss: 4.3951\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/best_model_epoch_16_loss_4.3951.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a tall building with a clock on the top\n",
      "  - Predicted:    a picture of the book, a paperback edition\n",
      "Sample 2:\n",
      "  - Ground Truth: a bus with a lot of luggage inside\n",
      "  - Predicted:    the purple and maroon lines are on a white background\n",
      "Sample 3:\n",
      "  - Ground Truth: a lunch box with a container of food\n",
      "  - Predicted:    a red and gray striped background with a black border\n",
      "Sample 4:\n",
      "  - Ground Truth: a man riding a wave on a surfboard\n",
      "  - Predicted:    the art of personalizing your resume\n",
      "Sample 5:\n",
      "  - Ground Truth: a dog sitting in a car\n",
      "  - Predicted:    a single frame of the video is displayed in a gray background\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 16...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_16_loss_4_3951.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf05e305e1a46efa5c7509cb4b484e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 17/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f58f6ec76744041a0606cf5588bae48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1389\n",
      "    - Avg Semantic Loss:  0.9047\n",
      "    - Total Weighted Loss: 4.4007\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a skateboarder is doing a trick in a skate park\n",
      "  - Predicted:    a large pink and purple striped wall\n",
      "Sample 2:\n",
      "  - Ground Truth: a man riding a surfboard on a wave\n",
      "  - Predicted:    a gray square with a red and white border\n",
      "Sample 3:\n",
      "  - Ground Truth: a snowboarder is doing a trick on a ramp\n",
      "  - Predicted:    an image of the bible with a large text\n",
      "Sample 4:\n",
      "  - Ground Truth: a group of boats on a beach\n",
      "  - Predicted:    a green and purple line with a red border\n",
      "Sample 5:\n",
      "  - Ground Truth: a woman riding a horse in a dirt arena\n",
      "  - Predicted:    the first person to do it has a purple background\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 17...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_17_loss_4_4007.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4735544f0e4e35997df40f148a5087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 18/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4815e6f9f11d421886e979e380dc9128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1426\n",
      "    - Avg Semantic Loss:  0.9047\n",
      "    - Total Weighted Loss: 4.4045\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a green and white fire hydrant\n",
      "  - Predicted:    an image of a square with red and blue lines\n",
      "Sample 2:\n",
      "  - Ground Truth: a young boy eating a donut\n",
      "  - Predicted:    the vertical axis of a bar chart with several colors\n",
      "Sample 3:\n",
      "  - Ground Truth: a car driving on a highway under a sign\n",
      "  - Predicted:    a collection of vertical lines in a dark grey\n",
      "Sample 4:\n",
      "  - Ground Truth: a bathroom with a sink and a mirror\n",
      "  - Predicted:    person, one of the best known faces in a popular and highly successful animated tv series\n",
      "Sample 5:\n",
      "  - Ground Truth: a kitchen with a stove, refrigerator, and microwave\n",
      "  - Predicted:    the screen is pink, purple and green\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 18...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_18_loss_4_4045.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f218a52bb42fba9b74b84852f140e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 19/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfea6fab92bc478aae5dc9918b18ba49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1280\n",
      "    - Avg Semantic Loss:  0.9117\n",
      "    - Total Weighted Loss: 4.4072\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a grill with food on it\n",
      "  - Predicted:    the red line is a graph showing that the temperature in australia has increased over time\n",
      "Sample 2:\n",
      "  - Ground Truth: a bathroom with a sink and a mirror\n",
      "  - Predicted:    a screenshot of a line graph showing the number of people in china\n",
      "Sample 3:\n",
      "  - Ground Truth: a monkey sitting on a ledge eating a banana\n",
      "  - Predicted:    a man with a red and black stripe on his shirt\n",
      "Sample 4:\n",
      "  - Ground Truth: a bowl of soup on a table with two plates\n",
      "  - Predicted:    a rainbow colored rectangle that shows two different colors\n",
      "Sample 5:\n",
      "  - Ground Truth: a baby is sitting on a couch with a teddy bear\n",
      "  - Predicted:    a grey background with blue and green stripes\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 19...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_19_loss_4_4072.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fdcd6d875e41e4b3d88e9e62e4721e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 20/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914470c29a494c3ab800556c7f6142d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1051\n",
      "    - Avg Semantic Loss:  0.9068\n",
      "    - Total Weighted Loss: 4.3722\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/best_model_epoch_20_loss_4.3722.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: two giraffes in the wild\n",
      "  - Predicted:    a line of text that says \"text\"\n",
      "Sample 2:\n",
      "  - Ground Truth: a person riding skis down a snowy mountain\n",
      "  - Predicted:    the map shows a large area with several lines\n",
      "Sample 3:\n",
      "  - Ground Truth: a street sign on a pole in front of tall buildings\n",
      "  - Predicted:    a long line of pink and gray lines on a white background\n",
      "Sample 4:\n",
      "  - Ground Truth: a box of fruit\n",
      "  - Predicted:    the background is shown in green, red and purple\n",
      "Sample 5:\n",
      "  - Ground Truth: a baby elephant is standing next to an adult elephant\n",
      "  - Predicted:    the person on the phone with no one around\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 20...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_20_loss_4_3722.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d4d9004dcf4a9cb68477f74441fa2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 21/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b0cea9b30f411f88d258bfd590af28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1013\n",
      "    - Avg Semantic Loss:  0.9085\n",
      "    - Total Weighted Loss: 4.3725\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a train traveling down the tracks with a few cars\n",
      "  - Predicted:    a line drawing of a red and green tree\n",
      "Sample 2:\n",
      "  - Ground Truth: a bathroom with a sink, toilet and bathtub\n",
      "  - Predicted:    a graphic background for a horizontal line\n",
      "Sample 3:\n",
      "  - Ground Truth: a group of motorcycles parked on the side of a mountain road\n",
      "  - Predicted:    a screenshot of the dark theme on an android phone\n",
      "Sample 4:\n",
      "  - Ground Truth: a man in a suit and tie standing in a doorway\n",
      "  - Predicted:    a vertical line of text on a black background\n",
      "Sample 5:\n",
      "  - Ground Truth: a train on the tracks\n",
      "  - Predicted:    an image of a computer screen with an active text field\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 21...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_21_loss_4_3725.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7561a820ea984f419c6a5a1de97ddc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 22/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273d800ffc764cecbc03c96292f91c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1035\n",
      "    - Avg Semantic Loss:  0.9011\n",
      "    - Total Weighted Loss: 4.3562\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/best_model_epoch_22_loss_4.3562.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a couple of people sitting on a bench near a boat\n",
      "  - Predicted:    a gray background with a red and purple line\n",
      "Sample 2:\n",
      "  - Ground Truth: a man looking at bananas\n",
      "  - Predicted:    a text and graphic page for a line\n",
      "Sample 3:\n",
      "  - Ground Truth: a white and black vase sitting on a table\n",
      "  - Predicted:    a red and black line background with a circle\n",
      "Sample 4:\n",
      "  - Ground Truth: a man sitting at a desk with a laptop computer\n",
      "  - Predicted:    a line drawing of a person with the sun in front\n",
      "Sample 5:\n",
      "  - Ground Truth: a man in a tuxedo\n",
      "  - Predicted:    the web page with an image of a bird\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 22...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_22_loss_4_3562.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7823eea4132645e49d362e74540011d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 23/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27734d92694040569dbcd6237c6fdd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1007\n",
      "    - Avg Semantic Loss:  0.8875\n",
      "    - Total Weighted Loss: 4.3195\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/best_model_epoch_23_loss_4.3195.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a baseball player swinging at a ball\n",
      "  - Predicted:    a vertical stripe, a pink and grey line\n",
      "Sample 2:\n",
      "  - Ground Truth: a person walking down a street with a red umbrella\n",
      "  - Predicted:    a pink background with some lines and curves\n",
      "Sample 3:\n",
      "  - Ground Truth: two black swans swimming in the water\n",
      "  - Predicted:    an image of a small grey and red line\n",
      "Sample 4:\n",
      "  - Ground Truth: a bench sitting on a dirt path in a park\n",
      "  - Predicted:    a vertical red and purple gradient background\n",
      "Sample 5:\n",
      "  - Ground Truth: a living room with a table and chairs\n",
      "  - Predicted:    a dark purple and pink border on the screen\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 23...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_23_loss_4_3195.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0861731313e402685575f009e64ed57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 24/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f93dc34da44910a885ead447caddec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1074\n",
      "    - Avg Semantic Loss:  0.8923\n",
      "    - Total Weighted Loss: 4.3382\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a young boy eating a donut\n",
      "  - Predicted:    an image of a purple and pink line\n",
      "Sample 2:\n",
      "  - Ground Truth: a zebra standing in a dirt field\n",
      "  - Predicted:    a purple and yellow stripe on a dark background\n",
      "Sample 3:\n",
      "  - Ground Truth: a man riding a motorcycle down a highway\n",
      "  - Predicted:    a green and blue striped pattern is shown in the background\n",
      "Sample 4:\n",
      "  - Ground Truth: a living room with a couch, a table, and a window\n",
      "  - Predicted:    a beautiful image of a flower in pink and gray\n",
      "Sample 5:\n",
      "  - Ground Truth: a cat sleeping on a bench\n",
      "  - Predicted:    a red and blue abstract graphic with a gradient effect\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 24...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_24_loss_4_3382.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd0395e04404fbd969452961f9afbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 25/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ed3411024b44f681c3db547676b3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.1008\n",
      "    - Avg Semantic Loss:  0.8932\n",
      "    - Total Weighted Loss: 4.3339\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a train traveling over a bridge over a body of water\n",
      "  - Predicted:    a purple, yellow and green machine\n",
      "Sample 2:\n",
      "  - Ground Truth: a couple of people sitting at a table outside of a building\n",
      "  - Predicted:    an animated black and white graphite line drawing\n",
      "Sample 3:\n",
      "  - Ground Truth: a train is traveling down the tracks\n",
      "  - Predicted:    a gray and pink graphite slide\n",
      "Sample 4:\n",
      "  - Ground Truth: a person on skis\n",
      "  - Predicted:    the background image shows a red and pink line\n",
      "Sample 5:\n",
      "  - Ground Truth: a baby elephant is standing next to an adult elephant\n",
      "  - Predicted:    a white circle with an arrow pointing to it\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 25...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_25_loss_4_3339.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898dd28e52244dc691b960e505997d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 26/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132abcd32e204f83bf91c1528ecc5300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0935\n",
      "    - Avg Semantic Loss:  0.8894\n",
      "    - Total Weighted Loss: 4.3170\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/best_model_epoch_26_loss_4.3170.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a group of people in wet suits on surfboards\n",
      "  - Predicted:    3d text in 3D with a purple background\n",
      "Sample 2:\n",
      "  - Ground Truth: a woman holding a tennis racket on a tennis court\n",
      "  - Predicted:    the purple and blue lines are shown in a rectangular shape\n",
      "Sample 3:\n",
      "  - Ground Truth: a woman holding a rainbow colored umbrella\n",
      "  - Predicted:    a gray and pink colored background, with the words \"the great wall of china\"\n",
      "Sample 4:\n",
      "  - Ground Truth: a lunch box with two containers of food\n",
      "  - Predicted:    a large image of a red, green and yellow striped line\n",
      "Sample 5:\n",
      "  - Ground Truth: two zebras in a fenced in area\n",
      "  - Predicted:    pink and purple lines artboard\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 26...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_26_loss_4_3170.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92258c89d1bf4fa1971b53cb6c538876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 27/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0652837496804089b0ad58148a8b06ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0988\n",
      "    - Avg Semantic Loss:  0.9052\n",
      "    - Total Weighted Loss: 4.3618\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a man is skiing down a snow covered slope\n",
      "  - Predicted:    an airplane in the sky and an arrow on it\n",
      "Sample 2:\n",
      "  - Ground Truth: a desk with a laptop and a desktop computer\n",
      "  - Predicted:    the background is a purple and blue color\n",
      "Sample 3:\n",
      "  - Ground Truth: a man riding a wave on a surfboard\n",
      "  - Predicted:    a pink and purple vertical map background\n",
      "Sample 4:\n",
      "  - Ground Truth: a man riding a bike down a street\n",
      "  - Predicted:    a pink and red background with an image of a boat\n",
      "Sample 5:\n",
      "  - Ground Truth: a bear is standing in the water\n",
      "  - Predicted:    the same image as the one above\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 27...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_27_loss_4_3618.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d034e1f5844b59aa23a97cffbc8de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 28/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dde953cfa844341a28ed1f986e13cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0909\n",
      "    - Avg Semantic Loss:  0.8927\n",
      "    - Total Weighted Loss: 4.3227\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a dog sitting in the grass next to a frisbee\n",
      "  - Predicted:    the black and white map of a diagonal vertical line\n",
      "Sample 2:\n",
      "  - Ground Truth: a table set for a dinner party\n",
      "  - Predicted:    a long black and white diagonal line\n",
      "Sample 3:\n",
      "  - Ground Truth: a person riding skis down a snow covered slope\n",
      "  - Predicted:    an abstract white line texture with pink and purple stripes\n",
      "Sample 4:\n",
      "  - Ground Truth: a man riding a wave on a surfboard\n",
      "  - Predicted:    a gray and black abstract image with a pink border\n",
      "Sample 5:\n",
      "  - Ground Truth: a man riding a wave on a surfboard\n",
      "  - Predicted:    the abstract pink, blue and purple line art\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 28...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_28_loss_4_3227.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2959ffbc9d442a984d2959712ab2c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 29/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bd516bae5b4a2787a9fdb85293be1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0937\n",
      "    - Avg Semantic Loss:  0.8916\n",
      "    - Total Weighted Loss: 4.3228\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a cutting board with a bunch of donuts on it\n",
      "  - Predicted:    a line of purple and pink lines on a grey background\n",
      "Sample 2:\n",
      "  - Ground Truth: two giraffes standing in a field with trees in the background\n",
      "  - Predicted:    a long, white line that is a beautiful color\n",
      "Sample 3:\n",
      "  - Ground Truth: two sheep standing on a road\n",
      "  - Predicted:    a rainbow of shapes in a dark background\n",
      "Sample 4:\n",
      "  - Ground Truth: a plate of pasta and broccoli\n",
      "  - Predicted:    a long, pink and red striped line\n",
      "Sample 5:\n",
      "  - Ground Truth: a train is on the tracks at a station\n",
      "  - Predicted:    a grayish blue and purple smokey, grey cloud background\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 29...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_29_loss_4_3228.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a429f691af804ad2a8d4a81d97a033c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 30/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc357e7ef864a56b58817ca0e6f8d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0917\n",
      "    - Avg Semantic Loss:  0.8938\n",
      "    - Total Weighted Loss: 4.3261\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a couple of red double decker buses driving down a street\n",
      "  - Predicted:    a red and black stripey background\n",
      "Sample 2:\n",
      "  - Ground Truth: a plate of food with meat and broccoli\n",
      "  - Predicted:    a red and purple vertical line drawing\n",
      "Sample 3:\n",
      "  - Ground Truth: a group of people sitting on a park bench\n",
      "  - Predicted:    an image of a computer screen displaying a graph\n",
      "Sample 4:\n",
      "  - Ground Truth: a baby is laying in a basket with a teddy bear\n",
      "  - Predicted:    the purple and blue line story map\n",
      "Sample 5:\n",
      "  - Ground Truth: a group of boats on a beach\n",
      "  - Predicted:    a pink and purple graphite line art\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 30...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_30_loss_4_3261.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f189f9bcdf4deb953f1d7c22a21bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 31/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0ea136262747209d9b834a36e07f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0905\n",
      "    - Avg Semantic Loss:  0.9001\n",
      "    - Total Weighted Loss: 4.3408\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a yellow train on the tracks\n",
      "  - Predicted:    a line drawing of a tree with leaves\n",
      "Sample 2:\n",
      "  - Ground Truth: a man riding a surfboard on a wave\n",
      "  - Predicted:    a red, pink and gray line graph\n",
      "Sample 3:\n",
      "  - Ground Truth: a group of horses standing on a hill\n",
      "  - Predicted:    pink and blue stripe background for web and mobile\n",
      "Sample 4:\n",
      "  - Ground Truth: three giraffes in a pen\n",
      "  - Predicted:    a long pink and purple line on a white background\n",
      "Sample 5:\n",
      "  - Ground Truth: a bunk bed in a room with a door\n",
      "  - Predicted:    a pink and purple line of lines on a wall\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 31...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_31_loss_4_3408.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28946052cd3404f812ac14031227cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 32/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bd263011d14b8191ad2046bfa85edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0899\n",
      "    - Avg Semantic Loss:  0.9041\n",
      "    - Total Weighted Loss: 4.3502\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a blue motorcycle parked in a parking lot\n",
      "  - Predicted:    a purple and pink line on a red background\n",
      "Sample 2:\n",
      "  - Ground Truth: a man in a wet suit is riding a wave\n",
      "  - Predicted:    a dark, long vertical stripe with pink and brown lines\n",
      "Sample 3:\n",
      "  - Ground Truth: a train station with several trains on the tracks\n",
      "  - Predicted:    a long red and pink diagonal stripe\n",
      "Sample 4:\n",
      "  - Ground Truth: a man swinging a baseball bat on a baseball field\n",
      "  - Predicted:    a pink and gray background is shown on a large computer screen\n",
      "Sample 5:\n",
      "  - Ground Truth: a group of dogs running in a field\n",
      "  - Predicted:    a ball in the air with a long and thin line\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 32...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_32_loss_4_3502.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2c9900a27a4042b213c6e035ffe615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 33/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42f4721c084ecca09d2f20a87881d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0897\n",
      "    - Avg Semantic Loss:  0.8946\n",
      "    - Total Weighted Loss: 4.3261\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a table with two containers of food and a bowl of salad\n",
      "  - Predicted:    a female and male line of panthers\n",
      "Sample 2:\n",
      "  - Ground Truth: a dog laying in the sand under a chair\n",
      "  - Predicted:    a long black and grey line with the word\n",
      "Sample 3:\n",
      "  - Ground Truth: a kitchen with pots hanging from the ceiling\n",
      "  - Predicted:    an abstract background with pink and red lines\n",
      "Sample 4:\n",
      "  - Ground Truth: a white bowl with food\n",
      "  - Predicted:    a dark red and blue line on the background\n",
      "Sample 5:\n",
      "  - Ground Truth: a tall stone building with a clock on the side\n",
      "  - Predicted:    a red and pink abstract line art\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 33...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_33_loss_4_3261.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ad801405134d3ba8c589cef5162719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 34/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05438ffdfd2f4159a1bae67ea8988672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.0910\n",
      "    - Avg Semantic Loss:  0.8927\n",
      "    - Total Weighted Loss: 4.3227\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a bed with white sheets\n",
      "  - Predicted:    a black and grey background with the word \"breathtaking\"\n",
      "Sample 2:\n",
      "  - Ground Truth: three zebras grazing in a field near a body of water\n",
      "  - Predicted:    a large pink, blue and purple line art\n",
      "Sample 3:\n",
      "  - Ground Truth: a man sitting at a table eating a meal\n",
      "  - Predicted:    a red and gray pattern of a large floor\n",
      "Sample 4:\n",
      "  - Ground Truth: a large air france airplane flying through the air\n",
      "  - Predicted:    a pink and brown line on a grey background\n",
      "Sample 5:\n",
      "  - Ground Truth: a bathroom with a toilet, sink and mirror\n",
      "  - Predicted:    a collection of a pink and purple\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 34...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_161150/reconstructions/reconstruction_epoch_34_loss_4_3227.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f852f114394b1989c1e3b025fbbc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 35/50:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-909854676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_CONFIG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Training completed successfully! Results saved to: {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-909854676.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0meeg_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eeg_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgeneration_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                     \u001b[0meeg_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-909854676.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, eeg_spectrograms, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m             \u001b[0mlanguage_model_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             generated_ids = self.blip.language_model.generate(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage_model_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2868\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2870\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m    827\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_values, output_attentions, use_cache, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mattention_interface\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meager_attention_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sdpa\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 logger.warning_once(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment'\n",
    "\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    GRAD_ACCUMULATION_STEPS = 4\n",
    "    NUM_EPOCHS = 50\n",
    "    ADAPTER_LR = 1e-5  # Higher LR for the new adapter\n",
    "    BLIP_LR = 1e-6     # Lower LR for fine-tuning the pre-trained layers\n",
    "\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "    GRADIENT_CLIP_NORM = 2.0\n",
    "\n",
    "    VALIDATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 400\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 2000\n",
    "    VIS_GRID_SIZE = 5\n",
    "\n",
    "    DIFFUSION_MODEL_ID =\"runwayml/stable-diffusion-v1-5\"\n",
    "    EVAL_IMAGE_GENERATION_INTERVAL = 1\n",
    "    WARMUP_STEPS = 400  # Number of steps to gradually increase the LR\n",
    "    TOTAL_TRAIN_STEPS = 250 * NUM_EPOCHS\n",
    "    STAGE1_EPOCHS = 15\n",
    "    ALIGN_WEIGHT = 1.0\n",
    "    SEMANTIC_WEIGHT = 2.5\n",
    "    BEST_MODEL_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251001_103040/best_model_epoch_27_loss_5.0528.pth'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER MODULE ---\n",
    "# ==============================================================================\n",
    "\n",
    "class DeeperEEGNetAdapter(nn.Module):\n",
    "    \"\"\"A deeper, more powerful EEGNet-inspired adapter.\"\"\"\n",
    "    def __init__(self, in_chans=64, out_chans=3, F1=16, D=2, F2=32, F3=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # Block 1 (same as before)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, 64), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1 * D, (in_chans, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Block 2 (same as before)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F2, (1, 16), padding='same', groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F2, F2, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # --- NEW: Third block for more capacity ---\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(F2, F3, (1, 8), padding='same', groups=F2, bias=False),\n",
    "            nn.Conv2d(F3, F3, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(F3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.final_projection = nn.Conv2d(F3, out_chans, (1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((224, 224))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, x.size(1), -1)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x) # Pass through the new block\n",
    "        x = self.final_projection(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE HYBRID EEG-BLIP2 MODEL ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # --- REFACTOR: Instantiate the adapter directly ---\n",
    "        self.adapter = DeeperEEGNetAdapter(in_chans=64)\n",
    "\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # Freeze the entire BLIP model initially\n",
    "        for param in self.blip.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.adapter.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        # --- REFACTOR: Forward pass is now cleaner ---\n",
    "        pseudo_image = self.adapter(eeg_spectrograms)\n",
    "        vision_outputs = self.blip.vision_model(pseudo_image, return_dict=True)\n",
    "        eeg_features = vision_outputs.last_hidden_state\n",
    "\n",
    "        query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # --- REFACTOR: Generation pass is also cleaner ---\n",
    "            pseudo_image = self.adapter(eeg_spectrograms)\n",
    "            vision_outputs = self.blip.vision_model(pseudo_image, return_dict=True)\n",
    "            eeg_features = vision_outputs.last_hidden_state\n",
    "\n",
    "            query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=eeg_features,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs,\n",
    "                **kwargs\n",
    "            )\n",
    "            return generated_ids\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & COLLATING ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])\n",
    "    pil_images = [item[1] for item in batch]\n",
    "    captions = [item[2] for item in batch]\n",
    "    return spectrograms, pil_images, captions\n",
    "\n",
    "class EEGDatasetWithCaptions(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, transform, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "\n",
    "        # --- NEW: Define a dedicated augmentation pipeline ---\n",
    "        if self.augment:\n",
    "            self.augmentation_transform = transforms.Compose([\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "                # Randomly mask out frequency bands\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(0.1, 0.5))], p=0.5),\n",
    "                # Randomly mask out time steps\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(2.0, 5.0))], p=0.5),\n",
    "            ])\n",
    "\n",
    "        # This is your existing normalization transform\n",
    "        self.normalization_transform = transform\n",
    "\n",
    "        # Load pre-generated captions\n",
    "        captions_path = Path(root_dir) / f'{split}_gt_captions.json'\n",
    "        print(f\"Loading pre-generated captions from {captions_path}\")\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "\n",
    "        spectrogram = torch.load(self.root_dir / info['spectrogram_path'])\n",
    "\n",
    "        # --- APPLY AUGMENTATIONS (for training set only) ---\n",
    "        if self.augment:\n",
    "            spectrogram = self.augmentation_transform(spectrogram)\n",
    "\n",
    "        # Apply normalization after augmentation\n",
    "        spectrogram = self.normalization_transform(spectrogram)\n",
    "\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        gt_caption = self.captions.get(str(idx), \"an image\")\n",
    "\n",
    "        return spectrogram, image, gt_caption\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN TRAINING FUNCTION ---\n",
    "# ==============================================================================\n",
    "# InfoNCE loss function\n",
    "\n",
    "def info_nce_loss(query, positive_key, temperature=0.07):\n",
    "    # Ensure inputs are normalized\n",
    "    query = F.normalize(query, dim=-1)\n",
    "    positive_key = F.normalize(positive_key, dim=-1)\n",
    "\n",
    "    # Calculate the similarity matrix of every query with every key\n",
    "    # The diagonal of this matrix contains the positive pairs\n",
    "    logits = query @ positive_key.T\n",
    "\n",
    "    # The labels are the indices of the positive pairs (the diagonal)\n",
    "    labels = torch.arange(len(query), device=query.device)\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    return F.cross_entropy(logits / temperature, labels)\n",
    "\n",
    "\n",
    "class SemanticLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A loss function that measures the semantic similarity between two sets of captions\n",
    "    using a pre-trained SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Load a pre-trained model optimized for semantic similarity\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "        # Freeze the model's weights as we only use it for inference\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, predicted_captions, ground_truth_captions):\n",
    "        # Convert the text captions into semantic embedding vectors\n",
    "        pred_embeddings = self.model.encode(predicted_captions, convert_to_tensor=True)\n",
    "        gt_embeddings = self.model.encode(ground_truth_captions, convert_to_tensor=True)\n",
    "\n",
    "        # Calculate the cosine similarity. The loss is 1.0 minus the similarity.\n",
    "        # A higher similarity (closer to 1.0) results in a lower loss (closer to 0.0).\n",
    "        cosine_sim = F.cosine_similarity(pred_embeddings, gt_embeddings, dim=-1)\n",
    "        loss = 1.0 - cosine_sim.mean()\n",
    "        return loss\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "def create_and_save_reconstruction_grid(eval_samples, diffusion_pipe, epoch, val_loss, output_dir):\n",
    "    \"\"\"\n",
    "    Generates a 2x2 image grid comparing original and reconstructed images.\n",
    "    Top row: Original images with ground truth captions.\n",
    "    Bottom row: Reconstructed images with predicted captions.\n",
    "    \"\"\"\n",
    "    if len(eval_samples) < 2:\n",
    "        print(\"Not enough samples for a 2x2 grid, skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ–¼ï¸  Generating reconstruction grid for epoch {epoch}...\")\n",
    "\n",
    "    # Setup canvas and font\n",
    "    w, h = 512, 512  # Standard Stable Diffusion size\n",
    "    title_h = 60    # Space for titles\n",
    "    grid = Image.new('RGB', (w * 2, h * 2 + title_h * 2), 'black')\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 20)\n",
    "    except IOError:\n",
    "        print(\"Default font not found, using fallback.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Process two samples\n",
    "    for i in range(5):\n",
    "        predicted_cap, gt_cap, original_image = eval_samples[i]\n",
    "\n",
    "        # --- Generate the reconstructed image from the predicted caption ---\n",
    "        generator = torch.Generator(device=diffusion_pipe.device).manual_seed(42 + i)\n",
    "        reconstructed_image = diffusion_pipe(\n",
    "            prompt=predicted_cap, generator=generator, num_inference_steps=20\n",
    "        ).images[0]\n",
    "\n",
    "        # --- Paste images and draw titles ---\n",
    "        col_offset = i * w\n",
    "\n",
    "        # Top Row (Original)\n",
    "        draw.text((col_offset + 5, 5), f\"GT: {gt_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(original_image.resize((w, h)), (col_offset, title_h))\n",
    "\n",
    "        # Bottom Row (Reconstructed)\n",
    "        draw.text((col_offset + 5, h + title_h + 5), f\"Pred: {predicted_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(reconstructed_image.resize((w, h)), (col_offset, h + title_h * 2))\n",
    "\n",
    "    # --- Save the final grid ---\n",
    "    # Sanitize loss value for filename\n",
    "    val_loss_str = f\"{val_loss:.4f}\".replace('.', '_')\n",
    "    save_path = Path(output_dir) / 'reconstructions'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    filename = save_path / f\"reconstruction_epoch_{epoch}_loss_{val_loss_str}.png\"\n",
    "    grid.save(filename)\n",
    "    print(f\"  Saved reconstruction grid to {filename}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "def train_model(config):\n",
    "    # --- Setup (Device, Output Dir, etc.) ---\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Models and Data ---\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    model = EEG_BLIP2_Model(config, device) # Assumes the refactored, cleaner version\n",
    "\n",
    "    try:\n",
    "      print('Loading best model weights')\n",
    "      model.load_state_dict(torch.load(config.BEST_MODEL_DIR, map_location=device))\n",
    "      print('Successfully loaded best model weights!')\n",
    "    except:\n",
    "      print('Could not load the best model weights.')\n",
    "\n",
    "    print(\"Loading Stable Diffusion pipeline for visualizations...\")\n",
    "    diffusion_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        config.DIFFUSION_MODEL_ID,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        safety_checker=None, requires_safety_checker=False\n",
    "    ).to(device)\n",
    "    diffusion_pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    semantic_loss_fn = SemanticLoss(device)\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    spec_std += 1e-6\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "\n",
    "    train_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', transform, augment=True)\n",
    "    val_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', transform, augment=False)\n",
    "\n",
    "    # --- Training Components ---\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 30, \"do_sample\": True, \"min_new_tokens\": 8,\n",
    "        \"repetition_penalty\": 1.25, \"top_p\": 0.9, \"temperature\": 0.81\n",
    "    }\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    print(\"ðŸŽ¯ Starting Decoupled 2-Stage Training Loop\")\n",
    "    for epoch in range(15, config.NUM_EPOCHS):\n",
    "        # --- Stage 1 & 2 Freezing Logic ---\n",
    "        if epoch < config.STAGE1_EPOCHS:\n",
    "            if epoch == 0:\n",
    "                print(f\"\\n--- STAGE 1: Training Adapter ONLY (Epochs 0-{config.STAGE1_EPOCHS-1}) ---\")\n",
    "            for param in model.adapter.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in model.blip.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif epoch == config.STAGE1_EPOCHS:\n",
    "            print(f\"\\n--- STAGE 2: Fine-Tuning Q-Former ONLY (Epochs {config.STAGE1_EPOCHS}+) ---\")\n",
    "            for param in model.adapter.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.blip.qformer.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in model.blip.vision_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            if hasattr(model.blip, \"language_projection\"):\n",
    "                for param in model.blip.language_projection.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # --- Create/Re-create Optimizer & Scheduler at stage transitions ---\n",
    "        if epoch == 0 or epoch == config.STAGE1_EPOCHS:\n",
    "            print(f\"Creating optimizer for Stage {1 if epoch < config.STAGE1_EPOCHS else 2}...\")\n",
    "            if epoch < config.STAGE1_EPOCHS:\n",
    "                # Stage 1: Only adapter is trained\n",
    "                optimizer = optim.AdamW(model.adapter.parameters(), lr=config.ADAPTER_LR, weight_decay=config.WEIGHT_DECAY)\n",
    "            else:\n",
    "                # Stage 2: Only BLIP layers are trained\n",
    "                trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "                optimizer = optim.AdamW(trainable_params, lr=config.BLIP_LR, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "            num_training_steps = (len(train_dataset) // config.BATCH_SIZE) * config.NUM_EPOCHS\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=config.WARMUP_STEPS, num_training_steps=num_training_steps\n",
    "            )\n",
    "            print(\"Optimizer and scheduler created.\")\n",
    "\n",
    "        # --- Training Batch Loop ---\n",
    "        model.train()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
    "            sampler=SubsetRandomSampler(np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)),\n",
    "            drop_last=True, collate_fn=collate_fn)\n",
    "        train_bar = tqdm(train_loader, desc=f\"ðŸ”„ Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "\n",
    "        for batch_idx, (spectrograms, pil_images, gt_captions) in enumerate(train_bar):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                    eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                    image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_captions)\n",
    "\n",
    "                # Use consistent loss weights defined in config\n",
    "                loss = (config.ALIGN_WEIGHT * loss_align) + (config.SEMANTIC_WEIGHT * loss_semantic)\n",
    "                loss = loss / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), config.GRADIENT_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            train_bar.set_postfix({\"align_loss\": f\"{loss_align.item():.4f}\", \"sem_loss\": f\"{loss_semantic.item():.4f}\"})\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        print(f\"\\nðŸ” Running Validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Lists to store metrics for this epoch\n",
    "        val_align_losses = []\n",
    "        val_semantic_losses = []\n",
    "        eval_samples = [] # To store samples for printing and image generation\n",
    "\n",
    "        # Create the validation data loader\n",
    "        val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
    "            sampler=SubsetRandomSampler(val_indices), collate_fn=collate_fn)\n",
    "\n",
    "        # Disable gradients for the validation loop\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "            for spectrograms, pil_images, gt_captions in tqdm(val_loader, desc=\"Validation Loop\"):\n",
    "                spectrograms = spectrograms.to(device)\n",
    "\n",
    "                # --- Perform the forward pass ---\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "                image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                # Generate predicted captions from the EEG\n",
    "                generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                # --- Calculate the two primary losses ---\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_captions)\n",
    "\n",
    "                val_align_losses.append(loss_align.item())\n",
    "                val_semantic_losses.append(loss_semantic.item())\n",
    "\n",
    "                # --- Collect a few samples for visualization ---\n",
    "                if len(eval_samples) < config.VIS_GRID_SIZE:\n",
    "                    num_to_add = config.VIS_GRID_SIZE - len(eval_samples)\n",
    "                    for i in range(min(num_to_add, len(eeg_captions))):\n",
    "                        eval_samples.append((eeg_captions[i], gt_captions[i], pil_images[i]))\n",
    "\n",
    "        avg_val_align_loss = np.mean(val_align_losses)\n",
    "        avg_val_semantic_loss = np.mean(val_semantic_losses)\n",
    "\n",
    "        # Use consistent loss weights\n",
    "        total_val_loss = (config.ALIGN_WEIGHT * avg_val_align_loss) + (config.SEMANTIC_WEIGHT * avg_val_semantic_loss)\n",
    "\n",
    "        print(f\"  Validation Results:\")\n",
    "        print(f\"    - Avg Alignment Loss: {avg_val_align_loss:.4f}\")\n",
    "        print(f\"    - Avg Semantic Loss:  {avg_val_semantic_loss:.4f}\")\n",
    "        print(f\"    - Total Weighted Loss: {total_val_loss:.4f}\")\n",
    "\n",
    "        # Update the learning rate scheduler based on the total validation loss\n",
    "        # scheduler.step(total_val_loss)\n",
    "\n",
    "        # --- Save the best model based on the total validation loss ---\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            save_path = output_dir / f'best_model_epoch_{epoch+1}_loss_{total_val_loss:.4f}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"ðŸ† New best model saved to {save_path}\")\n",
    "\n",
    "        # --- Print caption comparisons ---\n",
    "        if eval_samples:\n",
    "            print(\"\\n--- âœï¸  Caption Generation Samples ---\")\n",
    "            for i, (predicted_cap, gt_cap, _) in enumerate(eval_samples):\n",
    "                print(f\"Sample {i+1}:\")\n",
    "                print(f\"  - Ground Truth: {gt_cap.strip()}\")\n",
    "                print(f\"  - Predicted:    {predicted_cap.strip()}\")\n",
    "            print(\"------------------------------------\")\n",
    "\n",
    "        if (epoch + 1) % config.EVAL_IMAGE_GENERATION_INTERVAL == 0 and eval_samples:\n",
    "            create_and_save_reconstruction_grid(\n",
    "                eval_samples,\n",
    "                diffusion_pipe,\n",
    "                epoch + 1,\n",
    "                total_val_loss,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "    return output_dir\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = TRAIN_CONFIG()\n",
    "    try:\n",
    "        output_dir = train_model(config)\n",
    "        if output_dir:\n",
    "            print(f\"âœ… Training completed successfully! Results saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eaa8999e2142308aa8eb2b404e7baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4434698983c74bf9811c1e624ee7284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved EEG_BLIP2_Model weights\n",
      "Loading Stable Diffusion pipeline for visualizations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86100c7170bc4f75bf8c0c2d5d49cf62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-generated captions from /content/final_lightweight_17k/train_gt_captions.json\n",
      "Loading pre-generated captions from /content/final_lightweight_17k/val_gt_captions.json\n",
      "ðŸŽ¯ Starting 2-Stage Training Loop\n",
      "\n",
      "--- STAGE 1: Training EEGNetAdapter Only ---\n",
      "Creating optimizer for Stage 1...\n",
      "Optimizer and scheduler created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2558806505.py:405: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13981750b2264ef8bfc3ec3d2cca1b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 1/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:543: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
      "  return F.conv2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359f06a99cff46b1b71f3f96c778ec7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7884\n",
      "    - Avg Semantic Loss:  0.8919\n",
      "    - Total Weighted Loss: 5.0182\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/best_model_epoch_1_loss_5.0182.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: two seagulls are swimming in the water\n",
      "  - Predicted:    a purple striped background with a thin line in the middle\n",
      "Sample 2:\n",
      "  - Ground Truth: an elephant standing in a field with bushes in the background\n",
      "  - Predicted:    the dark blue and red vertical lines on a gray background\n",
      "Sample 3:\n",
      "  - Ground Truth: a sheep is grazing on grass in a field\n",
      "  - Predicted:    a large square with a long gray line\n",
      "Sample 4:\n",
      "  - Ground Truth: a tall building with a clock on the top\n",
      "  - Predicted:    a person with long hair is shown in this picture\n",
      "Sample 5:\n",
      "  - Ground Truth: a baseball player swinging a bat at a ball\n",
      "  - Predicted:    a grey and red striped wall with a dark background\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 1...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_1_loss_5_0182.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeb717ea0a84b31b91becf1735b0c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 2/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eea3712de44ea58dbf443498ea93a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7897\n",
      "    - Avg Semantic Loss:  0.8939\n",
      "    - Total Weighted Loss: 5.0244\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a plane on the runway\n",
      "  - Predicted:    a pink and gray striped background\n",
      "Sample 2:\n",
      "  - Ground Truth: a boat that is traveling on the water\n",
      "  - Predicted:    a gray and pink striped background\n",
      "Sample 3:\n",
      "  - Ground Truth: a dog sitting in a car\n",
      "  - Predicted:    a woman with her hands on the keyboard and some text\n",
      "Sample 4:\n",
      "  - Ground Truth: a herd of cows grazing in a field\n",
      "  - Predicted:    a man is wearing a shirt with the words \"you are beautiful\"\n",
      "Sample 5:\n",
      "  - Ground Truth: a car with a surfboard on top of it\n",
      "  - Predicted:    a video of the screen with pink and purple lines\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 2...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_2_loss_5_0244.png\n",
      "\n",
      "--- STAGE 2: Fine-Tuning Q-Former ---\n",
      "  - Unfroze Language Projection layer.\n",
      "Creating optimizer for Stage 2...\n",
      "Optimizer and scheduler created.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15e79caaa3a45e2b448dc7ee3408fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 3/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bcd3ff2fa34c9d81f0b7443c27a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7895\n",
      "    - Avg Semantic Loss:  0.8914\n",
      "    - Total Weighted Loss: 5.0179\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/best_model_epoch_3_loss_5.0179.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a kitchen with a refrigerator, stove, and sink\n",
      "  - Predicted:    a vertical stripe with red and black lines\n",
      "Sample 2:\n",
      "  - Ground Truth: a vase with flowers in it\n",
      "  - Predicted:    a screenshot of the image on a mobile device\n",
      "Sample 3:\n",
      "  - Ground Truth: three giraffes in a zoo enclosure\n",
      "  - Predicted:    a gray, red and black screen is shown in the image\n",
      "Sample 4:\n",
      "  - Ground Truth: a plate of broccoli\n",
      "  - Predicted:    the wedding video is shown with two red and pink bars\n",
      "Sample 5:\n",
      "  - Ground Truth: a desk with a computer, a laptop, and a monitor\n",
      "  - Predicted:    a large horizontal line with red stripes and a small black box\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 3...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_3_loss_5_0179.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985c21925d954496a7424c270838d0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 4/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2370b71e7c54b3ca278979392b2bdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7895\n",
      "    - Avg Semantic Loss:  0.8869\n",
      "    - Total Weighted Loss: 5.0068\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/best_model_epoch_4_loss_5.0068.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a bowl of soup on a table with two plates\n",
      "  - Predicted:    the background of a dark gray screen with pink lines\n",
      "Sample 2:\n",
      "  - Ground Truth: a stop light and a sign that says no parking\n",
      "  - Predicted:    the background of a grayish, purple and black color\n",
      "Sample 3:\n",
      "  - Ground Truth: a laptop computer and a mouse on a table\n",
      "  - Predicted:    a grey and purple stripe on a screen\n",
      "Sample 4:\n",
      "  - Ground Truth: a dog sitting in a car\n",
      "  - Predicted:    a logo that says, 'The people's court'\n",
      "Sample 5:\n",
      "  - Ground Truth: a view of a highway with a sign that says \"exit\"\n",
      "  - Predicted:    the text is \"get a new life\" and then the image changes\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 4...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_4_loss_5_0068.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6075cae4f4914da3a203e743db60c355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 5/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d3067177144f50b28ee49dd964ff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7874\n",
      "    - Avg Semantic Loss:  0.8836\n",
      "    - Total Weighted Loss: 4.9964\n",
      "ðŸ† New best model saved to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/best_model_epoch_5_loss_4.9964.pth\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a group of people sitting around a table eating pizza\n",
      "  - Predicted:    a red rectangle with a black border\n",
      "Sample 2:\n",
      "  - Ground Truth: a man standing in a group of sheep\n",
      "  - Predicted:    a video of the logo for person\n",
      "Sample 3:\n",
      "  - Ground Truth: a small airplane on the ground\n",
      "  - Predicted:    the logo of company on the bottom\n",
      "Sample 4:\n",
      "  - Ground Truth: a church with a clock tower and a path\n",
      "  - Predicted:    a logo with a purple and blue background\n",
      "Sample 5:\n",
      "  - Ground Truth: a bear is standing in the water\n",
      "  - Predicted:    a red and white stripe is visible on the side of a video\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 5...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_5_loss_4_9964.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc4426731de4783a21a13e6a9ef0c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 6/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation for epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0ee94e13874bd78245f651b09f2099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Loop:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Results:\n",
      "    - Avg Alignment Loss: 2.7855\n",
      "    - Avg Semantic Loss:  0.8981\n",
      "    - Total Weighted Loss: 5.0306\n",
      "\n",
      "--- âœï¸  Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a pair of green scissors\n",
      "  - Predicted:    the background of a black and white screen, with the image showing an animated red diagonal line\n",
      "Sample 2:\n",
      "  - Ground Truth: a green and white fire hydrant\n",
      "  - Predicted:    a screenshot of a computer screen with an animated background\n",
      "Sample 3:\n",
      "  - Ground Truth: a bed with a white sheet\n",
      "  - Predicted:    the video is shot with a red and white background\n",
      "Sample 4:\n",
      "  - Ground Truth: a double decker bus driving down a street\n",
      "  - Predicted:    a pink and grey line on the background\n",
      "Sample 5:\n",
      "  - Ground Truth: a bear is walking through a field of green bushes\n",
      "  - Predicted:    a purple and orange striped background\n",
      "------------------------------------\n",
      "\n",
      "ðŸ–¼ï¸  Generating reconstruction grid for epoch 6...\n",
      "  Saved reconstruction grid to /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_234041/reconstructions/reconstruction_epoch_6_loss_5_0306.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f945d49a44b40529bf21069710d3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 7/40:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment'\n",
    "\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    GRAD_ACCUMULATION_STEPS = 2\n",
    "    NUM_EPOCHS = 40\n",
    "    ADAPTER_LR = 1e-5  # Higher LR for the new adapter\n",
    "    BLIP_LR = 3e-6     # Lower LR for fine-tuning the pre-trained layers\n",
    "\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "    GRADIENT_CLIP_NORM = 1.0\n",
    "\n",
    "    VALIDATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 700\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 3500\n",
    "    VIS_GRID_SIZE = 5\n",
    "\n",
    "    DIFFUSION_MODEL_ID =\"runwayml/stable-diffusion-v1-5\"\n",
    "    EVAL_IMAGE_GENERATION_INTERVAL = 1\n",
    "    WARMUP_STEPS = 150  # Number of steps to gradually increase the LR\n",
    "    TOTAL_TRAIN_STEPS = 218 * 40\n",
    "    STAGE1_EPOCHS = 2\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER MODULE ---\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    A powerful CNN adapter inspired by EEGNet principles to extract robust\n",
    "    features from EEG spectrograms. It processes the 64 channels to find\n",
    "    temporal and spatial patterns, then creates a 3-channel pseudo-image.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=64, out_chans=3, F1=16, D=2, F2=32, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Block 1: Temporal and Depthwise Spatial Convolutions\n",
    "        self.block1 = nn.Sequential(\n",
    "            # Input is [B, 1, Chans, Samples]\n",
    "            # Temporal Conv to learn filters across time\n",
    "            nn.Conv2d(1, F1, (1, 64), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            # Depthwise Spatial Conv to learn spatial filters for each feature map\n",
    "            nn.Conv2d(F1, F1 * D, (in_chans, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            # nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Block 2: Separable Convolutions for more abstract features\n",
    "        self.block2 = nn.Sequential(\n",
    "            # Separable Conv (Depthwise followed by Pointwise)\n",
    "            nn.Conv2d(F1 * D, F2, (1, 16), padding='same', groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F2, F2, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            # nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Final projection layer to create the 3-channel \"pseudo-image\"\n",
    "        self.final_projection = nn.Conv2d(F2, out_chans, (1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((224, 224))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is [Batch, 64 Channels, Freq Bins, Time Steps]\n",
    "        # We reshape it to fit the EEGNet structure: [Batch, 1, Channels, Samples]\n",
    "        # This treats the frequency and time dimensions as one long sequence.\n",
    "        x = x.view(x.size(0), 1, x.size(1), -1)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        # Project to 3 channels\n",
    "        x = self.final_projection(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # The output is a feature map, which will be resized by the vision model's patch embedding\n",
    "        return x\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, config, in_chans=64):\n",
    "        super().__init__()\n",
    "        # self.vision_model = Blip2VisionModel.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # Freeze the original vision model\n",
    "        # for param in self.vision_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.adapter = ChannelAdapter(in_chans=in_chans)\n",
    "\n",
    "        for p in self.adapter.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "    def forward(self, x, vision_model):\n",
    "      adapted_x = self.adapter(x)\n",
    "      vision_outputs = vision_model(adapted_x, return_dict=True)\n",
    "      return vision_outputs.last_hidden_state\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE HYBRID EEG-BLIP2 MODEL ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.eeg_encoder = EEGEncoder(config)\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            config.BLIP_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        # Freeze the entire BLIP model initially\n",
    "        for param in self.blip.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # print(\"Unfreezing specified layers for fine-tuning...\")\n",
    "\n",
    "        # --- THIS IS THE KEY CHANGE ---\n",
    "        # Unfreeze the ENTIRE Q-Former\n",
    "        # for param in self.blip.qformer.parameters():\n",
    "        #     param.requires_grad = True\n",
    "        # print(\" - Unfroze the entire Q-Former.\")\n",
    "\n",
    "        # # Also unfreeze the language projection layer (connects Q-Former to LLM)\n",
    "        # if hasattr(self.blip, \"language_projection\"):\n",
    "        #     for param in self.blip.language_projection.parameters():\n",
    "        #         param.requires_grad = True\n",
    "        #     print(\"  - Unfroze Language Projection layer.\")\n",
    "\n",
    "\n",
    "        self.eeg_encoder.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms, self.blip.vision_model)\n",
    "        query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            eeg_features = self.eeg_encoder(eeg_spectrograms, self.blip.vision_model)\n",
    "            query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=eeg_features,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs,\n",
    "                **kwargs\n",
    "            )\n",
    "            return generated_ids\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & COLLATING ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])\n",
    "    pil_images = [item[1] for item in batch]\n",
    "    captions = [item[2] for item in batch]\n",
    "    return spectrograms, pil_images, captions\n",
    "\n",
    "class EEGDatasetWithCaptions(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, transform, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "\n",
    "        # --- NEW: Define a dedicated augmentation pipeline ---\n",
    "        if self.augment:\n",
    "            self.augmentation_transform = transforms.Compose([\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "                # Randomly mask out frequency bands\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(0.1, 0.5))], p=0.5),\n",
    "                # Randomly mask out time steps\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(2.0, 5.0))], p=0.5),\n",
    "            ])\n",
    "\n",
    "        # This is your existing normalization transform\n",
    "        self.normalization_transform = transform\n",
    "\n",
    "        # Load pre-generated captions\n",
    "        captions_path = Path(root_dir) / f'{split}_gt_captions.json'\n",
    "        print(f\"Loading pre-generated captions from {captions_path}\")\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "\n",
    "        spectrogram = torch.load(self.root_dir / info['spectrogram_path'])\n",
    "\n",
    "        # --- APPLY AUGMENTATIONS (for training set only) ---\n",
    "        if self.augment:\n",
    "            spectrogram = self.augmentation_transform(spectrogram)\n",
    "\n",
    "        # Apply normalization after augmentation\n",
    "        spectrogram = self.normalization_transform(spectrogram)\n",
    "\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        gt_caption = self.captions.get(str(idx), \"an image\")\n",
    "\n",
    "        return spectrogram, image, gt_caption\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN TRAINING FUNCTION ---\n",
    "# ==============================================================================\n",
    "# InfoNCE loss function\n",
    "\n",
    "def info_nce_loss(query, positive_key, temperature=0.07):\n",
    "    # Ensure inputs are normalized\n",
    "    query = F.normalize(query, dim=-1)\n",
    "    positive_key = F.normalize(positive_key, dim=-1)\n",
    "\n",
    "    # Calculate the similarity matrix of every query with every key\n",
    "    # The diagonal of this matrix contains the positive pairs\n",
    "    logits = query @ positive_key.T\n",
    "\n",
    "    # The labels are the indices of the positive pairs (the diagonal)\n",
    "    labels = torch.arange(len(query), device=query.device)\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    return F.cross_entropy(logits / temperature, labels)\n",
    "\n",
    "\n",
    "class SemanticLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A loss function that measures the semantic similarity between two sets of captions\n",
    "    using a pre-trained SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Load a pre-trained model optimized for semantic similarity\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "        # Freeze the model's weights as we only use it for inference\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, predicted_captions, ground_truth_captions):\n",
    "        # Convert the text captions into semantic embedding vectors\n",
    "        pred_embeddings = self.model.encode(predicted_captions, convert_to_tensor=True)\n",
    "        gt_embeddings = self.model.encode(ground_truth_captions, convert_to_tensor=True)\n",
    "\n",
    "        # Calculate the cosine similarity. The loss is 1.0 minus the similarity.\n",
    "        # A higher similarity (closer to 1.0) results in a lower loss (closer to 0.0).\n",
    "        cosine_sim = F.cosine_similarity(pred_embeddings, gt_embeddings, dim=-1)\n",
    "        loss = 1.0 - cosine_sim.mean()\n",
    "        return loss\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "def create_and_save_reconstruction_grid(eval_samples, diffusion_pipe, epoch, val_loss, output_dir):\n",
    "    \"\"\"\n",
    "    Generates a 2x2 image grid comparing original and reconstructed images.\n",
    "    Top row: Original images with ground truth captions.\n",
    "    Bottom row: Reconstructed images with predicted captions.\n",
    "    \"\"\"\n",
    "    if len(eval_samples) < 2:\n",
    "        print(\"Not enough samples for a 2x2 grid, skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ–¼ï¸  Generating reconstruction grid for epoch {epoch}...\")\n",
    "\n",
    "    # Setup canvas and font\n",
    "    w, h = 512, 512  # Standard Stable Diffusion size\n",
    "    title_h = 60    # Space for titles\n",
    "    grid = Image.new('RGB', (w * 2, h * 2 + title_h * 2), 'black')\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 20)\n",
    "    except IOError:\n",
    "        print(\"Default font not found, using fallback.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Process two samples\n",
    "    for i in range(5):\n",
    "        predicted_cap, gt_cap, original_image = eval_samples[i]\n",
    "\n",
    "        # --- Generate the reconstructed image from the predicted caption ---\n",
    "        generator = torch.Generator(device=diffusion_pipe.device).manual_seed(42 + i)\n",
    "        reconstructed_image = diffusion_pipe(\n",
    "            prompt=predicted_cap, generator=generator, num_inference_steps=20\n",
    "        ).images[0]\n",
    "\n",
    "        # --- Paste images and draw titles ---\n",
    "        col_offset = i * w\n",
    "\n",
    "        # Top Row (Original)\n",
    "        draw.text((col_offset + 5, 5), f\"GT: {gt_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(original_image.resize((w, h)), (col_offset, title_h))\n",
    "\n",
    "        # Bottom Row (Reconstructed)\n",
    "        draw.text((col_offset + 5, h + title_h + 5), f\"Pred: {predicted_cap[:50]}...\", font=font, fill=\"white\")\n",
    "        grid.paste(reconstructed_image.resize((w, h)), (col_offset, h + title_h * 2))\n",
    "\n",
    "    # --- Save the final grid ---\n",
    "    # Sanitize loss value for filename\n",
    "    val_loss_str = f\"{val_loss:.4f}\".replace('.', '_')\n",
    "    save_path = Path(output_dir) / 'reconstructions'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    filename = save_path / f\"reconstruction_epoch_{epoch}_loss_{val_loss_str}.png\"\n",
    "    grid.save(filename)\n",
    "    print(f\"  Saved reconstruction grid to {filename}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "def train_model(config):\n",
    "    # --- Setup (Device, Output Dir, etc.) ---\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Models and Data ---\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    model = EEG_BLIP2_Model(config, device)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20250930_190044/best_model_epoch_10_loss_5.9745.pth',\n",
    "                                         map_location=device))\n",
    "        print('Loaded saved EEG_BLIP2_Model weights')\n",
    "    except:\n",
    "        print('Could not load eeg_blip2_model weights')\n",
    "\n",
    "    print(\"Loading Stable Diffusion pipeline for visualizations...\")\n",
    "    diffusion_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        config.DIFFUSION_MODEL_ID,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        safety_checker=None, requires_safety_checker=False\n",
    "    ).to(device)\n",
    "    diffusion_pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    semantic_loss_fn = SemanticLoss(device)\n",
    "\n",
    "    # --- Data Loading (with Epsilon fix) ---\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    spec_std += 1e-6\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "\n",
    "    train_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', transform, augment=True)\n",
    "    val_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', transform, augment=False)\n",
    "\n",
    "    # --- Optimizer, Scheduler, and Training Components ---\n",
    "    # NOTE: Moved outside the loop for correct initialization\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 30, \"do_sample\": True, \"min_new_tokens\": 6,\n",
    "        \"repetition_penalty\": 1.4, \"top_p\": 0.9, \"temperature\": 0.85\n",
    "    }\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    print(\"ðŸŽ¯ Starting 2-Stage Training Loop\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        # --- STAGE 1: Train Adapter Only (Epochs 0-19) ---\n",
    "        if epoch < config.STAGE1_EPOCHS:\n",
    "            if epoch == 0:\n",
    "                print(\"\\n--- STAGE 1: Training EEGNetAdapter Only ---\")\n",
    "            for param in model.eeg_encoder.adapter.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in model.blip.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # --- STAGE 2: Fine-Tune Q-Former (Epochs 20+) ---\n",
    "        elif epoch == config.STAGE1_EPOCHS:\n",
    "            print(\"\\n--- STAGE 2: Fine-Tuning Q-Former ---\")\n",
    "            for param in model.blip.qformer.parameters():\n",
    "                param.requires_grad = True\n",
    "            if hasattr(model.blip, \"language_projection\"):\n",
    "                for param in model.blip.language_projection.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(\"  - Unfroze Language Projection layer.\")\n",
    "\n",
    "        # --- Create Optimizer & Scheduler at the start of each stage ---\n",
    "        if epoch == 0 or epoch == config.STAGE1_EPOCHS:\n",
    "            print(f\"Creating optimizer for Stage {1 if epoch==0 else 2}...\")\n",
    "            adapter_params = [p for n, p in model.named_parameters() if p.requires_grad and 'adapter' in n]\n",
    "            blip_params = [p for n, p in model.named_parameters() if p.requires_grad and 'adapter' not in n]\n",
    "\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": adapter_params, \"lr\": config.ADAPTER_LR},\n",
    "                {\"params\": blip_params, \"lr\": config.BLIP_LR},\n",
    "            ]\n",
    "            optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "            # Calculate total steps for the whole training run for the scheduler\n",
    "            num_training_steps = (len(train_dataset) // config.BATCH_SIZE) * config.NUM_EPOCHS\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=300, num_training_steps=num_training_steps\n",
    "            )\n",
    "            print(\"Optimizer and scheduler created.\")\n",
    "\n",
    "        model.train()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.BATCH_SIZE,\n",
    "            sampler=SubsetRandomSampler(np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)),\n",
    "            drop_last=True, collate_fn=collate_fn\n",
    "        )\n",
    "        train_bar = tqdm(train_loader, desc=f\"ðŸ”„ Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "\n",
    "        for batch_idx, (spectrograms, pil_images, gt_captions) in enumerate(train_bar):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                    eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                    image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_captions)\n",
    "                loss = (1.0 * loss_align) + (2.5 * loss_semantic)\n",
    "                loss = loss / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            # scaler.scale() must be outside the autocast block\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimizer step logic must also be outside the autocast block\n",
    "            if (batch_idx + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), config.GRADIENT_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            train_bar.set_postfix({\"align_loss\": f\"{loss_align.item():.4f}\", \"sem_loss\": f\"{loss_semantic.item():.4f}\"})\n",
    "\n",
    "\n",
    "        print(f\"\\nðŸ” Running Validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Lists to store metrics for this epoch\n",
    "        val_align_losses = []\n",
    "        val_semantic_losses = []\n",
    "        eval_samples = [] # To store samples for printing and image generation\n",
    "\n",
    "        # Create the validation data loader\n",
    "        val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(val_indices), collate_fn=collate_fn)\n",
    "\n",
    "        # Disable gradients for the validation loop\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "            for spectrograms, pil_images, gt_captions in tqdm(val_loader, desc=\"Validation Loop\"):\n",
    "                spectrograms = spectrograms.to(device)\n",
    "\n",
    "                # --- Perform the forward pass ---\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "                image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                # Generate predicted captions from the EEG\n",
    "                generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                # --- Calculate the two primary losses ---\n",
    "                loss_align = info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "                loss_semantic = semantic_loss_fn(eeg_captions, gt_captions)\n",
    "\n",
    "                val_align_losses.append(loss_align.item())\n",
    "                val_semantic_losses.append(loss_semantic.item())\n",
    "\n",
    "                # --- Collect a few samples for visualization ---\n",
    "                if len(eval_samples) < config.VIS_GRID_SIZE:\n",
    "                    num_to_add = config.VIS_GRID_SIZE - len(eval_samples)\n",
    "                    for i in range(min(num_to_add, len(eeg_captions))):\n",
    "                        eval_samples.append((eeg_captions[i], gt_captions[i], pil_images[i]))\n",
    "\n",
    "        # --- Calculate and print average losses ---\n",
    "        avg_val_align_loss = np.mean(val_align_losses)\n",
    "        avg_val_semantic_loss = np.mean(val_semantic_losses)\n",
    "        # The total loss is a weighted sum, same as in the training loop\n",
    "        total_val_loss = (1.0 * avg_val_align_loss) + (2.5 * avg_val_semantic_loss)\n",
    "\n",
    "        print(f\"  Validation Results:\")\n",
    "        print(f\"    - Avg Alignment Loss: {avg_val_align_loss:.4f}\")\n",
    "        print(f\"    - Avg Semantic Loss:  {avg_val_semantic_loss:.4f}\")\n",
    "        print(f\"    - Total Weighted Loss: {total_val_loss:.4f}\")\n",
    "\n",
    "        # Update the learning rate scheduler based on the total validation loss\n",
    "        # scheduler.step(total_val_loss)\n",
    "\n",
    "        # --- Save the best model based on the total validation loss ---\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            save_path = output_dir / f'best_model_epoch_{epoch+1}_loss_{total_val_loss:.4f}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"ðŸ† New best model saved to {save_path}\")\n",
    "\n",
    "        # --- Print caption comparisons ---\n",
    "        if eval_samples:\n",
    "            print(\"\\n--- âœï¸  Caption Generation Samples ---\")\n",
    "            for i, (predicted_cap, gt_cap, _) in enumerate(eval_samples):\n",
    "                print(f\"Sample {i+1}:\")\n",
    "                print(f\"  - Ground Truth: {gt_cap.strip()}\")\n",
    "                print(f\"  - Predicted:    {predicted_cap.strip()}\")\n",
    "            print(\"------------------------------------\")\n",
    "\n",
    "        if (epoch + 1) % config.EVAL_IMAGE_GENERATION_INTERVAL == 0 and eval_samples:\n",
    "            create_and_save_reconstruction_grid(\n",
    "                eval_samples,\n",
    "                diffusion_pipe,\n",
    "                epoch + 1,\n",
    "                total_val_loss,\n",
    "                output_dir\n",
    "            )\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "    return output_dir\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = TRAIN_CONFIG()\n",
    "    try:\n",
    "        output_dir = train_model(config)\n",
    "        if output_dir:\n",
    "            print(f\"âœ… Training completed successfully! Results saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42u6inIygb7N"
   },
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate jiwer rouge_score sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸš€ Starting Evaluation on device: cuda ---\n",
      "Loading model from checkpoint: /content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251002_171100/best_model_epoch_28_loss_16.4225.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7612495390240edaa86d4dfe15fcb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Loading pre-generated captions from /content/val_gt_captions.json\n",
      "Loading evaluation metrics...\n",
      "Generating predictions for the entire validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fb9ba774bc43ddba080f1073203998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Captions:   0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2800 predictions.\n",
      "\n",
      "--- ðŸ“Š Computing Metrics ---\n",
      "\n",
      "\n",
      "==================================================\n",
      "      NeuroVision Model Performance Evaluation Report\n",
      "==================================================\n",
      "\n",
      "BLEU Score: 2.34\n",
      "  - Brevity Penalty (BP): 1.000 (1.0 is ideal)\n",
      "\n",
      "ROUGE Scores:\n",
      "  - ROUGE-1 (Unigram): 30.04\n",
      "  - ROUGE-2 (Bigram):  7.23\n",
      "  - ROUGE-L (LCS):     27.51\n",
      "\n",
      "Error Rates (Lower is Better):\n",
      "  - Word Error Rate (WER): 132.68%\n",
      "  - Character Error Rate (CER): 101.95%\n",
      "\n",
      "Semantic Similarity (Higher is Better):\n",
      "  - Average Cosine Similarity: 0.1045\n",
      "\n",
      "==================================================\n",
      "Evaluation Complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate # The main library for metrics\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class EVAL_CONFIG:\n",
    "    # --- IMPORTANT: Update this path to your best saved model ---\n",
    "    BEST_MODEL_PATH = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment/run_20251002_171100/best_model_epoch_28_loss_16.4225.pth'\n",
    "\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "    BATCH_SIZE = 16 # Use a reasonable batch size for inference\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. PASTE YOUR MODEL & DATASET DEFINITIONS HERE ---\n",
    "# PyTorch needs the class definitions to load the model state.\n",
    "# ==============================================================================\n",
    "\n",
    "# (Paste your final, correct EEGTransformerEncoder class here)\n",
    "class EEGTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A powerful hybrid CNN-Transformer encoder to extract rich features\n",
    "    directly from 64-channel EEG spectrograms.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=64, embed_dim=768, nhead=8, num_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Convolutional Stem: Extracts local spatio-temporal features\n",
    "        self.conv_stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim // 4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim // 4),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "        )\n",
    "\n",
    "        # 2. Transformer Encoder: Learns global relationships between features\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: [Batch, 64, Freq, Time]\n",
    "\n",
    "        # Pass through convolutional stem\n",
    "        x = self.conv_stem(x)\n",
    "        # Output x: [Batch, embed_dim, F/4, T/4]\n",
    "\n",
    "        # Prepare for transformer: flatten spatial dims and permute\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.flatten(2).permute(0, 2, 1) # -> [Batch, H*W, embed_dim]\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.output_layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE HYBRID EEG-BLIP2 MODEL ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    The final, refactored model that directly connects a powerful EEG encoder\n",
    "    to the BLIP-2 Q-Former via a projection layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, device, eeg_embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 1. Instantiate the powerful, from-scratch EEG encoder\n",
    "        self.eeg_encoder = EEGTransformerEncoder(embed_dim=eeg_embed_dim)\n",
    "\n",
    "        # 2. Load the pre-trained BLIP model\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # 3. Create the crucial projection layer\n",
    "        # This layer maps the EEG encoder's output dimension (e.g., 768) to the\n",
    "        # dimension the BLIP vision encoder's features, which the Q-Former expects (1408).\n",
    "        blip_vision_hidden_size = self.blip.vision_model.config.hidden_size\n",
    "        self.eeg_projection = nn.Linear(eeg_embed_dim, blip_vision_hidden_size)\n",
    "\n",
    "        # The __init__ method no longer handles freezing. This is now controlled by the training loop.\n",
    "\n",
    "        self.eeg_encoder.to(device)\n",
    "        self.eeg_projection.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        \"\"\" The direct EEG-to-Q-Former forward pass. \"\"\"\n",
    "        # 1. Get rich features from our custom EEG encoder\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "        # Output shape: [Batch, SeqLen, 768]\n",
    "\n",
    "        # 2. Project EEG features to match the Q-Former's expected input dimension\n",
    "        projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "        # Output shape: [Batch, SeqLen, 1408]\n",
    "\n",
    "        # 3. Pass projected features directly to the Q-Former\n",
    "        query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=projected_eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        # This function remains the same, using the standard image pathway\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # Follow the same direct pathway for generation\n",
    "            eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "            projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "\n",
    "            query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=projected_eeg_features,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs,\n",
    "                **kwargs\n",
    "            )\n",
    "            return generated_ids\n",
    "\n",
    "    def forward(self, eeg_spectrograms):\n",
    "        \"\"\"\n",
    "        Returns the EEG embedding and the raw logits from the language model.\n",
    "        \"\"\"\n",
    "        # 1. Get EEG features and project them\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "        projected_eeg_features = self.eeg_projection(eeg_features)\n",
    "\n",
    "        # 2. Get the 32 query embeddings from the Q-Former\n",
    "        query_tokens = self.blip.query_tokens.expand(projected_eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=projected_eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # 3. Project queries for the language model\n",
    "        language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "\n",
    "        # 4. Get the logits from the language model\n",
    "        outputs = self.blip.language_model(inputs_embeds=language_model_inputs, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the final EEG embedding for our alignment loss\n",
    "        eeg_embedding = query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return eeg_embedding, logits\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & COLLATING ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch):\n",
    "    spectrograms, pil_images, text_captions = zip(*[(item[0], item[1], item[2]) for item in batch])\n",
    "    labels = [item[3] for item in batch]\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return torch.stack(spectrograms), list(pil_images), list(text_captions), padded_labels\n",
    "\n",
    "\n",
    "class EEGDatasetWithCaptions(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, processor, transform, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "        # --- NEW: Define a dedicated augmentation pipeline ---\n",
    "        if self.augment:\n",
    "            self.augmentation_transform = transforms.Compose([\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "                # Randomly mask out frequency bands\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(0.1, 0.5))], p=0.5),\n",
    "                # Randomly mask out time steps\n",
    "                transforms.RandomApply([transforms.RandomErasing(p=1.0, scale=(0.02, 0.1), ratio=(2.0, 5.0))], p=0.5),\n",
    "            ])\n",
    "\n",
    "        # This is your existing normalization transform\n",
    "        self.normalization_transform = transform\n",
    "\n",
    "        # Load pre-generated captions\n",
    "        # captions_path = Path(root_dir) / f'{split}_gt_captions.json'\n",
    "        captions_path = Path('/content') / f'{split}_gt_captions.json'\n",
    "        print(f\"Loading pre-generated captions from {captions_path}\")\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "\n",
    "        spectrogram = torch.load(self.root_dir / info['spectrogram_path'])\n",
    "\n",
    "        # --- APPLY AUGMENTATIONS (for training set only) ---\n",
    "        if self.augment:\n",
    "            spectrogram = self.augmentation_transform(spectrogram)\n",
    "\n",
    "        # Apply normalization after augmentation\n",
    "        spectrogram = self.normalization_transform(spectrogram)\n",
    "\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        gt_caption = self.captions.get(str(idx), \"an image\")\n",
    "        labels = self.processor(text=gt_caption, return_tensors=\"pt\", padding=True).input_ids.squeeze()\n",
    "\n",
    "        return spectrogram, image, gt_caption, labels\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE EVALUATION SCRIPT ---\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model_performance(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"--- ðŸš€ Starting Evaluation on device: {device} ---\")\n",
    "\n",
    "    # --- Load Model and Processor ---\n",
    "    print(f\"Loading model from checkpoint: {config.BEST_MODEL_PATH}\")\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    model = EEG_BLIP2_Model(config, device)\n",
    "    model.load_state_dict(torch.load(config.BEST_MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    spec_std += 1e-6\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "\n",
    "    val_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', processor, transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Load Metric Calculators ---\n",
    "    print(\"Loading evaluation metrics...\")\n",
    "    bleu = evaluate.load('bleu')\n",
    "    rouge = evaluate.load('rouge')\n",
    "    wer = evaluate.load('wer')\n",
    "    cer = evaluate.load('cer')\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "    # --- Generation Loop ---\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    # Use deterministic beam search for formal evaluation\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 15, \"min_new_tokens\": 5,\n",
    "        \"num_beams\": 4,\n",
    "        \"repetition_penalty\": 1.4\n",
    "    }\n",
    "\n",
    "\n",
    "# \"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.85\n",
    "\n",
    "\n",
    "    print(\"Generating predictions for the entire validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, _, gt_captions, _ in tqdm(val_loader, desc=\"Generating Captions\"):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            generated_ids = model.generate(spectrograms, **generation_args)\n",
    "            predicted_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend([p.strip() for p in predicted_captions])\n",
    "            # Metrics like BLEU expect references to be a list of lists\n",
    "            all_references.extend([[g.strip()] for g in gt_captions])\n",
    "\n",
    "    print(f\"Generated {len(all_predictions)} predictions.\")\n",
    "\n",
    "    # --- Compute All Metrics ---\n",
    "    print(\"\\n--- ðŸ“Š Computing Metrics ---\")\n",
    "\n",
    "    # 1. BLEU Score\n",
    "    bleu_results = bleu.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "    # 2. ROUGE Scores\n",
    "    rouge_results = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "    # 3. Word Error Rate (WER) & Character Error Rate (CER)\n",
    "    wer_score = wer.compute(predictions=all_predictions, references=[r[0] for r in all_references])\n",
    "    cer_score = cer.compute(predictions=all_predictions, references=[r[0] for r in all_references])\n",
    "\n",
    "    # 4. Semantic Similarity\n",
    "    with torch.no_grad():\n",
    "        pred_embeddings = semantic_model.encode(all_predictions, convert_to_tensor=True)\n",
    "        ref_embeddings = semantic_model.encode([r[0] for r in all_references], convert_to_tensor=True)\n",
    "        cosine_scores = F.cosine_similarity(pred_embeddings, ref_embeddings, dim=-1)\n",
    "        avg_semantic_similarity = cosine_scores.mean().item()\n",
    "\n",
    "    # --- Print Final Report ---\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"      NeuroVision Model Performance Evaluation Report\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_results['bleu'] * 100:.2f}\")\n",
    "    print(f\"  - Brevity Penalty (BP): {bleu_results['brevity_penalty']:.3f} (1.0 is ideal)\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"  - ROUGE-1 (Unigram): {rouge_results['rouge1'] * 100:.2f}\")\n",
    "    print(f\"  - ROUGE-2 (Bigram):  {rouge_results['rouge2'] * 100:.2f}\")\n",
    "    print(f\"  - ROUGE-L (LCS):     {rouge_results['rougeL'] * 100:.2f}\")\n",
    "    print(\"\\nError Rates (Lower is Better):\")\n",
    "    print(f\"  - Word Error Rate (WER): {wer_score * 100:.2f}%\")\n",
    "    print(f\"  - Character Error Rate (CER): {cer_score * 100:.2f}%\")\n",
    "    print(\"\\nSemantic Similarity (Higher is Better):\")\n",
    "    print(f\"  - Average Cosine Similarity: {avg_semantic_similarity:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Evaluation Complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = EVAL_CONFIG()\n",
    "    evaluate_model_performance(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjX75UG3cXth"
   },
   "source": [
    "# Old scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting EEG-to-Text Training with Embedding Alignment Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1604bf95e3104aaba979cb8e88582b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff28fba3090c4ff4b2acb2cf9bc167c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Blip2VisionModel were not initialized from the model checkpoint at Salesforce/blip2-opt-2.7b and are newly initialized: ['embeddings.class_embedding', 'embeddings.patch_embedding.bias', 'embeddings.patch_embedding.weight', 'embeddings.position_embedding', 'encoder.layers.0.layer_norm1.bias', 'encoder.layers.0.layer_norm1.weight', 'encoder.layers.0.layer_norm2.bias', 'encoder.layers.0.layer_norm2.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.self_attn.projection.bias', 'encoder.layers.0.self_attn.projection.weight', 'encoder.layers.0.self_attn.qkv.bias', 'encoder.layers.0.self_attn.qkv.weight', 'encoder.layers.1.layer_norm1.bias', 'encoder.layers.1.layer_norm1.weight', 'encoder.layers.1.layer_norm2.bias', 'encoder.layers.1.layer_norm2.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.self_attn.projection.bias', 'encoder.layers.1.self_attn.projection.weight', 'encoder.layers.1.self_attn.qkv.bias', 'encoder.layers.1.self_attn.qkv.weight', 'encoder.layers.10.layer_norm1.bias', 'encoder.layers.10.layer_norm1.weight', 'encoder.layers.10.layer_norm2.bias', 'encoder.layers.10.layer_norm2.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.self_attn.projection.bias', 'encoder.layers.10.self_attn.projection.weight', 'encoder.layers.10.self_attn.qkv.bias', 'encoder.layers.10.self_attn.qkv.weight', 'encoder.layers.11.layer_norm1.bias', 'encoder.layers.11.layer_norm1.weight', 'encoder.layers.11.layer_norm2.bias', 'encoder.layers.11.layer_norm2.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.self_attn.projection.bias', 'encoder.layers.11.self_attn.projection.weight', 'encoder.layers.11.self_attn.qkv.bias', 'encoder.layers.11.self_attn.qkv.weight', 'encoder.layers.12.layer_norm1.bias', 'encoder.layers.12.layer_norm1.weight', 'encoder.layers.12.layer_norm2.bias', 'encoder.layers.12.layer_norm2.weight', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.self_attn.projection.bias', 'encoder.layers.12.self_attn.projection.weight', 'encoder.layers.12.self_attn.qkv.bias', 'encoder.layers.12.self_attn.qkv.weight', 'encoder.layers.13.layer_norm1.bias', 'encoder.layers.13.layer_norm1.weight', 'encoder.layers.13.layer_norm2.bias', 'encoder.layers.13.layer_norm2.weight', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.self_attn.projection.bias', 'encoder.layers.13.self_attn.projection.weight', 'encoder.layers.13.self_attn.qkv.bias', 'encoder.layers.13.self_attn.qkv.weight', 'encoder.layers.14.layer_norm1.bias', 'encoder.layers.14.layer_norm1.weight', 'encoder.layers.14.layer_norm2.bias', 'encoder.layers.14.layer_norm2.weight', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.self_attn.projection.bias', 'encoder.layers.14.self_attn.projection.weight', 'encoder.layers.14.self_attn.qkv.bias', 'encoder.layers.14.self_attn.qkv.weight', 'encoder.layers.15.layer_norm1.bias', 'encoder.layers.15.layer_norm1.weight', 'encoder.layers.15.layer_norm2.bias', 'encoder.layers.15.layer_norm2.weight', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.self_attn.projection.bias', 'encoder.layers.15.self_attn.projection.weight', 'encoder.layers.15.self_attn.qkv.bias', 'encoder.layers.15.self_attn.qkv.weight', 'encoder.layers.16.layer_norm1.bias', 'encoder.layers.16.layer_norm1.weight', 'encoder.layers.16.layer_norm2.bias', 'encoder.layers.16.layer_norm2.weight', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.self_attn.projection.bias', 'encoder.layers.16.self_attn.projection.weight', 'encoder.layers.16.self_attn.qkv.bias', 'encoder.layers.16.self_attn.qkv.weight', 'encoder.layers.17.layer_norm1.bias', 'encoder.layers.17.layer_norm1.weight', 'encoder.layers.17.layer_norm2.bias', 'encoder.layers.17.layer_norm2.weight', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.self_attn.projection.bias', 'encoder.layers.17.self_attn.projection.weight', 'encoder.layers.17.self_attn.qkv.bias', 'encoder.layers.17.self_attn.qkv.weight', 'encoder.layers.18.layer_norm1.bias', 'encoder.layers.18.layer_norm1.weight', 'encoder.layers.18.layer_norm2.bias', 'encoder.layers.18.layer_norm2.weight', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.self_attn.projection.bias', 'encoder.layers.18.self_attn.projection.weight', 'encoder.layers.18.self_attn.qkv.bias', 'encoder.layers.18.self_attn.qkv.weight', 'encoder.layers.19.layer_norm1.bias', 'encoder.layers.19.layer_norm1.weight', 'encoder.layers.19.layer_norm2.bias', 'encoder.layers.19.layer_norm2.weight', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.self_attn.projection.bias', 'encoder.layers.19.self_attn.projection.weight', 'encoder.layers.19.self_attn.qkv.bias', 'encoder.layers.19.self_attn.qkv.weight', 'encoder.layers.2.layer_norm1.bias', 'encoder.layers.2.layer_norm1.weight', 'encoder.layers.2.layer_norm2.bias', 'encoder.layers.2.layer_norm2.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.self_attn.projection.bias', 'encoder.layers.2.self_attn.projection.weight', 'encoder.layers.2.self_attn.qkv.bias', 'encoder.layers.2.self_attn.qkv.weight', 'encoder.layers.20.layer_norm1.bias', 'encoder.layers.20.layer_norm1.weight', 'encoder.layers.20.layer_norm2.bias', 'encoder.layers.20.layer_norm2.weight', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.self_attn.projection.bias', 'encoder.layers.20.self_attn.projection.weight', 'encoder.layers.20.self_attn.qkv.bias', 'encoder.layers.20.self_attn.qkv.weight', 'encoder.layers.21.layer_norm1.bias', 'encoder.layers.21.layer_norm1.weight', 'encoder.layers.21.layer_norm2.bias', 'encoder.layers.21.layer_norm2.weight', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.self_attn.projection.bias', 'encoder.layers.21.self_attn.projection.weight', 'encoder.layers.21.self_attn.qkv.bias', 'encoder.layers.21.self_attn.qkv.weight', 'encoder.layers.22.layer_norm1.bias', 'encoder.layers.22.layer_norm1.weight', 'encoder.layers.22.layer_norm2.bias', 'encoder.layers.22.layer_norm2.weight', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.self_attn.projection.bias', 'encoder.layers.22.self_attn.projection.weight', 'encoder.layers.22.self_attn.qkv.bias', 'encoder.layers.22.self_attn.qkv.weight', 'encoder.layers.23.layer_norm1.bias', 'encoder.layers.23.layer_norm1.weight', 'encoder.layers.23.layer_norm2.bias', 'encoder.layers.23.layer_norm2.weight', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.self_attn.projection.bias', 'encoder.layers.23.self_attn.projection.weight', 'encoder.layers.23.self_attn.qkv.bias', 'encoder.layers.23.self_attn.qkv.weight', 'encoder.layers.24.layer_norm1.bias', 'encoder.layers.24.layer_norm1.weight', 'encoder.layers.24.layer_norm2.bias', 'encoder.layers.24.layer_norm2.weight', 'encoder.layers.24.mlp.fc1.bias', 'encoder.layers.24.mlp.fc1.weight', 'encoder.layers.24.mlp.fc2.bias', 'encoder.layers.24.mlp.fc2.weight', 'encoder.layers.24.self_attn.projection.bias', 'encoder.layers.24.self_attn.projection.weight', 'encoder.layers.24.self_attn.qkv.bias', 'encoder.layers.24.self_attn.qkv.weight', 'encoder.layers.25.layer_norm1.bias', 'encoder.layers.25.layer_norm1.weight', 'encoder.layers.25.layer_norm2.bias', 'encoder.layers.25.layer_norm2.weight', 'encoder.layers.25.mlp.fc1.bias', 'encoder.layers.25.mlp.fc1.weight', 'encoder.layers.25.mlp.fc2.bias', 'encoder.layers.25.mlp.fc2.weight', 'encoder.layers.25.self_attn.projection.bias', 'encoder.layers.25.self_attn.projection.weight', 'encoder.layers.25.self_attn.qkv.bias', 'encoder.layers.25.self_attn.qkv.weight', 'encoder.layers.26.layer_norm1.bias', 'encoder.layers.26.layer_norm1.weight', 'encoder.layers.26.layer_norm2.bias', 'encoder.layers.26.layer_norm2.weight', 'encoder.layers.26.mlp.fc1.bias', 'encoder.layers.26.mlp.fc1.weight', 'encoder.layers.26.mlp.fc2.bias', 'encoder.layers.26.mlp.fc2.weight', 'encoder.layers.26.self_attn.projection.bias', 'encoder.layers.26.self_attn.projection.weight', 'encoder.layers.26.self_attn.qkv.bias', 'encoder.layers.26.self_attn.qkv.weight', 'encoder.layers.27.layer_norm1.bias', 'encoder.layers.27.layer_norm1.weight', 'encoder.layers.27.layer_norm2.bias', 'encoder.layers.27.layer_norm2.weight', 'encoder.layers.27.mlp.fc1.bias', 'encoder.layers.27.mlp.fc1.weight', 'encoder.layers.27.mlp.fc2.bias', 'encoder.layers.27.mlp.fc2.weight', 'encoder.layers.27.self_attn.projection.bias', 'encoder.layers.27.self_attn.projection.weight', 'encoder.layers.27.self_attn.qkv.bias', 'encoder.layers.27.self_attn.qkv.weight', 'encoder.layers.28.layer_norm1.bias', 'encoder.layers.28.layer_norm1.weight', 'encoder.layers.28.layer_norm2.bias', 'encoder.layers.28.layer_norm2.weight', 'encoder.layers.28.mlp.fc1.bias', 'encoder.layers.28.mlp.fc1.weight', 'encoder.layers.28.mlp.fc2.bias', 'encoder.layers.28.mlp.fc2.weight', 'encoder.layers.28.self_attn.projection.bias', 'encoder.layers.28.self_attn.projection.weight', 'encoder.layers.28.self_attn.qkv.bias', 'encoder.layers.28.self_attn.qkv.weight', 'encoder.layers.29.layer_norm1.bias', 'encoder.layers.29.layer_norm1.weight', 'encoder.layers.29.layer_norm2.bias', 'encoder.layers.29.layer_norm2.weight', 'encoder.layers.29.mlp.fc1.bias', 'encoder.layers.29.mlp.fc1.weight', 'encoder.layers.29.mlp.fc2.bias', 'encoder.layers.29.mlp.fc2.weight', 'encoder.layers.29.self_attn.projection.bias', 'encoder.layers.29.self_attn.projection.weight', 'encoder.layers.29.self_attn.qkv.bias', 'encoder.layers.29.self_attn.qkv.weight', 'encoder.layers.3.layer_norm1.bias', 'encoder.layers.3.layer_norm1.weight', 'encoder.layers.3.layer_norm2.bias', 'encoder.layers.3.layer_norm2.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.self_attn.projection.bias', 'encoder.layers.3.self_attn.projection.weight', 'encoder.layers.3.self_attn.qkv.bias', 'encoder.layers.3.self_attn.qkv.weight', 'encoder.layers.30.layer_norm1.bias', 'encoder.layers.30.layer_norm1.weight', 'encoder.layers.30.layer_norm2.bias', 'encoder.layers.30.layer_norm2.weight', 'encoder.layers.30.mlp.fc1.bias', 'encoder.layers.30.mlp.fc1.weight', 'encoder.layers.30.mlp.fc2.bias', 'encoder.layers.30.mlp.fc2.weight', 'encoder.layers.30.self_attn.projection.bias', 'encoder.layers.30.self_attn.projection.weight', 'encoder.layers.30.self_attn.qkv.bias', 'encoder.layers.30.self_attn.qkv.weight', 'encoder.layers.31.layer_norm1.bias', 'encoder.layers.31.layer_norm1.weight', 'encoder.layers.31.layer_norm2.bias', 'encoder.layers.31.layer_norm2.weight', 'encoder.layers.31.mlp.fc1.bias', 'encoder.layers.31.mlp.fc1.weight', 'encoder.layers.31.mlp.fc2.bias', 'encoder.layers.31.mlp.fc2.weight', 'encoder.layers.31.self_attn.projection.bias', 'encoder.layers.31.self_attn.projection.weight', 'encoder.layers.31.self_attn.qkv.bias', 'encoder.layers.31.self_attn.qkv.weight', 'encoder.layers.32.layer_norm1.bias', 'encoder.layers.32.layer_norm1.weight', 'encoder.layers.32.layer_norm2.bias', 'encoder.layers.32.layer_norm2.weight', 'encoder.layers.32.mlp.fc1.bias', 'encoder.layers.32.mlp.fc1.weight', 'encoder.layers.32.mlp.fc2.bias', 'encoder.layers.32.mlp.fc2.weight', 'encoder.layers.32.self_attn.projection.bias', 'encoder.layers.32.self_attn.projection.weight', 'encoder.layers.32.self_attn.qkv.bias', 'encoder.layers.32.self_attn.qkv.weight', 'encoder.layers.33.layer_norm1.bias', 'encoder.layers.33.layer_norm1.weight', 'encoder.layers.33.layer_norm2.bias', 'encoder.layers.33.layer_norm2.weight', 'encoder.layers.33.mlp.fc1.bias', 'encoder.layers.33.mlp.fc1.weight', 'encoder.layers.33.mlp.fc2.bias', 'encoder.layers.33.mlp.fc2.weight', 'encoder.layers.33.self_attn.projection.bias', 'encoder.layers.33.self_attn.projection.weight', 'encoder.layers.33.self_attn.qkv.bias', 'encoder.layers.33.self_attn.qkv.weight', 'encoder.layers.34.layer_norm1.bias', 'encoder.layers.34.layer_norm1.weight', 'encoder.layers.34.layer_norm2.bias', 'encoder.layers.34.layer_norm2.weight', 'encoder.layers.34.mlp.fc1.bias', 'encoder.layers.34.mlp.fc1.weight', 'encoder.layers.34.mlp.fc2.bias', 'encoder.layers.34.mlp.fc2.weight', 'encoder.layers.34.self_attn.projection.bias', 'encoder.layers.34.self_attn.projection.weight', 'encoder.layers.34.self_attn.qkv.bias', 'encoder.layers.34.self_attn.qkv.weight', 'encoder.layers.35.layer_norm1.bias', 'encoder.layers.35.layer_norm1.weight', 'encoder.layers.35.layer_norm2.bias', 'encoder.layers.35.layer_norm2.weight', 'encoder.layers.35.mlp.fc1.bias', 'encoder.layers.35.mlp.fc1.weight', 'encoder.layers.35.mlp.fc2.bias', 'encoder.layers.35.mlp.fc2.weight', 'encoder.layers.35.self_attn.projection.bias', 'encoder.layers.35.self_attn.projection.weight', 'encoder.layers.35.self_attn.qkv.bias', 'encoder.layers.35.self_attn.qkv.weight', 'encoder.layers.36.layer_norm1.bias', 'encoder.layers.36.layer_norm1.weight', 'encoder.layers.36.layer_norm2.bias', 'encoder.layers.36.layer_norm2.weight', 'encoder.layers.36.mlp.fc1.bias', 'encoder.layers.36.mlp.fc1.weight', 'encoder.layers.36.mlp.fc2.bias', 'encoder.layers.36.mlp.fc2.weight', 'encoder.layers.36.self_attn.projection.bias', 'encoder.layers.36.self_attn.projection.weight', 'encoder.layers.36.self_attn.qkv.bias', 'encoder.layers.36.self_attn.qkv.weight', 'encoder.layers.37.layer_norm1.bias', 'encoder.layers.37.layer_norm1.weight', 'encoder.layers.37.layer_norm2.bias', 'encoder.layers.37.layer_norm2.weight', 'encoder.layers.37.mlp.fc1.bias', 'encoder.layers.37.mlp.fc1.weight', 'encoder.layers.37.mlp.fc2.bias', 'encoder.layers.37.mlp.fc2.weight', 'encoder.layers.37.self_attn.projection.bias', 'encoder.layers.37.self_attn.projection.weight', 'encoder.layers.37.self_attn.qkv.bias', 'encoder.layers.37.self_attn.qkv.weight', 'encoder.layers.38.layer_norm1.bias', 'encoder.layers.38.layer_norm1.weight', 'encoder.layers.38.layer_norm2.bias', 'encoder.layers.38.layer_norm2.weight', 'encoder.layers.38.mlp.fc1.bias', 'encoder.layers.38.mlp.fc1.weight', 'encoder.layers.38.mlp.fc2.bias', 'encoder.layers.38.mlp.fc2.weight', 'encoder.layers.38.self_attn.projection.bias', 'encoder.layers.38.self_attn.projection.weight', 'encoder.layers.38.self_attn.qkv.bias', 'encoder.layers.38.self_attn.qkv.weight', 'encoder.layers.4.layer_norm1.bias', 'encoder.layers.4.layer_norm1.weight', 'encoder.layers.4.layer_norm2.bias', 'encoder.layers.4.layer_norm2.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.self_attn.projection.bias', 'encoder.layers.4.self_attn.projection.weight', 'encoder.layers.4.self_attn.qkv.bias', 'encoder.layers.4.self_attn.qkv.weight', 'encoder.layers.5.layer_norm1.bias', 'encoder.layers.5.layer_norm1.weight', 'encoder.layers.5.layer_norm2.bias', 'encoder.layers.5.layer_norm2.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.self_attn.projection.bias', 'encoder.layers.5.self_attn.projection.weight', 'encoder.layers.5.self_attn.qkv.bias', 'encoder.layers.5.self_attn.qkv.weight', 'encoder.layers.6.layer_norm1.bias', 'encoder.layers.6.layer_norm1.weight', 'encoder.layers.6.layer_norm2.bias', 'encoder.layers.6.layer_norm2.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.self_attn.projection.bias', 'encoder.layers.6.self_attn.projection.weight', 'encoder.layers.6.self_attn.qkv.bias', 'encoder.layers.6.self_attn.qkv.weight', 'encoder.layers.7.layer_norm1.bias', 'encoder.layers.7.layer_norm1.weight', 'encoder.layers.7.layer_norm2.bias', 'encoder.layers.7.layer_norm2.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.self_attn.projection.bias', 'encoder.layers.7.self_attn.projection.weight', 'encoder.layers.7.self_attn.qkv.bias', 'encoder.layers.7.self_attn.qkv.weight', 'encoder.layers.8.layer_norm1.bias', 'encoder.layers.8.layer_norm1.weight', 'encoder.layers.8.layer_norm2.bias', 'encoder.layers.8.layer_norm2.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.self_attn.projection.bias', 'encoder.layers.8.self_attn.projection.weight', 'encoder.layers.8.self_attn.qkv.bias', 'encoder.layers.8.self_attn.qkv.weight', 'encoder.layers.9.layer_norm1.bias', 'encoder.layers.9.layer_norm1.weight', 'encoder.layers.9.layer_norm2.bias', 'encoder.layers.9.layer_norm2.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.self_attn.projection.bias', 'encoder.layers.9.self_attn.projection.weight', 'encoder.layers.9.self_attn.qkv.bias', 'encoder.layers.9.self_attn.qkv.weight', 'post_layernorm.bias', 'post_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8e20058e040699745f6567e36d588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing specified layers for fine-tuning...\n",
      "  - Unfroze Language Projection layer.\n",
      "  - Unfroze the last Vision Model layer.\n",
      "  - Unfroze the last Language Model layer.\n",
      "ðŸŽ¯ Starting Training Loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-779291635.py:263: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec93f77a95bc4aa5a4b11749770a23a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 1/50 [TRAIN]:   0%|          | 0/1750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running Validation & Visualization for epoch 1\n",
      "  Validation Loss: 2.2821\n",
      "ðŸ† New best model saved with validation loss: 2.2821\n",
      "\n",
      "--- Caption Generation Samples ---\n",
      "Sample 1:\n",
      "  - Ground Truth: a man with a donut in his mouth\n",
      "  - Predicted:    david taylor in the desert. all rights reserved 2012 dalton brown\n",
      "Sample 2:\n",
      "  - Ground Truth: a pair of scissors\n",
      "  - Predicted:    color of the day with a blue door\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d4fbae55864214bd8e33b735781064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ”„ Epoch 2/50 [TRAIN]:   0%|          | 0/1750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-779291635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_CONFIG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Training completed successfully! Results saved to: {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-779291635.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_align\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcse_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAD_ACCUMULATION_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAD_ACCUMULATION_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "import warnings\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment'\n",
    "\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUMULATION_STEPS = 16\n",
    "    NUM_EPOCHS = 50\n",
    "    LR = 5e-5\n",
    "\n",
    "    WEIGHT_DECAY = 5e-3\n",
    "    GRADIENT_CLIP_NORM = 1.0\n",
    "\n",
    "    VALIDATION_INTERVAL = 1\n",
    "    VISUALIZATION_INTERVAL = 1 # Note: Visualization happens with validation\n",
    "    VAL_SAMPLES_PER_EPOCH = 700\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 3500\n",
    "    VIS_GRID_SIZE = 4\n",
    "\n",
    "    ALPHA_ALIGN = 1.0      # weight for cosine alignment\n",
    "    BETA_CE     = 5.0      # weight for cross-entropy (strong)\n",
    "    GAMMA_EOS   = 10.0\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER MODULE ---\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAdapter(nn.Module):\n",
    "    \"\"\"Lightweight adapter to turn 64-channel spectrogram -> 3-channel-like input for BLIP vision.\"\"\"\n",
    "    def __init__(self, in_chans=64, mid_ch=64, out_ch=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, mid_ch, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.act = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, config, in_chans=64):\n",
    "        super().__init__()\n",
    "        self.vision_model = Blip2VisionModel.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # Freeze the original vision model\n",
    "        for param in self.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.adapter = ChannelAdapter(in_chans=in_chans, mid_ch=64, out_ch=3)\n",
    "\n",
    "        for p in self.adapter.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      adapted_x = self.adapter(x)\n",
    "      vision_outputs = self.vision_model(adapted_x, return_dict=True)\n",
    "      return vision_outputs.last_hidden_state\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. THE HYBRID EEG-BLIP2 MODEL ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.eeg_encoder = EEGEncoder(config)\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            config.BLIP_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        # Freeze the entire BLIP model initially\n",
    "        for param in self.blip.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(\"Unfreezing specified layers for fine-tuning...\")\n",
    "        # Unfreeze the language projection layer (connects Q-Former to LLM)\n",
    "        if hasattr(self.blip, \"language_projection\"):\n",
    "            for p in self.blip.language_projection.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            print(\"  - Unfroze Language Projection layer.\")\n",
    "\n",
    "        # Unfreeze the last layer(s) of the Q-Former for fine-tuning\n",
    "        try:\n",
    "            qenc_layers = self.blip.qformer.encoder.layer\n",
    "            n_unfreeze_qformer = 2\n",
    "            for idx in range(len(qenc_layers) - n_unfreeze_qformer, len(qenc_layers)):\n",
    "                for p in qenc_layers[idx].parameters():\n",
    "                    p.requires_grad = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            vision_layers = self.blip.vision_model.encoder.layers\n",
    "            for p in vision_layers[-2].parameters():\n",
    "                p.requires_grad = True\n",
    "            print(\"  - Unfroze the last Vision Model layer.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not unfreeze Vision Model layer: {e}\")\n",
    "\n",
    "        # 4. Unfreeze the last Language Model layer\n",
    "        try:\n",
    "            # Note: The exact path might differ slightly between transformers versions\n",
    "            llm_layers = self.blip.language_model.model.decoder.layers\n",
    "            for p in llm_layers[-2].parameters():\n",
    "                p.requires_grad = True\n",
    "            print(\"  - Unfroze the last Language Model layer.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not unfreeze Language Model layer: {e}\")\n",
    "\n",
    "\n",
    "        self.eeg_encoder.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "        query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=eeg_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_features,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            eeg_features = self.eeg_encoder(eeg_spectrograms)\n",
    "            query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=eeg_features,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs,\n",
    "                **kwargs\n",
    "            )\n",
    "            return generated_ids\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & COLLATING ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])\n",
    "    pil_images = [item[1] for item in batch]\n",
    "    return spectrograms, pil_images\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, transform):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "        spectrogram = self.transform(torch.load(self.root_dir / info['spectrogram_path']))\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        return spectrogram, image\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN TRAINING FUNCTION ---\n",
    "# ==============================================================================\n",
    "# InfoNCE loss function\n",
    "def info_nce_loss(query, positive_key, negative_keys=None, temperature=0.07):\n",
    "    # Ensure inputs are normalized\n",
    "    query = nn.functional.normalize(query, dim=-1)\n",
    "    positive_key = nn.functional.normalize(positive_key, dim=-1)\n",
    "\n",
    "    # Calculate positive logits\n",
    "    positive_logits = torch.sum(query * positive_key, dim=-1, keepdim=True)\n",
    "\n",
    "    # Concatenate all other items in the batch as negative keys\n",
    "    if negative_keys is None:\n",
    "        negative_keys = positive_key\n",
    "\n",
    "    negative_logits = query @ negative_keys.T\n",
    "\n",
    "    # Mask out self-comparison for the InfoNCE formula\n",
    "    logits = torch.cat([positive_logits, negative_logits], dim=1)\n",
    "\n",
    "    # Create labels for cross-entropy: first element is the positive pair\n",
    "    labels = torch.zeros(len(query), dtype=torch.long, device=query.device)\n",
    "\n",
    "    return nn.functional.cross_entropy(logits / temperature, labels)\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "def train_model(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"ðŸš€ Starting EEG-to-Text Training with Embedding Alignment Loss\")\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    student_model = EEG_BLIP2_Model(config, device)\n",
    "\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "    train_dataset = EEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', transform)\n",
    "    val_dataset = EEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', transform)\n",
    "\n",
    "    val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(val_indices), collate_fn=collate_fn)\n",
    "\n",
    "    fixed_spectrograms, fixed_pil_images = next(iter(val_loader))\n",
    "    fixed_spectrograms = fixed_spectrograms.to(device)\n",
    "\n",
    "    # OPTIMIZED: Only pass trainable parameters to the optimizer\n",
    "    trainable_params = filter(lambda p: p.requires_grad, student_model.parameters())\n",
    "    optimizer = optim.AdamW(trainable_params, lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=1e-7)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, min_lr=1e-7)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    loss_fn = nn.CosineEmbeddingLoss()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"ðŸŽ¯ Starting Training Loop\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        student_model.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(train_indices), drop_last=True, collate_fn=collate_fn)\n",
    "        train_bar = tqdm(train_loader, desc=f\"ðŸ”„ Epoch {epoch+1}/{config.NUM_EPOCHS} [TRAIN]\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, (spectrograms, pil_images) in enumerate(train_bar):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embedding = student_model.get_eeg_embedding(spectrograms)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_embedding = student_model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                target = torch.ones(eeg_embedding.size(0)).to(device)\n",
    "                loss_align = info_nce_loss(eeg_embedding, image_embedding, temperature=0.07)\n",
    "                cse_loss = loss_fn(eeg_embedding, image_embedding, target)\n",
    "\n",
    "                # CORRECTED: Was `loss = loss / ...` but `loss` wasn't defined. Use `loss_align`.\n",
    "                loss = (2.0 * loss_align + cse_loss) / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), config.GRADIENT_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # IMPROVED: Show the actual alignment loss, not the scaled version\n",
    "            train_bar.set_postfix(align_loss=f\"{loss_align.item():.4f}\")\n",
    "\n",
    "        if (epoch + 1) % config.VALIDATION_INTERVAL == 0:\n",
    "            print(f\"\\nðŸ” Running Validation & Visualization for epoch {epoch+1}\")\n",
    "            student_model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "                for spectrograms, pil_images in val_loader:\n",
    "                    spectrograms = spectrograms.to(device)\n",
    "                    eeg_embedding = student_model.get_eeg_embedding(spectrograms)\n",
    "                    image_embedding = student_model.get_image_embedding(pil_images, processor)\n",
    "                    target = torch.ones(eeg_embedding.size(0)).to(device)\n",
    "                    cse_loss = loss_fn(eeg_embedding, image_embedding, target).item()\n",
    "                    loss_align = info_nce_loss(eeg_embedding, image_embedding, temperature=0.07)\n",
    "                    loss = 2.0 * loss_align + cse_loss\n",
    "                    val_loss += loss\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                # Save only the trainable part of the model: the EEG encoder and fine-tuned layers\n",
    "                torch.save(student_model.state_dict(), output_dir / 'best_model.pth')\n",
    "                print(f\"ðŸ† New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "            with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "                generated_ids = student_model.generate(\n",
    "                    fixed_spectrograms[:config.VIS_GRID_SIZE],\n",
    "                    do_sample=True, temperature=1.0, top_p=0.9,\n",
    "                    min_new_tokens=8, max_new_tokens=32, repetition_penalty=1.5\n",
    "                    # num_beams=4\n",
    "                )\n",
    "                predicted_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                # Get ground truth captions\n",
    "                # CORRECTED: Removed explicit .to(dtype=torch.float16) for robustness\n",
    "                gt_pixel_values = processor(images=fixed_pil_images, return_tensors=\"pt\").to(device).pixel_values\n",
    "                gt_captions_ids = student_model.blip.generate(gt_pixel_values)\n",
    "                gt_captions = processor.batch_decode(gt_captions_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "                print(\"\\n--- Caption Generation Samples ---\")\n",
    "                for i in range(len(predicted_captions)):\n",
    "                    print(f\"Sample {i+1}:\")\n",
    "                    print(f\"  - Ground Truth: {gt_captions[i].strip()}\")\n",
    "                    print(f\"  - Predicted:    {predicted_captions[i].strip()}\")\n",
    "                print(\"------------------------------------\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "    return output_dir\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = TRAIN_CONFIG()\n",
    "    try:\n",
    "        output_dir = train_model(config)\n",
    "        if output_dir:\n",
    "            print(f\"âœ… Training completed successfully! Results saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpZUSSiH3Wxn"
   },
   "source": [
    "# Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import clip\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION (MERGED) ---\n",
    "# ==============================================================================\n",
    "class ENHANCED_CONFIG:\n",
    "    # --- Paths and Models ---\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/EEG_BLIP2_Alignment_Stable'\n",
    "    BLIP_MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "    CLIP_MODEL_NAME = \"ViT-B/32\"\n",
    "    DIFFUSION_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "    # --- Stable Hyperparameters ---\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 1e-5\n",
    "    WEIGHT_DECAY = 5e-3\n",
    "    GRADIENT_CLIP_NORM = 1.0\n",
    "    GRAD_ACCUMULATION_STEPS = 1\n",
    "\n",
    "    # --- Training Loop Settings ---\n",
    "    NUM_EPOCHS = 50\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 3500\n",
    "    VAL_SAMPLES_PER_EPOCH = 700\n",
    "\n",
    "    # --- New Loss Weights ---\n",
    "    EMBEDDING_WEIGHT = 1.0\n",
    "    CAPTION_SIM_WEIGHT = 3.0\n",
    "    CLIP_ALIGNMENT_WEIGHT = 0.25\n",
    "\n",
    "    # --- Evaluation Settings ---\n",
    "    EVAL_IMAGE_GENERATION_INTERVAL = 1\n",
    "    EVAL_SAMPLES_TO_GENERATE = 3 # How many images to generate each evaluation\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. STABLE EEG ENCODER MODULE (FROM OLD SCRIPT) ---\n",
    "# ==============================================================================\n",
    "class ChannelAdapter(nn.Module):\n",
    "    def __init__(self, in_chans=64, mid_ch=64, out_ch=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans, mid_ch, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.act = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified, stable encoder that ONLY contains the adapter.\n",
    "    It receives the vision model during its forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=64):\n",
    "        super().__init__()\n",
    "        self.adapter = ChannelAdapter(in_chans=in_chans)\n",
    "\n",
    "    def forward(self, x, vision_model):\n",
    "      # Takes the main vision model as an argument\n",
    "      adapted_x = self.adapter(x)\n",
    "      vision_outputs = vision_model(adapted_x, return_dict=True)\n",
    "      return vision_outputs.last_hidden_state\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. STABLE HYBRID EEG-BLIP2 MODEL (FROM OLD SCRIPT) ---\n",
    "# ==============================================================================\n",
    "class EEG_BLIP2_Model(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.eeg_encoder = EEGEncoder() # Now much simpler\n",
    "        self.blip = Blip2ForConditionalGeneration.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "\n",
    "        # Freeze the entire BLIP model initially\n",
    "        for param in self.blip.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(\"Unfreezing specified layers for fine-tuning...\")\n",
    "        # Unfreeze the language projection layer (connects Q-Former to LLM)\n",
    "        if hasattr(self.blip, \"language_projection\"):\n",
    "            for p in self.blip.language_projection.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            print(\"  - Unfroze Language Projection layer.\")\n",
    "\n",
    "        # Unfreeze the last layer(s) of the Q-Former for fine-tuning\n",
    "        try:\n",
    "            qenc_layers = self.blip.qformer.encoder.layer\n",
    "            n_unfreeze_qformer = 4\n",
    "            for idx in range(len(qenc_layers) - n_unfreeze_qformer, len(qenc_layers)):\n",
    "                for p in qenc_layers[idx].parameters():\n",
    "                    p.requires_grad = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            vision_layers = self.blip.vision_model.encoder.layers\n",
    "            for p in vision_layers[-4].parameters():\n",
    "                p.requires_grad = True\n",
    "            print(\"  - Unfroze the last Vision Model layer.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not unfreeze Vision Model layer: {e}\")\n",
    "\n",
    "        # 4. Unfreeze the last Language Model layer\n",
    "        try:\n",
    "            # Note: The exact path might differ slightly between transformers versions\n",
    "            llm_layers = self.blip.language_model.model.decoder.layers\n",
    "            for p in llm_layers[-4].parameters():\n",
    "                p.requires_grad = True\n",
    "            print(\"  - Unfroze the last Language Model layer.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not unfreeze Language Model layer: {e}\")\n",
    "\n",
    "        self.eeg_encoder.to(device)\n",
    "        self.blip.to(device)\n",
    "\n",
    "    def get_eeg_embedding(self, eeg_spectrograms):\n",
    "        # Pass the single, authoritative vision model to the encoder\n",
    "        eeg_features = self.eeg_encoder(eeg_spectrograms, self.blip.vision_model)\n",
    "\n",
    "        query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens, encoder_hidden_states=eeg_features, return_dict=True)\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def get_image_embedding(self, pil_images, processor):\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs.pixel_values\n",
    "\n",
    "        # Use the single, authoritative vision model\n",
    "        image_features = self.blip.vision_model(pixel_values).last_hidden_state\n",
    "        query_tokens = self.blip.query_tokens.expand(image_features.shape[0], -1, -1)\n",
    "        query_outputs = self.blip.qformer(\n",
    "            query_embeds=query_tokens, encoder_hidden_states=image_features, return_dict=True)\n",
    "        return query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def generate(self, eeg_spectrograms, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # Pass the single, authoritative vision model to the encoder\n",
    "            eeg_features = self.eeg_encoder(eeg_spectrograms, self.blip.vision_model)\n",
    "\n",
    "            query_tokens = self.blip.query_tokens.expand(eeg_features.shape[0], -1, -1)\n",
    "            query_outputs = self.blip.qformer(\n",
    "                query_embeds=query_tokens, encoder_hidden_states=eeg_features, return_dict=True)\n",
    "            language_model_inputs = self.blip.language_projection(query_outputs.last_hidden_state)\n",
    "            generated_ids = self.blip.language_model.generate(\n",
    "                inputs_embeds=language_model_inputs, **kwargs)\n",
    "            return generated_ids\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET & HELPERS (MERGED) ---\n",
    "# ==============================================================================\n",
    "class EEGDatasetWithCaptions(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, transform):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "\n",
    "        captions_path = Path(root_dir) / f'{split}_gt_captions.json'\n",
    "        if not captions_path.exists():\n",
    "            raise FileNotFoundError(f\"Caption file not found: {captions_path}. Please run the caption generation script first.\")\n",
    "        print(f\"Loading pre-generated captions from {captions_path}\")\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "        spectrogram = self.transform(torch.load(self.root_dir / info['spectrogram_path']))\n",
    "        image = Image.open(self.root_dir / info['image_path']).convert(\"RGB\")\n",
    "        gt_caption = self.captions.get(str(idx), \"an image\")\n",
    "        return spectrogram, image, gt_caption\n",
    "\n",
    "def collate_fn(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])\n",
    "    pil_images = [item[1] for item in batch]\n",
    "    captions = [item[2] for item in batch]\n",
    "    return spectrograms, pil_images, captions\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. ENHANCED LOSS FUNCTIONS (FROM NEW SCRIPT) ---\n",
    "# ==============================================================================\n",
    "class CLIPAlignmentLoss(nn.Module):\n",
    "    def __init__(self, device, model_name=\"ViT-B/32\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.clip_model, self.clip_preprocess = clip.load(model_name, device=device)\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        if isinstance(images[0], Image.Image):\n",
    "            image_inputs = torch.stack([self.clip_preprocess(img) for img in images]).to(self.device)\n",
    "        else:\n",
    "            image_inputs = images.to(self.device)\n",
    "        text_inputs = clip.tokenize(texts, truncate=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image_inputs)\n",
    "            text_features = self.clip_model.encode_text(text_inputs)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        similarity = torch.sum(image_features * text_features, dim=-1)\n",
    "        return -similarity.mean()\n",
    "\n",
    "class ComprehensiveLoss(nn.Module):\n",
    "    def __init__(self, device, config):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "        self.clip_loss = CLIPAlignmentLoss(device, config.CLIP_MODEL_NAME)\n",
    "        self.cosine_loss = nn.CosineEmbeddingLoss()\n",
    "        for param in self.sentence_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def info_nce_loss(self, query, positive_key, temperature=0.07):\n",
    "        query = F.normalize(query, dim=-1)\n",
    "        positive_key = F.normalize(positive_key, dim=-1)\n",
    "        positive_logits = torch.sum(query * positive_key, dim=-1, keepdim=True)\n",
    "        negative_logits = query @ positive_key.T\n",
    "        logits = torch.cat([positive_logits, negative_logits], dim=1) / temperature\n",
    "        labels = torch.zeros(len(query), dtype=torch.long, device=self.device)\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def forward(self, eeg_embeddings, image_embeddings, eeg_captions, gt_captions, pil_images):\n",
    "        losses = {}\n",
    "        eeg_embeddings = F.normalize(eeg_embeddings, dim=-1)\n",
    "        image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "\n",
    "        losses['embedding_contrastive'] = self.info_nce_loss(eeg_embeddings, image_embeddings)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eeg_caption_embeddings = self.sentence_model.encode(eeg_captions, convert_to_tensor=True)\n",
    "            gt_caption_embeddings = self.sentence_model.encode(gt_captions, convert_to_tensor=True)\n",
    "\n",
    "        caption_sim = F.cosine_similarity(eeg_caption_embeddings, gt_caption_embeddings, dim=-1)\n",
    "        losses['caption_similarity'] = 1.0 - caption_sim.mean()\n",
    "\n",
    "        losses['clip_alignment'] = self.clip_loss(pil_images, eeg_captions)\n",
    "\n",
    "        total_loss = (\n",
    "            self.config.EMBEDDING_WEIGHT * losses['embedding_contrastive'] +\n",
    "            self.config.CAPTION_SIM_WEIGHT * losses['caption_similarity'] +\n",
    "            self.config.CLIP_ALIGNMENT_WEIGHT * losses['clip_alignment']\n",
    "        )\n",
    "        losses['total'] = total_loss\n",
    "        return losses\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 6. IMAGE EVALUATION (FROM NEW SCRIPT) ---\n",
    "# ==============================================================================\n",
    "class ImageGenerationEvaluator:\n",
    "    def __init__(self, device, config):\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        print(\"Loading Stable Diffusion pipeline...\")\n",
    "        self.diffusion_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            config.DIFFUSION_MODEL_ID,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False\n",
    "        ).to(device)\n",
    "        self.diffusion_pipe.set_progress_bar_config(disable=True)\n",
    "        self.eval_dir = Path(config.OUTPUT_DIR) / 'generated_images'\n",
    "        self.eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def generate_and_save_samples(self, eeg_captions, original_images, epoch):\n",
    "        print(f\"\\nðŸ–¼ï¸  Generating {len(eeg_captions)} images for evaluation...\")\n",
    "        for i, (caption, original_image) in enumerate(zip(eeg_captions, original_images)):\n",
    "            generator = torch.Generator(device=self.device).manual_seed(42 + i)\n",
    "            generated_image = self.diffusion_pipe(\n",
    "                prompt=caption, generator=generator, num_inference_steps=20\n",
    "            ).images[0]\n",
    "\n",
    "            comparison = Image.new('RGB', (1024, 512))\n",
    "            comparison.paste(original_image.resize((512, 512)), (0, 0))\n",
    "            comparison.paste(generated_image.resize((512, 512)), (512, 0))\n",
    "\n",
    "            comparison_path = self.eval_dir / f'epoch_{epoch}_sample_{i}.png'\n",
    "            comparison.save(comparison_path)\n",
    "            print(f\"  Saved sample {i+1} to {comparison_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 7. MAIN TRAINING FUNCTION (MERGED AND CORRECTED) ---\n",
    "# ==============================================================================\n",
    "def train_model(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"ðŸš€ Starting Merged EEG-to-Text Training\")\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(config.BLIP_MODEL_NAME)\n",
    "    model = EEG_BLIP2_Model(config, device)\n",
    "\n",
    "    generation_args = {\n",
    "    \"max_new_tokens\": 30,  # Generate up to 30 new words\n",
    "    \"min_length\": 8,       # Generate at least 8 words\n",
    "    \"num_beams\": 3,        # Use 3 beams for higher quality search\n",
    "    \"do_sample\": False     # Use deterministic generation for loss calculation\n",
    "    }\n",
    "\n",
    "    comprehensive_loss = ComprehensiveLoss(device, config)\n",
    "    image_evaluator = ImageGenerationEvaluator(device, config)\n",
    "\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    spec_std += 1e-6\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()),\n",
    "    ])\n",
    "\n",
    "    train_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', transform)\n",
    "    val_dataset = EEGDatasetWithCaptions(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', transform)\n",
    "\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.AdamW(trainable_params, lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, min_lr=1e-7)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    simple_alignment_loss = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    print(\"ðŸŽ¯ Starting Training Loop\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(train_indices), drop_last=True, collate_fn=collate_fn)\n",
    "        train_bar = tqdm(train_loader, desc=f\"ðŸ”„ Epoch {epoch+1}/{config.NUM_EPOCHS} [TRAIN]\")\n",
    "\n",
    "        for batch_idx, (spectrograms, pil_images, gt_captions) in enumerate(train_bar):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "\n",
    "                if epoch < 10: # STAGE 1: Foundational Alignment\n",
    "                    target = torch.ones(eeg_embeddings.size(0)).to(device)\n",
    "                    loss = simple_alignment_loss(eeg_embeddings, image_embeddings, target)\n",
    "                else: # STAGE 2: Full Comprehensive Loss\n",
    "                    with torch.no_grad():\n",
    "                        generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                        eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                    loss_dict = comprehensive_loss(eeg_embeddings, image_embeddings, eeg_captions, gt_captions, pil_images)\n",
    "                    loss = loss_dict['total'] / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    config.GRADIENT_CLIP_NORM\n",
    "                )\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item() * config.GRAD_ACCUMULATION_STEPS)\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        print(f\"\\nðŸ” Running Validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        eval_samples = [] # To store samples for image generation\n",
    "\n",
    "        val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(val_indices), collate_fn=collate_fn)\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "            for spectrograms, pil_images, gt_captions in val_loader:\n",
    "                spectrograms = spectrograms.to(device)\n",
    "                eeg_embeddings = model.get_eeg_embedding(spectrograms)\n",
    "                image_embeddings = model.get_image_embedding(pil_images, processor)\n",
    "                generated_ids = model.generate(spectrograms, **generation_args)\n",
    "                eeg_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                # loss_dict = comprehensive_loss(\n",
    "                #     eeg_embeddings, image_embeddings, eeg_captions, gt_captions, pil_images)\n",
    "                # val_losses.append(loss_dict['total'].item())\n",
    "                target = torch.ones(eeg_embeddings.size(0)).to(device)\n",
    "                val_losses.append(simple_alignment_loss(eeg_embeddings, image_embeddings, target).item())\n",
    "\n",
    "                # Collect samples for image generation\n",
    "                if len(eval_samples) < config.EVAL_SAMPLES_TO_GENERATE:\n",
    "                    for i in range(len(eeg_captions)):\n",
    "                        if len(eval_samples) < config.EVAL_SAMPLES_TO_GENERATE:\n",
    "                            eval_samples.append((eeg_captions[i], gt_captions[i], pil_images[i]))\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), output_dir / 'best_model.pth')\n",
    "            print(f\"ðŸ† New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        if eval_samples:\n",
    "            print(\"\\n--- âœï¸  Caption Generation Samples ---\")\n",
    "            for i, (predicted_cap, gt_cap, _) in enumerate(eval_samples):\n",
    "              print(f\"Sample {i+1}:\")\n",
    "              print(f\"  - Ground Truth: {gt_cap.strip()}\")\n",
    "              print(f\"  - Predicted:    {predicted_cap.strip()}\")\n",
    "              print(\"------------------------------------\")\n",
    "\n",
    "        # --- Image Generation Evaluation ---\n",
    "        if (epoch + 1) % config.EVAL_IMAGE_GENERATION_INTERVAL == 0 and eval_samples:\n",
    "            eval_captions,  _, eval_images = zip(*eval_samples)\n",
    "            image_evaluator.generate_and_save_samples(list(eval_captions), list(eval_images), epoch + 1)\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "    return output_dir\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # It's recommended to run caption generation separately first.\n",
    "    # For now, we assume the JSON files exist.\n",
    "    config = ENHANCED_CONFIG()\n",
    "    try:\n",
    "        output_dir = train_model(config)\n",
    "        if output_dir:\n",
    "            print(f\"âœ… Training completed successfully! Results saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_cache_captions(config, processor, blip_model, device):\n",
    "    \"\"\"\n",
    "    A one-time function to generate and save captions for train and val splits.\n",
    "    \"\"\"\n",
    "    for split in ['train', 'val']:\n",
    "        captions_path = Path(config.PROCESSED_DATA_ROOT) / f'{split}_gt_captions.json'\n",
    "        if captions_path.exists():\n",
    "            print(f\"Captions for '{split}' split already exist at {captions_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating ground truth captions for '{split}' split...\")\n",
    "        df = pd.read_csv(config.METADATA_CSV)\n",
    "        split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "\n",
    "        captions = {}\n",
    "        batch_size = 16\n",
    "        blip_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(split_df), batch_size), desc=f\"Generating {split} captions\"):\n",
    "                batch_info = split_df.iloc[i:i+batch_size]\n",
    "                batch_images = [\n",
    "                    Image.open(Path(config.PROCESSED_DATA_ROOT) / info['image_path']).convert(\"RGB\")\n",
    "                    for _, info in batch_info.iterrows()\n",
    "                ]\n",
    "\n",
    "                inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "                generated_ids = blip_model.generate(**inputs, max_length=50)\n",
    "                batch_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                for j, caption in enumerate(batch_captions):\n",
    "                    captions[str(i + j)] = caption.strip()\n",
    "\n",
    "        with open(captions_path, 'w') as f:\n",
    "            json.dump(captions, f, indent=2)\n",
    "        print(f\"âœ… Saved {len(captions)} captions for '{split}' split to {captions_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
