{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ajinkya-18/NeuroVision/blob/main/neurovision4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2XELXdf3TZm"
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d84138d27f44c5b6763b46ee89a25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train2017.zip:   0%|          | 0.00/18.0G [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.signal import butter, sosfiltfilt, spectrogram, iirnotch, tf2sos\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class CONFIG_FINAL_PREP:\n",
    "    HUGGINGFACE_DATASET = \"Alljoined/05_125\"\n",
    "    COCO_TRAIN_URL = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "    COCO_VAL_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    TEMP_DIR = \"/content/coco_temp\"\n",
    "    TRAIN_ZIP_PATH = os.path.join(TEMP_DIR, \"train2017.zip\")\n",
    "    VAL_ZIP_PATH = os.path.join(TEMP_DIR, \"val2017.zip\")\n",
    "    EXTRACT_DIR = os.path.join(TEMP_DIR, \"coco_full_unzipped\")\n",
    "\n",
    "    # --- DEFINITIVE FIX: All processing will happen on the fast local disk first ---\n",
    "    LOCAL_OUTPUT_ROOT = '/content/alljoined_lightweight_17k'\n",
    "\n",
    "    # --- Final Destination on Google Drive ---\n",
    "    DRIVE_OUTPUT_ROOT = '/content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k'\n",
    "\n",
    "    COCO_17K_DIR_NAME = 'coco_images_17k'\n",
    "    SPECTROGRAMS_DIR_NAME = 'spectrograms'\n",
    "    TOTAL_SAMPLES = 17000\n",
    "    TRAIN_SIZE = 14000\n",
    "    VAL_SIZE = 2800\n",
    "    TEST_SIZE = 200\n",
    "    FS = 250; LOW_CUT = 4; HIGH_CUT = 100; FILTER_ORDER = 5; NPERSEG = 128\n",
    "    NOVERLAP = 64; EEG_CHANNELS = 64; TARGET_EEG_LEN = 334\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. HELPER FUNCTIONS (Unchanged) ---\n",
    "# ==============================================================================\n",
    "def download_file(url, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"File {Path(save_path).name} already exists. Skipping download.\")\n",
    "        return True\n",
    "    try:\n",
    "        response = requests.get(url, stream=True); response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(save_path, 'wb') as f, tqdm(desc=Path(save_path).name, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024) as bar:\n",
    "            for data in response.iter_content(chunk_size=1024*1024):\n",
    "                f.write(data); bar.update(len(data))\n",
    "        return True\n",
    "    except requests.exceptions.RequestException: return False\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs; low = lowcut / nyq; high = highcut / nyq\n",
    "    sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "    return sosfiltfilt(sos, data, axis=1)\n",
    "\n",
    "def generate_stft_spectrogram(eeg_data, config):\n",
    "    fs = config.FS; q = 30.0\n",
    "    b50, a50 = iirnotch(50.0, q, fs); sos50 = tf2sos(b50, a50)\n",
    "    notched = sosfiltfilt(sos50, eeg_data, axis=1)\n",
    "    b60, a60 = iirnotch(60.0, q, fs); sos60 = tf2sos(b60, a60)\n",
    "    notched = sosfiltfilt(sos60, notched, axis=1)\n",
    "    filtered = bandpass_filter(notched, config.LOW_CUT, config.HIGH_CUT, fs, config.FILTER_ORDER)\n",
    "    specs = [np.log1p(spectrogram(filtered[i,:], fs,nperseg=config.NPERSEG,noverlap=config.NOVERLAP)[2]) for i in range(filtered.shape[0])]\n",
    "    return torch.tensor(np.array(specs), dtype=torch.float32)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. MAIN SCRIPT (Refactored to work locally) ---\n",
    "# ==============================================================================\n",
    "def create_final_dataset(config):\n",
    "    # --- Setup LOCAL Paths ---\n",
    "    output_root = Path(config.LOCAL_OUTPUT_ROOT)\n",
    "    filtered_images_dest_dir = output_root / config.COCO_17K_DIR_NAME\n",
    "    spectrograms_dest_dir = output_root / config.SPECTROGRAMS_DIR_NAME\n",
    "    metadata_save_path = output_root / 'metadata.csv'\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- All fast, local operations ---\n",
    "    # ... (The entire dataset creation process now happens on the fast local disk) ...\n",
    "\n",
    "    ds = load_dataset(config.HUGGINGFACE_DATASET)\n",
    "    full_dataset = concatenate_datasets([ds['train'], ds['test']])\n",
    "    df = full_dataset.to_pandas()\n",
    "    df_subset = df.sample(n=min(config.TOTAL_SAMPLES, len(df)), random_state=42)\n",
    "\n",
    "    # This step is now much faster as it copies between two local folders\n",
    "    required_coco_ids = set(df_subset['coco_id'].unique())\n",
    "    print(f\"Copying {len(required_coco_ids)} images locally...\")\n",
    "    source_dirs = [Path(config.EXTRACT_DIR) / 'train2017', Path(config.EXTRACT_DIR) / 'val2017']\n",
    "    for coco_id in tqdm(required_coco_ids, desc=\"Copying images\"):\n",
    "        img_fn = f\"{coco_id:012d}.jpg\"; dest_p = filtered_images_dest_dir / img_fn\n",
    "        if dest_p.exists(): continue\n",
    "        for source_dir in source_dirs:\n",
    "            src_p = source_dir / img_fn\n",
    "            if src_p.exists():\n",
    "                # Create parent dir for the image and then copy\n",
    "                os.makedirs(dest_p.parent, exist_ok=True)\n",
    "                shutil.copy(src_p, dest_p)\n",
    "                break\n",
    "\n",
    "    train_df, test_df = train_test_split(df_subset, test_size=config.TEST_SIZE, random_state=42)\n",
    "    val_split_ratio = config.VAL_SIZE / (config.TRAIN_SIZE + config.VAL_SIZE)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=val_split_ratio, random_state=42)\n",
    "    train_df.loc[:, 'split'] = 'train'; val_df.loc[:, 'split'] = 'val'; test_df.loc[:, 'split'] = 'test'\n",
    "    final_df = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "    all_metadata = []\n",
    "    for split_name, split_df in {'train': train_df, 'val': val_df, 'test': test_df}.items():\n",
    "        split_spectrogram_dir = spectrograms_dest_dir / split_name\n",
    "        split_spectrogram_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for index, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Processing '{split_name}' split\"):\n",
    "            eeg_raw_list = row['EEG']\n",
    "            processed_channels = [(p:=[0.0]*(config.TARGET_EEG_LEN-len(c)))+c if len(c)<config.TARGET_EEG_LEN else c[-config.TARGET_EEG_LEN:] if len(c)>config.TARGET_EEG_LEN else c for c in eeg_raw_list]\n",
    "            eeg_data = np.array(processed_channels, dtype=np.float32)\n",
    "            spectrogram_tensor = generate_stft_spectrogram(eeg_data, config)\n",
    "            image_filename = f\"{row['coco_id']:012d}.jpg\"\n",
    "            spectrogram_filename = f\"sample_{index}.pt\"\n",
    "            spectrogram_save_path = split_spectrogram_dir / spectrogram_filename\n",
    "            torch.save(spectrogram_tensor, spectrogram_save_path)\n",
    "            all_metadata.append({'image_path': os.path.join(config.COCO_17K_DIR_NAME, image_filename),\n",
    "                                 'spectrogram_path': os.path.join(config.SPECTROGRAMS_DIR_NAME, split_name, spectrogram_filename),\n",
    "                                 'split': split_name})\n",
    "\n",
    "    metadata_df = pd.DataFrame(all_metadata)\n",
    "    metadata_df.to_csv(metadata_save_path, index=False)\n",
    "\n",
    "    # --- FINAL STEP: Copy the completed dataset to Google Drive ---\n",
    "    print(\"\\n--- âœ… Local Processing Complete! ---\")\n",
    "    print(f\"Copying the final dataset from '{config.LOCAL_OUTPUT_ROOT}' to '{config.DRIVE_OUTPUT_ROOT}'...\")\n",
    "\n",
    "    # We use a system command for a robust copy operation.\n",
    "    # The '!' runs this as a shell command in Colab.\n",
    "    # It will overwrite the destination to ensure it's a clean copy.\n",
    "    if os.path.exists(config.DRIVE_OUTPUT_ROOT):\n",
    "        shutil.rmtree(config.DRIVE_OUTPUT_ROOT)\n",
    "    shutil.copytree(config.LOCAL_OUTPUT_ROOT, config.DRIVE_OUTPUT_ROOT)\n",
    "\n",
    "    print(\"\\n--- ðŸŽ‰ All Done! Your dataset is successfully saved to Google Drive. ---\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    # These setup steps are fast\n",
    "    config = CONFIG_FINAL_PREP()\n",
    "    os.makedirs(config.TEMP_DIR, exist_ok=True)\n",
    "    if not download_file(config.COCO_TRAIN_URL, config.TRAIN_ZIP_PATH): exit()\n",
    "    if not download_file(config.COCO_VAL_URL, config.VAL_ZIP_PATH): exit()\n",
    "    for zip_path in [config.TRAIN_ZIP_PATH, config.VAL_ZIP_PATH]:\n",
    "        split_name = Path(zip_path).stem\n",
    "        if not (Path(config.EXTRACT_DIR) / split_name).exists():\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf: zf.extractall(config.EXTRACT_DIR)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "    # Main processing function\n",
    "    create_final_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command recursively (-r) zips the specified folder.\n",
    "!zip -r /content/alljoined_lightweight_17k.zip /content/alljoined_lightweight_17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# This command initiates the download of the zip file.\n",
    "files.download('/content/alljoined_lightweight_17k.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command creates a single, compressed archive of your entire dataset.\n",
    "# The 'c' is for create, 'z' for gzip compression, 'f' for file.\n",
    "!tar -czf /content/alljoined_lightweight_17k.tar.gz -C /content/alljoined_lightweight_17k ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This copies the single .tar.gz file to your destination.\n",
    "# This operation is much more stable than copying thousands of individual files.\n",
    "!cp /content/alljoined_lightweight_17k.tar.gz /content/drive/MyDrive/NeuroVision/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('/content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k/coco_images_17k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compressed .tar.gz archive of your new lightweight dataset\n",
    "!tar -czf /content/drive/MyDrive/NeuroVision/final_lightweight_17k.tar.gz -C /content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aKuclCNmXEi"
   },
   "source": [
    "# Contrastive Encoder Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/content/final_lightweight_17k', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k.tar.gz /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf /content/alljoined_lightweight_17k.tar.gz -C /content/final_lightweight_17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Extract the archive locally (very fast)\n",
    "# !tar -xzf /content/drive/MyDrive/NeuroVision/alljoined_lightweight_17k.tar.gz -C /content/final_lightweight_17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "# IMPORTANT: Update this path to your main lightweight dataset folder.\n",
    "DATASET_ROOT = Path('/content/final_lightweight_17k')\n",
    "\n",
    "# Number of random samples to display from each data split.\n",
    "N_SAMPLES_PER_SPLIT = 5\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. DATA LOADING & VISUALIZATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "def visualize_random_spectrograms(root_path, n_samples):\n",
    "    \"\"\"\n",
    "    Loads and visualizes random raw spectrograms from the lightweight dataset.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "    spectrograms_dir = root_path / 'spectrograms'\n",
    "\n",
    "    # Check if the main directories exist\n",
    "    if not root_path.exists() or not spectrograms_dir.exists():\n",
    "        print(f\"Error: Dataset root or spectrograms directory not found at '{root_path}'\")\n",
    "        print(\"Please update the DATASET_ROOT variable to the correct path.\")\n",
    "        return\n",
    "\n",
    "    # --- Create the plot grid ---\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(splits),\n",
    "        ncols=n_samples,\n",
    "        figsize=(5 * n_samples, 4 * len(splits)),\n",
    "        squeeze=False # Ensures axes is always a 2D array\n",
    "    )\n",
    "\n",
    "    fig.suptitle('Random Raw Spectrograms per Split', fontsize=20)\n",
    "\n",
    "    for i, split_name in enumerate(splits):\n",
    "        split_path = spectrograms_dir / split_name\n",
    "\n",
    "        try:\n",
    "            sample_files = list(split_path.glob('sample_*.pt'))\n",
    "            if not sample_files:\n",
    "                print(f\"Warning: No sample files found in '{split_path}'. Skipping.\")\n",
    "                # Turn off unused axes in the grid for a cleaner look\n",
    "                for j in range(n_samples):\n",
    "                    axes[i, j].axis('off')\n",
    "                continue\n",
    "\n",
    "            random_files = random.sample(sample_files, min(n_samples, len(sample_files)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing files in '{split_path}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- Plot each sample for the current split ---\n",
    "        for j, file_path in enumerate(random_files):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            # UPDATED: Load the tensor directly, as it's not in a dictionary anymore.\n",
    "            spectrogram = torch.load(file_path)\n",
    "\n",
    "            # Use the first channel of the spectrogram for visualization\n",
    "            # The spectrograms are now small (e.g., 65x5), not 224x224\n",
    "            ax.imshow(spectrogram[0], cmap='viridis', origin='lower', aspect='auto')\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(split_name.title(), fontsize=14, weight='bold')\n",
    "\n",
    "            ax.set_title(f\"{file_path.stem}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    visualize_random_spectrograms(DATASET_ROOT, N_SAMPLES_PER_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf78qebtmKwc"
   },
   "source": [
    "# Contrastive Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466d1dd6cf6042008f8665aba1e4e231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Spectrogram Stats:   0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats calculated and saved to /content/final_lightweight_17k\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Path to your lightweight dataset on the local Colab disk\n",
    "DATA_ROOT = '/content/final_lightweight_17k'\n",
    "SPECTROGRAM_TRAIN_DIR = Path(DATA_ROOT) / 'spectrograms' / 'train'\n",
    "\n",
    "# Initialize accumulators\n",
    "channel_sum = torch.zeros(64, dtype=torch.float64)\n",
    "channel_sum_sq = torch.zeros(64, dtype=torch.float64)\n",
    "pixel_count = 0\n",
    "\n",
    "# Calculate stats only on the training set\n",
    "files = list(SPECTROGRAM_TRAIN_DIR.glob('*.pt'))\n",
    "for path in tqdm(files, desc=\"Calculating Spectrogram Stats\"):\n",
    "    data = torch.load(path)\n",
    "    channel_sum += data.sum(dim=[1, 2]).to(torch.float64)\n",
    "    channel_sum_sq += (data.to(torch.float64) ** 2).sum(dim=[1, 2])\n",
    "    pixel_count += data.shape[1] * data.shape[2]\n",
    "\n",
    "mean = (channel_sum / pixel_count).to(torch.float32)\n",
    "std = torch.sqrt((channel_sum_sq / pixel_count) - mean.to(torch.float64)**2).to(torch.float32)\n",
    "\n",
    "# Save the stats to the dataset folder\n",
    "torch.save(mean, Path(DATA_ROOT) / 'spec_mean.pt')\n",
    "torch.save(std, Path(DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "print(f\"\\nStats calculated and saved to {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION for Training from Scratch ---\n",
    "# ==============================================================================\n",
    "class CONFIG_CONTRASTIVE_TRAIN:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    RESUME_CHECKPOINT_PATH = '/content/drive/MyDrive/NeuroVision/contrastive_scratch_outputs/run_20250917_221600/contrastive_checkpoint_best.pth'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/contrastive_scratch_outputs'\n",
    "    LR = 5e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    BATCH_SIZE = 48\n",
    "    NUM_EPOCHS = 200\n",
    "    NUM_WORKERS = 2\n",
    "    EEG_CHANNELS = 64\n",
    "    ENCODER_DIM = 192\n",
    "    PROJECTION_DIM = 256\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. MODEL DEFINITIONS (Unchanged) ---\n",
    "# ==============================================================================\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim):\n",
    "        super().__init__(); self.projection = nn.Linear(embedding_dim, projection_dim); self.gelu = nn.GELU(); self.fc = nn.Linear(projection_dim, projection_dim); self.dropout = nn.Dropout(0.1); self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x); x = self.gelu(projected); x = self.fc(x); x = self.dropout(x); x = x + projected; x = self.layer_norm(x); return x\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__(); self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "    def forward(self, eeg_embeddings, image_embeddings):\n",
    "        eeg_embeddings = F.normalize(eeg_embeddings, p=2, dim=1); image_embeddings = F.normalize(image_embeddings, p=2, dim=1); logits = (eeg_embeddings @ image_embeddings.T) / self.temperature; labels = torch.arange(len(logits)).to(logits.device); loss_eeg = F.cross_entropy(logits, labels); loss_image = F.cross_entropy(logits.T, labels); return (loss_eeg + loss_image) / 2\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. DATASET CLASS (Unchanged) ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir = Path(root_dir); self.eeg_transform = eeg_transform; self.image_transform = image_transform\n",
    "        meta_df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = meta_df[meta_df['split'].str.strip() == split].reset_index(drop=True)\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.split_df.iloc[idx]; spectrogram_path = self.root_dir / sample_info['spectrogram_path']; image_path = self.root_dir / sample_info['image_path']\n",
    "        spectrogram_tensor = torch.load(spectrogram_path); image = Image.open(image_path).convert(\"RGB\")\n",
    "        spectrogram_tensor = self.eeg_transform(spectrogram_tensor); image_tensor = self.image_transform(image)\n",
    "        return spectrogram_tensor, image_tensor\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. MAIN TRAINING SCRIPT (with Normalization) ---\n",
    "# ==============================================================================\n",
    "def run_contrastive_training(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = output_dir / 'contrastive_checkpoint_best.pth'\n",
    "    writer = SummaryWriter(log_dir=str(output_dir / 'logs'))\n",
    "\n",
    "    # --- Prepare Data (Updated with Normalization) ---\n",
    "    print(\"1. Preparing data and DataLoaders...\")\n",
    "\n",
    "    # Load the pre-calculated spectrogram stats\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32, scale=False),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist()) # Add normalization\n",
    "    ])\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    metadata_csv_path = Path(config.PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "\n",
    "    train_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, metadata_csv_path, 'train', eeg_transform, image_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=config.NUM_WORKERS, drop_last=True, persistent_workers=True)\n",
    "\n",
    "    val_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, metadata_csv_path, 'val', eeg_transform, image_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=config.NUM_WORKERS, persistent_workers=True)\n",
    "\n",
    "    # --- Initialize Models ---\n",
    "    print(\"2. Initializing models...\")\n",
    "    eeg_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True, in_chans=config.EEG_CHANNELS, num_classes=0).to(device)\n",
    "    image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0).to(device)\n",
    "    for param in image_encoder.parameters(): param.requires_grad = False\n",
    "    eeg_projection = ProjectionHead(config.ENCODER_DIM, config.PROJECTION_DIM).to(device)\n",
    "    image_projection = ProjectionHead(config.ENCODER_DIM, config.PROJECTION_DIM).to(device)\n",
    "\n",
    "    # --- Training Setup ---\n",
    "    criterion = ContrastiveLoss().to(device)\n",
    "    trainable_params = list(eeg_encoder.parameters()) + list(eeg_projection.parameters()) + list(image_projection.parameters())\n",
    "    optimizer = optim.AdamW(trainable_params, lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # --- Training & Validation Loop ---\n",
    "    print(\"\\n--- Starting Contrastive Training From Scratch ---\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        eeg_encoder.train(); eeg_projection.train(); image_projection.train(); image_encoder.eval()\n",
    "\n",
    "        total_train_loss = 0.0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS} [Train]\")\n",
    "        for spectrogram_tensors, image_tensors in train_progress_bar:\n",
    "            spectrogram_tensors, image_tensors = spectrogram_tensors.to(device), image_tensors.to(device); optimizer.zero_grad()\n",
    "            eeg_features = eeg_encoder.forward_features(spectrogram_tensors)[:, 0]\n",
    "            with torch.no_grad(): image_features = image_encoder.forward_features(image_tensors)[:, 0]\n",
    "            eeg_embeddings = eeg_projection(eeg_features); image_embeddings = image_projection(image_features)\n",
    "            loss = criterion(eeg_embeddings, image_embeddings); loss.backward(); optimizer.step()\n",
    "            total_train_loss += loss.item(); train_progress_bar.set_postfix(Loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "        eeg_encoder.eval(); eeg_projection.eval(); image_projection.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for spectrogram_tensors, image_tensors in val_progress_bar:\n",
    "                spectrogram_tensors, image_tensors = spectrogram_tensors.to(device), image_tensors.to(device)\n",
    "                eeg_features = eeg_encoder.forward_features(spectrogram_tensors)[:, 0]; image_features = image_encoder.forward_features(image_tensors)[:, 0]\n",
    "                eeg_embeddings = eeg_projection(eeg_features); image_embeddings = image_projection(image_features)\n",
    "                loss = criterion(eeg_embeddings, image_embeddings); total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "        writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"New best validation loss ({best_val_loss:.4f}). Saving checkpoint to {checkpoint_path}\")\n",
    "            checkpoint = {'epoch': epoch, 'eeg_encoder_state_dict': eeg_encoder.state_dict(),\n",
    "                          'eeg_projection_state_dict': eeg_projection.state_dict(),\n",
    "                          'image_projection_state_dict': image_projection.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(), 'loss': best_val_loss}\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"\\n--- Training complete. Best model checkpoint saved to {checkpoint_path} ---\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     cl_train_config = CONFIG_CONTRASTIVE_TRAIN()\n",
    "#     run_contrastive_training(cl_train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class CONFIG_TSNE:\n",
    "    # --- Source Data Path (on local Colab disk) ---\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "\n",
    "    # --- IMPORTANT: UPDATE THIS PATH to your best saved checkpoint file ---\n",
    "    SAVED_CHECKPOINT_PATH = '/content/drive/MyDrive/NeuroVision/contrastive_scratch_outputs/run_20250917_221600/contrastive_checkpoint_best.pth'\n",
    "\n",
    "    # --- Parameters ---\n",
    "    BATCH_SIZE = 40\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "    # --- Model Dimensions (must match the trained model) ---\n",
    "    EEG_CHANNELS = 64\n",
    "    ENCODER_DIM = 192\n",
    "    PROJECTION_DIM = 256\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. MODEL AND DATASET DEFINITIONS (from training script) ---\n",
    "# ==============================================================================\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x); x = self.gelu(projected); x = self.fc(x)\n",
    "        x = self.dropout(x); x = x + projected; x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.eeg_transform = eeg_transform\n",
    "        meta_df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = meta_df[meta_df['split'].str.strip() == split].reset_index(drop=True)\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.split_df.iloc[idx]\n",
    "        spectrogram_path = self.root_dir / sample_info['spectrogram_path']\n",
    "        spectrogram_tensor = torch.load(spectrogram_path)\n",
    "        spectrogram_tensor = self.eeg_transform(spectrogram_tensor)\n",
    "        return spectrogram_tensor\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. MAIN VISUALIZATION SCRIPT ---\n",
    "# ==============================================================================\n",
    "def run_tsne_visualization(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- 1. Load the trained models from the checkpoint ---\n",
    "    print(\"Loading trained EEG encoder and projection head...\")\n",
    "    eeg_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=False, in_chans=config.EEG_CHANNELS, num_classes=0)\n",
    "    eeg_projection = ProjectionHead(config.ENCODER_DIM, config.PROJECTION_DIM)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(config.SAVED_CHECKPOINT_PATH, map_location=device)\n",
    "        eeg_encoder.load_state_dict(checkpoint['eeg_encoder_state_dict'])\n",
    "        eeg_projection.load_state_dict(checkpoint['eeg_projection_state_dict'])\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ERROR: Checkpoint file not found at '{config.SAVED_CHECKPOINT_PATH}'\")\n",
    "        print(\"Please update the path in the configuration.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred while loading the model: {e}\")\n",
    "        return\n",
    "\n",
    "    eeg_encoder.to(device).eval()\n",
    "    eeg_projection.to(device).eval()\n",
    "\n",
    "    # --- 2. Prepare the Test Dataset ---\n",
    "    print(\"Preparing test data loader...\")\n",
    "    # Normalization stats are needed for the transform\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([\n",
    "        transforms.ToDtype(torch.float32, scale=False),\n",
    "        transforms.Resize((224, 224), antialias=True),\n",
    "        transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist())\n",
    "    ])\n",
    "    metadata_csv_path = Path(config.PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    test_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, metadata_csv_path, 'val', eeg_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "    # --- 3. Extract Features from the Test Set ---\n",
    "    print(\"Extracting features from the test set...\")\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for spectrogram_tensors in tqdm(test_loader, desc=\"Extracting Features\"):\n",
    "            spectrogram_tensors = spectrogram_tensors.to(device)\n",
    "            features = eeg_encoder.forward_features(spectrogram_tensors)[:, 0]\n",
    "            embeddings = eeg_projection(features)\n",
    "            all_features.append(embeddings.cpu().numpy())\n",
    "\n",
    "    features_array = np.concatenate(all_features, axis=0)\n",
    "    print(f\"Extracted {features_array.shape[0]} feature vectors.\")\n",
    "\n",
    "    # --- 4. Run t-SNE ---\n",
    "    print(\"\\nRunning t-SNE... (This may take a few minutes)\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000, init='pca', learning_rate='auto')\n",
    "    tsne_results = tsne.fit_transform(features_array)\n",
    "    print(\"t-SNE complete.\")\n",
    "\n",
    "    # --- 5. Plot the Results ---\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5)\n",
    "    plt.title('t-SNE Visualization of Final EEG Features (Val Set)', fontsize=16)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tsne_config = CONFIG_TSNE()\n",
    "    run_tsne_visualization(tsne_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isxBC83ymFjm"
   },
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER_CHECKPOINT_PATH = '/content/drive/MyDrive/NeuroVision/contrastive_scratch_outputs/run_20250917_221600/contrastive_checkpoint_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â³ Installing and upgrading libraries...\")\n",
    "# The -q flag makes the output cleaner\n",
    "!pip install -q --upgrade diffusers transformers accelerate torchmetrics timm\n",
    "!pip install ftfy regex lpips\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "An error occurred while trying to fetch segmind/tiny-sd: segmind/tiny-sd does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch segmind/tiny-sd: segmind/tiny-sd does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 205MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c8e2f29ab4035b256199ee06b9389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Preparing data...\n",
      "3. Setting up optimizer and loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "/tmp/ipython-input-3465415857.py:190: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "\n",
      "--- Starting Training with Parallel Fusion Encoder ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a22a70440e483cad35fdc48c92705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3465415857.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe699fc3c4754787832a0da2d214d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Val]:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3465415857.py:247: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 -> Val Loss: 0.9177, LR: 9.05e-06\n",
      "âœ¨ New best validation loss. Saving encoder to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/parallel_encoder_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3465415857.py:277: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359944726d574efb8ee49a2c2e81d692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reconstructions to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/reconstructions_epoch_001.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8f458978a648f78d66027e470b4ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92163aba2ea54bc4b1818e0fbd4d2508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Val]:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 -> Val Loss: 0.9030, LR: 6.58e-06\n",
      "âœ¨ New best validation loss. Saving encoder to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/parallel_encoder_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5d4a8ce36b4f708773510ceea257ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reconstructions to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/reconstructions_epoch_002.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb19c9f65f3a4b9db2e43745d53d15f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01662c589f0c48a28ee027891e152023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Val]:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 -> Val Loss: 0.8792, LR: 3.52e-06\n",
      "âœ¨ New best validation loss. Saving encoder to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/parallel_encoder_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd98591b20ea433dad7f4f3c77a842ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reconstructions to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/reconstructions_epoch_003.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89029ce9c9a45d19b4ce9327a01eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0579a885046b78434dffc159f4623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Val]:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 -> Val Loss: 0.8701, LR: 1.05e-06\n",
      "âœ¨ New best validation loss. Saving encoder to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/parallel_encoder_best.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5294d6e0a4475b8536b0e968d5687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reconstructions to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/reconstructions_epoch_004.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6f8713d9194494ba8ee3e3ceca8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1363dfd6b5aa4c24b904c5b83aa447b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Val]:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 -> Val Loss: 0.8735, LR: 1.00e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a82b4c3b73246438ea02b5f509733ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reconstructions to /content/drive/MyDrive/NeuroVision/parallel_fusion_encoder/run_20250924_101049/reconstructions_epoch_005.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9495a5627e94072b51b72bff1478293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Train]:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torchvision.models as models\n",
    "import lpips\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/parallel_fusion_encoder'\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUMULATION_STEPS = 16\n",
    "    NUM_EPOCHS = 100\n",
    "    LR = 1e-5\n",
    "\n",
    "    # --- Loss Weights ---\n",
    "    MAE_LOSS_WEIGHT = 0.5\n",
    "    LPIPS_LOSS_WEIGHT = 1.5\n",
    "\n",
    "    # --- Encoder Dimensions ---\n",
    "    CNN_FEATURES_DIM = 256  # Output of ResNet's layer3\n",
    "    VIT_EMBED_DIM = 192     # Output of ViT-tiny\n",
    "    FUSION_DIM = 256        # Common dimension for cross-attention\n",
    "    EEG_EMBED_DIM = 768     # Final dimension for UNet\n",
    "\n",
    "    # --- Script Control ---\n",
    "    VISUALIZATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 280\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 1400\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. CORE ARCHITECTURE MODULES ---\n",
    "# ==============================================================================\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"A simple cross-attention block.\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        q = self.to_q(query)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        dots = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class ParallelFusionEncoder(nn.Module):\n",
    "    def __init__(self, cnn_out_dim, vit_embed_dim, fusion_dim, unet_cond_dim, in_chans=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- 1. CNN Branch (Local Features) ---\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        resnet.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.cnn_branch = nn.Sequential(*list(resnet.children())[:-3]) # Output of layer3\n",
    "\n",
    "        # --- 2. ViT Branch (Global Features) ---\n",
    "        self.vit_branch = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0, in_chans=in_chans)\n",
    "\n",
    "        # --- 3. Alignment Layers (Corrected) ---\n",
    "        # These project the features from each branch to the common FUSION_DIM\n",
    "        self.align_cnn_features = nn.Linear(cnn_out_dim, fusion_dim)\n",
    "        self.align_vit_features = nn.Linear(vit_embed_dim, fusion_dim) # Expects 192, outputs FUSION_DIM\n",
    "\n",
    "        # --- 4. Fusion Block ---\n",
    "        self.fusion_block = CrossAttentionBlock(dim=fusion_dim)\n",
    "\n",
    "        # --- 5. Final Projector ---\n",
    "        # Takes concatenated [global_vec, local_vec]\n",
    "        self.final_projector = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, unet_cond_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(unet_cond_dim * 2, unet_cond_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Branch 1: CNN ---\n",
    "        cnn_map = self.cnn_branch(x)\n",
    "        B, C, H, W = cnn_map.shape\n",
    "        cnn_seq = cnn_map.flatten(2).permute(0, 2, 1) # -> (B, 196, 256)\n",
    "\n",
    "        # --- Branch 2: ViT ---\n",
    "        vit_seq = self.vit_branch.forward_features(x) # -> (B, 197, 192)\n",
    "\n",
    "        # --- Align dimensions ---\n",
    "        cnn_aligned = self.align_cnn_features(cnn_seq)       # -> (B, 196, FUSION_DIM)\n",
    "        vit_aligned = self.align_vit_features(vit_seq) # -> (B, 197, FUSION_DIM)\n",
    "\n",
    "        # --- Fuse features ---\n",
    "        fused_cnn = self.fusion_block(\n",
    "            query=cnn_aligned,\n",
    "            context=vit_aligned\n",
    "        )\n",
    "\n",
    "        # --- Get final feature vectors ---\n",
    "        global_features = vit_aligned[:, 0]       # Use the ViT's [CLS] token\n",
    "        local_features = fused_cnn.mean(dim=1)  # Average the context-aware local features\n",
    "\n",
    "        # --- Combine and project ---\n",
    "        combined_features = torch.cat([global_features, local_features], dim=-1)\n",
    "        final_embedding = self.final_projector(combined_features)\n",
    "\n",
    "        return final_embedding\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. DATASET (Unchanged) ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.eeg_transform = eeg_transform\n",
    "        self.image_transform = image_transform\n",
    "        df = pd.read_csv(metadata_csv)\n",
    "        self.split_df = df[df['split'].str.strip() == split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.split_df.iloc[idx]\n",
    "        spec_p = self.root_dir / info['spectrogram_path']\n",
    "        img_p = self.root_dir / info['image_path']\n",
    "        spec = torch.load(spec_p)\n",
    "        image = Image.open(img_p).convert(\"RGB\")\n",
    "        return self.eeg_transform(spec), self.image_transform(image)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. MAIN TRAINING SCRIPT ---\n",
    "# ==============================================================================\n",
    "def train_parallel_model(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = output_dir / 'parallel_encoder_best.pth'\n",
    "\n",
    "    print(\"1. Loading models...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"segmind/tiny-sd\", subfolder=\"vae\").to(device).eval()\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"segmind/tiny-sd\", subfolder=\"unet\").to(device).eval()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    for p in unet.parameters(): p.requires_grad = False\n",
    "\n",
    "    eeg_encoder = ParallelFusionEncoder(\n",
    "        cnn_out_dim=config.CNN_FEATURES_DIM,\n",
    "        vit_embed_dim=config.VIT_EMBED_DIM,\n",
    "        fusion_dim=config.FUSION_DIM,\n",
    "        unet_cond_dim=config.EEG_EMBED_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"2. Preparing data...\")\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([transforms.ToDtype(torch.float32), transforms.Resize((224, 224)), transforms.Normalize(mean=spec_mean.tolist(), std=spec_std.tolist())])\n",
    "    image_transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    train_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', eeg_transform, image_transform)\n",
    "    val_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', eeg_transform, image_transform)\n",
    "\n",
    "    val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(val_indices))\n",
    "    fixed_spectrograms, fixed_real_images = next(iter(val_loader)); fixed_spectrograms, fixed_real_images = fixed_spectrograms.to(device), fixed_real_images.to(device)\n",
    "\n",
    "    print(\"3. Setting up optimizer and loss...\")\n",
    "    optimizer = optim.AdamW(eeg_encoder.parameters(), lr=config.LR, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-7)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"segmind/tiny-sd\", subfolder=\"scheduler\")\n",
    "    mae_loss_fn = nn.L1Loss()\n",
    "    lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"\\n--- Starting Training with Parallel Fusion Encoder ---\")\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        eeg_encoder.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_loader_subset = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=SubsetRandomSampler(train_indices), drop_last=True)\n",
    "        train_bar = tqdm(train_loader_subset, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (spectrograms, real_images) in enumerate(train_bar):\n",
    "            spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                unet_embedding = eeg_encoder(spectrograms)\n",
    "                eeg_cond = unet_embedding.unsqueeze(1)\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n",
    "                sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt()\n",
    "                pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / alpha_prod_t.sqrt()\n",
    "                generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                loss_mae = mae_loss_fn(noise_pred, noise)\n",
    "                loss_lpips = lpips_loss_fn(generated_images, real_images).mean()\n",
    "                loss = (loss_mae * config.MAE_LOSS_WEIGHT) + (loss_lpips * config.LPIPS_LOSS_WEIGHT)\n",
    "\n",
    "                loss = loss / config.GRAD_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % config.GRAD_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(eeg_encoder.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_bar.set_postfix({'loss_mae': f'{loss_mae.item():.3f}', 'loss_lpips': f'{loss_lpips.item():.3f}', 'total_loss': f'{loss.item() * config.GRAD_ACCUMULATION_STEPS:.3f}'})\n",
    "\n",
    "        # --- Validation & Visualization Loops (Unchanged) ---\n",
    "        eeg_encoder.eval()\n",
    "        total_val_loss = 0\n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for spectrograms, real_images in val_bar:\n",
    "                spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                    unet_embedding = eeg_encoder(spectrograms)\n",
    "                    eeg_cond = unet_embedding.unsqueeze(1)\n",
    "                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "                    alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n",
    "                    sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt()\n",
    "                    pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / alpha_prod_t.sqrt()\n",
    "                    generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                    loss_mae = mae_loss_fn(noise_pred, noise)\n",
    "                    loss_lpips = lpips_loss_fn(generated_images, real_images).mean()\n",
    "                    loss = (loss_mae * config.MAE_LOSS_WEIGHT) + (loss_lpips * config.LPIPS_LOSS_WEIGHT)\n",
    "                    total_val_loss += loss.item()\n",
    "                    val_bar.set_postfix({'val_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} -> Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"âœ¨ New best validation loss. Saving encoder to {checkpoint_path}\")\n",
    "            torch.save({'eeg_encoder_state_dict': eeg_encoder.state_dict()}, checkpoint_path)\n",
    "\n",
    "        if (epoch + 1) % config.VISUALIZATION_INTERVAL == 0:\n",
    "            eeg_encoder.eval()\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                unet_embedding = eeg_encoder(fixed_spectrograms)\n",
    "                eeg_cond = unet_embedding.unsqueeze(1)\n",
    "                latents = torch.randn((fixed_spectrograms.shape[0], 4, 64, 64), device=device, dtype=torch.float16)\n",
    "                noise_scheduler.set_timesteps(50)\n",
    "                for t in tqdm(noise_scheduler.timesteps, desc=\"Generating Images\", leave=False):\n",
    "                    noise_pred = unet(latents, t, encoder_hidden_states=eeg_cond.half()).sample\n",
    "                    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                generated_images = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "            comp_grid = torch.cat([fixed_real_images, generated_images.float()])\n",
    "            comp_grid = (comp_grid * 0.5 + 0.5).clamp(0, 1)\n",
    "            save_path = output_dir / f'reconstructions_epoch_{epoch+1:03d}.png'\n",
    "            save_image(comp_grid, save_path, nrow=config.BATCH_SIZE)\n",
    "            print(f\"Saved reconstructions to {save_path}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_parallel_model(TRAIN_CONFIG())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l_k7JXY19qJ"
   },
   "source": [
    "# Test Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "from torchvision.utils import save_image\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TEST_CONFIG:\n",
    "    # --- Paths ---\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    # IMPORTANT: Update this path to point to your saved model checkpoint\n",
    "    EEG_ENCODER_CHECKPOINT_PATH = '/content/drive/MyDrive/NeuroVision/end_to_end_perceptual/run_20250922_220152/eeg_encoder_best.pth'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/test_results'\n",
    "\n",
    "    # --- Model Dimensions (must match training) ---\n",
    "    BACKBONE_DIM = 192\n",
    "    EEG_EMBED_DIM = 768 # For UNet\n",
    "    CLIP_EMBED_DIM = 512 # For CLIP\n",
    "\n",
    "    # --- Inference Parameters ---\n",
    "    BATCH_SIZE = 5 # How many images to generate at once\n",
    "    NUM_TEST_SAMPLES = 20 # Max number of images to generate from the test set\n",
    "    INFERENCE_STEPS = 50 # Number of denoising steps\n",
    "    GUIDANCE_SCALE = 10 # How strongly to adhere to the EEG signal (higher means stronger)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER (Copy-pasted from your training script) ---\n",
    "# ==============================================================================\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, backbone_dim, projection_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=False, in_chans=64, num_classes=0)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(projection_dim),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.forward_features(x)[:, 0]\n",
    "        return self.projector(features)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. DATASET (Copy-pasted from your training script) ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir=Path(root_dir)\n",
    "        self.eeg_transform=eeg_transform\n",
    "        self.image_transform=image_transform\n",
    "        df=pd.read_csv(metadata_csv)\n",
    "        self.split_df=df[df['split'].str.strip()==split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        info=self.split_df.iloc[idx]\n",
    "        spec_p=self.root_dir/info['spectrogram_path']\n",
    "        img_p=self.root_dir/info['image_path']\n",
    "        spec=torch.load(spec_p)\n",
    "        image=Image.open(img_p).convert(\"RGB\")\n",
    "        return self.eeg_transform(spec), self.image_transform(image)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. MAIN TESTING SCRIPT ---\n",
    "# ==============================================================================\n",
    "def test_reconstruction(config):\n",
    "    # --- Setup ---\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'test_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Using device: {device}. Test results will be saved to {output_dir}\")\n",
    "\n",
    "    # --- 1. Load All Models ---\n",
    "    print(\"1. Loading pre-trained models (VAE, UNet, and EEG Encoder)...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"segmind/tiny-sd\", subfolder=\"vae\", torch_dtype=torch.float16).to(device).eval()\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"segmind/tiny-sd\", subfolder=\"unet\", torch_dtype=torch.float16).to(device).eval()\n",
    "\n",
    "    # Load your trained EEG Encoder\n",
    "    eeg_encoder = EEGEncoder(config.BACKBONE_DIM, config.EEG_EMBED_DIM).to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load(config.EEG_ENCODER_CHECKPOINT_PATH, map_location=device)\n",
    "        eeg_encoder.load_state_dict(checkpoint['eeg_encoder_state_dict'])\n",
    "        eeg_encoder.eval().to(torch.float16) # Use float16 for faster inference\n",
    "        print(\"âœ… Successfully loaded EEG Encoder checkpoint.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ›‘ Error loading EEG Encoder checkpoint: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare Data ---\n",
    "    print(\"2. Preparing test dataset...\")\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([transforms.ToDtype(torch.float32), transforms.Resize((224,224)), transforms.Normalize(mean=spec_mean.tolist(),std=spec_std.tolist())])\n",
    "    image_transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    test_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'test', eeg_transform, image_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"Found {len(test_dataset)} samples in the test set.\")\n",
    "\n",
    "    # --- 3. Setup Inference Pipeline ---\n",
    "    scheduler = DDIMScheduler.from_pretrained(\"segmind/tiny-sd\", subfolder=\"scheduler\")\n",
    "    scheduler.set_timesteps(config.INFERENCE_STEPS)\n",
    "\n",
    "    # --- 4. Run Inference Loop ---\n",
    "    print(\"\\n--- Starting Image Generation from Test Set ---\")\n",
    "    total_samples_generated = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (spectrograms, real_images) in enumerate(tqdm(test_loader, desc=\"Generating Images\")):\n",
    "            if total_samples_generated >= config.NUM_TEST_SAMPLES:\n",
    "                break\n",
    "\n",
    "            spectrograms = spectrograms.to(device, dtype=torch.float16)\n",
    "            batch_size = spectrograms.shape[0]\n",
    "            real_images.to(device)\n",
    "\n",
    "            # Get EEG embeddings from your trained encoder\n",
    "            with torch.autocast('cuda'):\n",
    "                unet_embedding = eeg_encoder(spectrograms)\n",
    "            eeg_cond = unet_embedding.unsqueeze(1) # Add sequence dimension for UNet\n",
    "\n",
    "            # Create unconditional embeddings for Classifier-Free Guidance\n",
    "            uncond_embedding = torch.zeros_like(eeg_cond)\n",
    "            # Concatenate for parallel processing\n",
    "            embeddings = torch.cat([uncond_embedding, eeg_cond])\n",
    "\n",
    "            # Prepare initial random noise\n",
    "            latents = torch.randn((batch_size, unet.config.in_channels, 64, 64), device=device, dtype=torch.float16)\n",
    "            latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "            # Denoising loop\n",
    "            for t in tqdm(scheduler.timesteps, leave=False, desc=\"Denoising\"):\n",
    "                # We need to process noise for both conditional and unconditional embeddings\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                with torch.autocast('cuda'):\n",
    "                    noise_pred = unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "                # Perform guidance\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + config.GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # Compute the previous noisy sample x_t -> x_{t-1}\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "            # Decode the final latents into images\n",
    "            with torch.autocast('cuda'):\n",
    "                generated_images = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "            # --- 5. Save Comparison Images ---\n",
    "            for j in range(batch_size):\n",
    "                # Clamp and denormalize images to [0, 1] range for saving\n",
    "                real = (real_images[j].unsqueeze(0) * 0.5 + 0.5).clamp(0, 1).to(device)\n",
    "                generated = (generated_images[j].unsqueeze(0) * 0.5 + 0.5).clamp(0, 1).to(device)\n",
    "\n",
    "                # Create a side-by-side grid: [Original Image | Generated Image]\n",
    "                comparison_grid = torch.cat([real, generated])\n",
    "                save_path = output_dir / f'comparison_{total_samples_generated + j:04d}.png'\n",
    "                save_image(comparison_grid, save_path, nrow=2)\n",
    "\n",
    "            total_samples_generated += batch_size\n",
    "\n",
    "    print(f\"\\n--- Testing Complete ---\")\n",
    "    print(f\"âœ… Generated {total_samples_generated} images. Results saved in: {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Make sure to update the checkpoint path in the config class!\n",
    "    config = TEST_CONFIG()\n",
    "    if 'YYYYMMDD' in config.EEG_ENCODER_CHECKPOINT_PATH:\n",
    "        print(\"ðŸ›‘ PLEASE UPDATE 'EEG_ENCODER_CHECKPOINT_PATH' in the TEST_CONFIG class before running.\")\n",
    "    else:\n",
    "        test_reconstruction(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtntBKutkXut"
   },
   "source": [
    "# dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/end_to_end_perceptual'\n",
    "    EEG_ENCODER_PRETRAIN_PATH = None\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 100\n",
    "    LR = 1e-5\n",
    "\n",
    "    # --- Loss Weights ---\n",
    "    PERCEPTUAL_LOSS_WEIGHT = 1.0\n",
    "\n",
    "    BACKBONE_DIM = 192\n",
    "    EEG_EMBED_DIM = 768\n",
    "    VISUALIZATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 400\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 2000\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. PERCEPTUAL LOSS MODULE (USING RESNET-18) ---\n",
    "# ==============================================================================\n",
    "class ResNetPerceptualLoss(nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(ResNetPerceptualLoss, self).__init__()\n",
    "        # Load pre-trained ResNet-18 and use its feature extraction layers\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # We take all layers except the final classification layer (fc)\n",
    "        self.model = nn.Sequential(*list(resnet.children())[:-1]).eval()\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.transform = nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        # Register buffers for ImageNet normalization\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Normalize images from your [-1, 1] range to ImageNet's expected format\n",
    "        input = (input + 1) / 2.0\n",
    "        target = (target + 1) / 2.0\n",
    "        input = (input - self.mean) / self.std\n",
    "        target = (target - self.mean) / self.std\n",
    "\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "\n",
    "        input_features = self.model(input)\n",
    "        target_features = self.model(target)\n",
    "\n",
    "        return self.loss_fn(input_features, target_features)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. EEG ENCODER MODEL ---\n",
    "# ==============================================================================\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, backbone_dim, projection_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=False, in_chans=64, num_classes=0)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(projection_dim),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.forward_features(x)[:, 0]\n",
    "        return self.projector(features)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. DATASET ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir=Path(root_dir)\n",
    "        self.eeg_transform=eeg_transform\n",
    "        self.image_transform=image_transform\n",
    "        df=pd.read_csv(metadata_csv)\n",
    "        self.split_df=df[df['split'].str.strip()==split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        info=self.split_df.iloc[idx]\n",
    "        spec_p=self.root_dir/info['spectrogram_path']\n",
    "        img_p=self.root_dir/info['image_path']\n",
    "        spec=torch.load(spec_p)\n",
    "        image=Image.open(img_p).convert(\"RGB\")\n",
    "        return self.eeg_transform(spec), self.image_transform(image)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN TRAINING SCRIPT ---\n",
    "# ==============================================================================\n",
    "def train_end_to_end(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = output_dir / 'eeg_encoder_best.pth'\n",
    "    print(f\"Using device: {device}. Checkpoints will be saved to {output_dir}\")\n",
    "\n",
    "    # --- Load Models ---\n",
    "    print(\"1. Loading models...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"segmind/tiny-sd\", subfolder=\"vae\").to(device).eval()\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"segmind/tiny-sd\", subfolder=\"unet\").to(device).eval()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    for p in unet.parameters(): p.requires_grad = False\n",
    "\n",
    "    eeg_encoder = EEGEncoder(config.BACKBONE_DIM, config.EEG_EMBED_DIM).to(device)\n",
    "    if config.EEG_ENCODER_PRETRAIN_PATH:\n",
    "        try:\n",
    "            print(f\"   Loading pre-trained weights from {config.EEG_ENCODER_PRETRAIN_PATH}\")\n",
    "            pretrain_ckpt = torch.load(config.EEG_ENCODER_PRETRAIN_PATH, map_location=device)\n",
    "            eeg_encoder.load_state_dict(pretrain_ckpt['eeg_encoder_state_dict'])\n",
    "            print(\"   âœ… Successfully loaded pre-trained EEG Encoder weights.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Could not load pre-trained weights: {e}. Starting from scratch.\")\n",
    "\n",
    "    # --- Data Prep ---\n",
    "    print(\"2. Preparing data...\")\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([transforms.ToDtype(torch.float32), transforms.Resize((224,224)), transforms.Normalize(mean=spec_mean.tolist(),std=spec_std.tolist())])\n",
    "    image_transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    train_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', eeg_transform, image_transform)\n",
    "    val_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', eeg_transform, image_transform)\n",
    "\n",
    "    val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=val_sampler, num_workers=0)\n",
    "\n",
    "    fixed_spectrograms, fixed_real_images = next(iter(val_loader)); fixed_spectrograms,fixed_real_images = fixed_spectrograms.to(device),fixed_real_images.to(device)\n",
    "    save_image(fixed_real_images*0.5+0.5, output_dir/'real_samples.png', nrow=config.BATCH_SIZE)\n",
    "\n",
    "    # --- Optimizer, Loss, etc. ---\n",
    "    print(\"3. Setting up optimizer and loss...\")\n",
    "    optimizer = optim.AdamW(eeg_encoder.parameters(), lr=config.LR, weight_decay=5e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=5e-7)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"segmind/tiny-sd\", subfolder=\"scheduler\")\n",
    "\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    perceptual_loss_fn = ResNetPerceptualLoss().to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"\\n--- Starting End-to-End Training ---\")\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        eeg_encoder.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        train_loader_subset = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=train_sampler, num_workers=0, drop_last=True)\n",
    "\n",
    "        train_pbar = tqdm(train_loader_subset, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "        for spectrograms, real_images in train_pbar:\n",
    "            spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings = eeg_encoder(spectrograms)\n",
    "                eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "                generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                loss_perceptual = perceptual_loss_fn(generated_images, real_images)\n",
    "\n",
    "                loss = loss_mse + (loss_perceptual * config.PERCEPTUAL_LOSS_WEIGHT)\n",
    "\n",
    "            train_pbar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_percept': f'{loss_perceptual:.3f}', 'loss_total': f'{loss:.3f}'})\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        eeg_encoder.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "            for spectrograms, real_images in val_pbar:\n",
    "                spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    eeg_embeddings = eeg_encoder(spectrograms)\n",
    "                    eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                    alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                    sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                    sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                    pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "                    generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                    loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                    loss_perceptual = perceptual_loss_fn(generated_images, real_images)\n",
    "\n",
    "                    loss = loss_mse + (loss_perceptual * config.PERCEPTUAL_LOSS_WEIGHT)\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                val_pbar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_percept': f'{loss_perceptual:.3f}', 'loss_total': f'{loss:.3f}'})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} -> Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"âœ¨ New best validation loss. Saving EEG Encoder to {checkpoint_path}\")\n",
    "            torch.save({'eeg_encoder_state_dict': eeg_encoder.state_dict()}, checkpoint_path)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        if (epoch + 1) % config.VISUALIZATION_INTERVAL == 0:\n",
    "            print(f\"--- Generating reconstructions for epoch {epoch+1} ---\")\n",
    "            eeg_encoder.eval()\n",
    "            with torch.no_grad():\n",
    "                eeg_embeddings = eeg_encoder(fixed_spectrograms)\n",
    "                eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                latents = torch.randn((fixed_spectrograms.shape[0], 4, 64, 64), device=device, dtype=torch.float16)\n",
    "                noise_scheduler.set_timesteps(50)\n",
    "\n",
    "                for t in tqdm(noise_scheduler.timesteps, desc=\"Generating Images\"):\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        noise_pred = unet(latents, t, encoder_hidden_states=eeg_cond.to(latents.dtype)).sample\n",
    "                    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        generated_images = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                comp_grid = torch.cat([fixed_real_images, generated_images.to(torch.float32)])\n",
    "                comp_grid = (comp_grid * 0.5 + 0.5).clamp(0, 1)\n",
    "                save_path = output_dir / f'reconstructions_epoch_{epoch+1:03d}.png'\n",
    "                save_image(comp_grid, save_path, nrow=config.BATCH_SIZE)\n",
    "                print(f\"Saved reconstructions to {save_path}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_end_to_end(TRAIN_CONFIG())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    # --- Paths ---\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/end_to_end_training'\n",
    "\n",
    "    # --- Pre-trained Models (if any) ---\n",
    "    EEG_ENCODER_PRETRAIN_PATH = '/content/drive/MyDrive/NeuroVision/end_to_end_training/run_20250922_132618/eeg_encoder_best.pth'\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    BATCH_SIZE = 3\n",
    "    NUM_EPOCHS = 100\n",
    "    LR = 1e-5\n",
    "\n",
    "    # --- Loss Weights ---\n",
    "    SSIM_LOSS_WEIGHT = 0.5\n",
    "    MSE_LOSS_WEIGHT = 1.5\n",
    "\n",
    "    # --- Model Dimensions ---\n",
    "    BACKBONE_DIM = 192\n",
    "    EEG_EMBED_DIM = 768\n",
    "\n",
    "    # --- Other ---\n",
    "    VISUALIZATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 280\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 1400\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. EEG ENCODER MODEL (WITH IMPROVED PROJECTOR) ---\n",
    "# ==============================================================================\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, backbone_dim, projection_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=False, in_chans=64, num_classes=0)\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(projection_dim),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.forward_features(x)[:, 0]\n",
    "        embeddings = self.projector(features)\n",
    "        return embeddings\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. DATASET ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir=Path(root_dir)\n",
    "        self.eeg_transform=eeg_transform\n",
    "        self.image_transform=image_transform\n",
    "        df=pd.read_csv(metadata_csv)\n",
    "        self.split_df=df[df['split'].str.strip()==split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info=self.split_df.iloc[idx]\n",
    "        spec_p=self.root_dir/info['spectrogram_path']\n",
    "        img_p=self.root_dir/info['image_path']\n",
    "        spec=torch.load(spec_p)\n",
    "        image=Image.open(img_p).convert(\"RGB\")\n",
    "        return self.eeg_transform(spec), self.image_transform(image)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. MAIN TRAINING SCRIPT ---\n",
    "# ==============================================================================\n",
    "def train_end_to_end(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = output_dir / 'eeg_encoder_best.pth'\n",
    "    print(f\"Using device: {device}. Checkpoints will be saved to {output_dir}\")\n",
    "\n",
    "    # --- Load Frozen Diffusion Models ---\n",
    "    print(\"1. Loading frozen VAE and UNet...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"segmind/tiny-sd\", subfolder=\"vae\").to(device).eval()\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"segmind/tiny-sd\", subfolder=\"unet\").to(device).eval()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    for p in unet.parameters(): p.requires_grad = False\n",
    "\n",
    "    # --- Load Trainable EEG Encoder ---\n",
    "    print(\"2. Initializing EEG Encoder...\")\n",
    "    eeg_encoder = EEGEncoder(config.BACKBONE_DIM, config.EEG_EMBED_DIM).to(device)\n",
    "    if config.EEG_ENCODER_PRETRAIN_PATH:\n",
    "        try:\n",
    "            print(f\"Loading pre-trained weights from {config.EEG_ENCODER_PRETRAIN_PATH}\")\n",
    "            pretrain_ckpt = torch.load(config.EEG_ENCODER_PRETRAIN_PATH, map_location=device)\n",
    "            eeg_encoder.load_state_dict(pretrain_ckpt['eeg_encoder_state_dict'])\n",
    "            print(\"âœ… Successfully loaded pre-trained EEG Encoder weights.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load pre-trained weights: {e}. Starting from scratch.\")\n",
    "\n",
    "    # --- Data Prep ---\n",
    "    print(\"3. Preparing data...\")\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "    eeg_transform = transforms.Compose([transforms.ToDtype(torch.float32), transforms.Resize((224,224)), transforms.Normalize(mean=spec_mean.tolist(),std=spec_std.tolist())])\n",
    "    image_transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    train_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', eeg_transform, image_transform)\n",
    "    val_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', eeg_transform, image_transform)\n",
    "    val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=val_sampler, num_workers=0)\n",
    "    fixed_spectrograms, fixed_real_images = next(iter(val_loader)); fixed_spectrograms,fixed_real_images = fixed_spectrograms.to(device),fixed_real_images.to(device)\n",
    "    save_image(fixed_real_images*0.5+0.5, output_dir/'real_samples.png', nrow=config.BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # --- Optimizer, Loss, etc. ---\n",
    "    print(\"4. Setting up optimizer and loss...\")\n",
    "    optimizer = optim.AdamW(eeg_encoder.parameters(), lr=config.LR, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=1e-7)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"segmind/tiny-sd\", subfolder=\"scheduler\")\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ssim_module = StructuralSimilarityIndexMeasure(data_range=2.0).to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"\\n--- Starting End-to-End Training ---\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        eeg_encoder.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        train_loader_subset = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=train_sampler, num_workers=0, drop_last=True)\n",
    "\n",
    "        train_pbar = tqdm(train_loader_subset, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "\n",
    "        for spectrograms, real_images in train_pbar:\n",
    "            # --- FIX: Move both inputs to the correct device ---\n",
    "            spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                eeg_embeddings = eeg_encoder(spectrograms)\n",
    "                eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                ssim_val = ssim_module(generated_images, real_images)\n",
    "                loss_ssim = 1.0 - ssim_val\n",
    "                loss = (loss_mse * config.MSE_LOSS_WEIGHT) + (loss_ssim * config.SSIM_LOSS_WEIGHT)\n",
    "\n",
    "            train_pbar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_ssim': f'{loss_ssim:.3f}', 'tot_loss': f'{loss:.3f}'})\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # --- Validation ---\n",
    "        eeg_encoder.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "\n",
    "            for spectrograms, real_images in val_pbar:\n",
    "                spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                eeg_embeddings = eeg_encoder(spectrograms)\n",
    "                eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "\n",
    "                generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                ssim_val = ssim_module(generated_images, real_images)\n",
    "                loss_ssim = 1.0 - ssim_val\n",
    "                loss = (loss_mse * config.MSE_LOSS_WEIGHT) + (loss_ssim * config.SSIM_LOSS_WEIGHT)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                val_pbar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_ssim': f'{loss_ssim:.3f}', 'tot_loss': f'{loss:.3f}'})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        scheduler.step() # Step the scheduler each epoch\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} -> Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"âœ¨ New best validation loss. Saving EEG Encoder to {checkpoint_path}\")\n",
    "            torch.save({'eeg_encoder_state_dict': eeg_encoder.state_dict()}, checkpoint_path)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        if (epoch + 1) % config.VISUALIZATION_INTERVAL == 0:\n",
    "            print(f\"--- Generating reconstructions for epoch {epoch+1} ---\")\n",
    "            eeg_encoder.eval()\n",
    "            with torch.no_grad():\n",
    "                eeg_embeddings = eeg_encoder(fixed_spectrograms)\n",
    "                eeg_cond = eeg_embeddings.unsqueeze(1)\n",
    "\n",
    "                latents = torch.randn((fixed_spectrograms.shape[0], 4, 64, 64), device=device, dtype=torch.float16)\n",
    "                noise_scheduler.set_timesteps(50)\n",
    "                print('begin image gen')\n",
    "\n",
    "                for t in tqdm(noise_scheduler.timesteps, desc=\"Generating Images\"):\n",
    "                  with torch.amp.autocast('cuda'):\n",
    "                    noise_pred = unet(latents, t, encoder_hidden_states=eeg_cond.to(latents.dtype)).sample\n",
    "                  latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "                print('pass thru vae for img gen')\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                  generated_images = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "                print('create img grid')\n",
    "                comp_grid = torch.cat([fixed_real_images, generated_images.to(torch.float32)])\n",
    "                comp_grid = (comp_grid * 0.5 + 0.5).clamp(0, 1)\n",
    "                save_path = output_dir / f'reconstructions_epoch_{epoch+1:03d}.png'\n",
    "                save_image(comp_grid, save_path, nrow=config.BATCH_SIZE)\n",
    "                print(f\"Saved reconstructions to {save_path}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_end_to_end(TRAIN_CONFIG())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "import clip\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "class TRAIN_CONFIG:\n",
    "    PROCESSED_DATA_ROOT = '/content/final_lightweight_17k'\n",
    "    METADATA_CSV = Path(PROCESSED_DATA_ROOT) / 'metadata.csv'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/NeuroVision/end_to_end_clip'\n",
    "    EEG_ENCODER_PRETRAIN_PATH = None #'/content/drive/MyDrive/NeuroVision/contrastive_scratch_outputs/run_20250917_221600/contrastive_checkpoint_best.pth' #'/content/drive/MyDrive/NeuroVision/end_to_end_clip/run_20250923_101956/eeg_encoder_best.pth' # Optional: Start with a previously trained encoder\n",
    "    BATCH_SIZE = 2 # Lowered for increased memory usage\n",
    "    NUM_EPOCHS = 100\n",
    "    LR = 1e-5\n",
    "\n",
    "    # --- Loss Weights ---\n",
    "    PERCEPTUAL_LOSS_WEIGHT = 0.75\n",
    "    CLIP_LOSS_WEIGHT = 0.2 # New, powerful semantic loss\n",
    "    MSE_LOSS_WEIGHT = 2.5\n",
    "\n",
    "    BACKBONE_DIM = 192\n",
    "    EEG_EMBED_DIM = 768 # For UNet\n",
    "    CLIP_EMBED_DIM = 512 # For CLIP\n",
    "\n",
    "    VISUALIZATION_INTERVAL = 1\n",
    "    VAL_SAMPLES_PER_EPOCH = 400\n",
    "    TRAIN_SAMPLES_PER_EPOCH = 2000\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. CLIP LOSS MODULE ---\n",
    "# ==============================================================================\n",
    "class CLIPLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CLIPLoss, self).__init__()\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.clip_transform = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, eeg_embedding, image):\n",
    "        # Image is in [-1, 1], preprocess expects [0, 1]\n",
    "        image_for_clip = (image + 1) / 2.0\n",
    "\n",
    "        preprocessed_image = self.clip_transform(image_for_clip)\n",
    "        # CLIP's preprocess includes normalization\n",
    "        image_features = self.model.encode_image(preprocessed_image)\n",
    "\n",
    "        # L2 normalize both embeddings\n",
    "        eeg_embedding = F.normalize(eeg_embedding, p=2, dim=-1)\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "\n",
    "        # Calculate cosine similarity loss\n",
    "        return (1 - F.cosine_similarity(eeg_embedding, image_features)).mean()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. PERCEPTUAL LOSS (ResNet) ---\n",
    "# ==============================================================================\n",
    "class ResNetPerceptualLoss(nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(ResNetPerceptualLoss, self).__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.model = nn.Sequential(*list(resnet.children())[:-1]).eval()\n",
    "        for param in self.model.parameters(): param.requires_grad = False\n",
    "        self.transform = nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = (input + 1) / 2.0; target = (target + 1) / 2.0\n",
    "        input = (input - self.mean) / self.std; target = (target - self.mean) / self.std\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        input_features = self.model(input); target_features = self.model(target)\n",
    "        return self.loss_fn(input_features, target_features)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. EEG ENCODER (with two heads) ---\n",
    "# ==============================================================================\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, backbone_dim, unet_dim, clip_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=False, in_chans=64, num_classes=0)\n",
    "\n",
    "        # Head A: For guiding the UNet\n",
    "        self.unet_projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, unet_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(unet_dim),\n",
    "            nn.Linear(unet_dim, unet_dim)\n",
    "        )\n",
    "\n",
    "        # Head B: For matching CLIP's semantic space\n",
    "        self.clip_projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, clip_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(clip_dim * 2, clip_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.forward_features(x)[:, 0]\n",
    "        unet_embedding = self.unet_projector(features)\n",
    "        clip_embedding = self.clip_projector(features)\n",
    "        return unet_embedding, clip_embedding\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. DATASET ---\n",
    "# ==============================================================================\n",
    "class LightweightEEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, metadata_csv, split, eeg_transform, image_transform):\n",
    "        self.root_dir=Path(root_dir)\n",
    "        self.eeg_transform=eeg_transform\n",
    "        self.image_transform=image_transform\n",
    "        df=pd.read_csv(metadata_csv)\n",
    "        self.split_df=df[df['split'].str.strip()==split].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.split_df)\n",
    "    def __getitem__(self, idx):\n",
    "        info=self.split_df.iloc[idx]\n",
    "        spec_p=self.root_dir/info['spectrogram_path']\n",
    "        img_p=self.root_dir/info['image_path']\n",
    "        spec=torch.load(spec_p)\n",
    "        image=Image.open(img_p).convert(\"RGB\")\n",
    "        return self.eeg_transform(spec), self.image_transform(image)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 6. MAIN TRAINING SCRIPT ---\n",
    "# ==============================================================================\n",
    "def train_end_to_end_clip(config):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = Path(config.OUTPUT_DIR) / f'run_{timestamp}'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = output_dir / 'eeg_encoder_best.pth'\n",
    "    writer = SummaryWriter(log_dir=str(output_dir / 'logs'))\n",
    "    print(f\"Using device: {device}. Checkpoints will be saved to {output_dir}\")\n",
    "\n",
    "    print(\"1. Loading models...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"segmind/tiny-sd\", subfolder=\"vae\").to(device).eval()\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"segmind/tiny-sd\", subfolder=\"unet\").to(device).eval()\n",
    "    for p in vae.parameters(): p.requires_grad = False\n",
    "    for p in unet.parameters(): p.requires_grad = False\n",
    "\n",
    "    eeg_encoder = EEGEncoder(config.BACKBONE_DIM, config.EEG_EMBED_DIM, config.CLIP_EMBED_DIM).to(device)\n",
    "    if config.EEG_ENCODER_PRETRAIN_PATH:\n",
    "        try:\n",
    "            print(f\"Loading pre-trained weights from {config.EEG_ENCODER_PRETRAIN_PATH}\")\n",
    "            pretrain_ckpt = torch.load(config.EEG_ENCODER_PRETRAIN_PATH, map_location=device)\n",
    "            eeg_encoder.backbone.load_state_dict(pretrain_ckpt['eeg_encoder_state_dict'])\n",
    "            print(\"âœ… Successfully loaded pre-trained EEG Encoder weights.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load pre-trained weights: {e}. Starting from scratch.\")\n",
    "\n",
    "    print(\"2. Preparing data...\")\n",
    "    spec_mean = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_mean.pt')\n",
    "    spec_std = torch.load(Path(config.PROCESSED_DATA_ROOT) / 'spec_std.pt')\n",
    "\n",
    "    eeg_transform = transforms.Compose([transforms.ToDtype(torch.float32), transforms.Resize((224,224)), transforms.Normalize(mean=spec_mean.tolist(),std=spec_std.tolist())])\n",
    "    image_transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    train_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'train', eeg_transform, image_transform)\n",
    "    val_dataset = LightweightEEGDataset(config.PROCESSED_DATA_ROOT, config.METADATA_CSV, 'val', eeg_transform, image_transform)\n",
    "\n",
    "    val_indices = np.random.choice(len(val_dataset), min(len(val_dataset), config.VAL_SAMPLES_PER_EPOCH), replace=False)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, sampler=val_sampler, num_workers=0)\n",
    "\n",
    "    fixed_spectrograms, fixed_real_images = next(iter(val_loader)); fixed_spectrograms,fixed_real_images = fixed_spectrograms.to(device),fixed_real_images.to(device)\n",
    "    save_image(fixed_real_images*0.5+0.5, output_dir/'real_samples.png', nrow=config.BATCH_SIZE)\n",
    "\n",
    "    print(\"3. Setting up optimizer and loss...\")\n",
    "    optimizer = optim.AdamW(eeg_encoder.parameters(), lr=config.LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-7)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"segmind/tiny-sd\", subfolder=\"scheduler\")\n",
    "\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    perceptual_loss_fn = ResNetPerceptualLoss().to(device)\n",
    "    # clip_loss_fn = CLIPLoss(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"\\n--- Starting End-to-End Training with CLIP Guidance ---\")\n",
    "\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        eeg_encoder.train()\n",
    "        train_indices = np.random.choice(len(train_dataset), min(len(train_dataset), config.TRAIN_SAMPLES_PER_EPOCH), replace=False)\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        train_loader_subset = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, sampler=train_sampler, num_workers=0, drop_last=True)\n",
    "\n",
    "        train_bar = tqdm(train_loader_subset, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "\n",
    "        for spectrograms, real_images in train_bar:\n",
    "            spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                unet_embedding, clip_embedding = eeg_encoder(spectrograms)\n",
    "                eeg_cond = unet_embedding.unsqueeze(1)\n",
    "\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "                generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                # --- NEW COMBINED LOSS CALCULATION ---\n",
    "                loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                loss_perceptual = perceptual_loss_fn(generated_images, real_images)\n",
    "                loss_clip = clip_loss_fn(clip_embedding, real_images)\n",
    "\n",
    "                # Combine the losses\n",
    "                loss = (loss_mse * config.MSE_LOSS_WEIGHT) + (loss_perceptual * config.PERCEPTUAL_LOSS_WEIGHT) + (loss_clip * config.CLIP_LOSS_WEIGHT)\n",
    "\n",
    "            train_bar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_percep': f'{loss_perceptual:.3f}', 'loss_clip': f'{loss_clip:.3f}', 'loss_total': f'{loss:.3f}'})\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # --- Validation Loop ---\n",
    "        eeg_encoder.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "\n",
    "            for spectrograms, real_images in val_bar:\n",
    "                spectrograms, real_images = spectrograms.to(device), real_images.to(device)\n",
    "\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    latents = vae.encode(real_images).latent_dist.sample() * vae.config.scaling_factor\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                    unet_embedding, clip_embedding = eeg_encoder(spectrograms)\n",
    "                    eeg_cond = unet_embedding.unsqueeze(1)\n",
    "\n",
    "                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=eeg_cond).sample\n",
    "\n",
    "                    alpha_prod_t = noise_scheduler.alphas_cumprod.to(device)[timesteps]\n",
    "                    sqrt_alpha_prod_t = alpha_prod_t.sqrt().view(-1, 1, 1, 1)\n",
    "                    sqrt_one_minus_alpha_prod_t = (1 - alpha_prod_t).sqrt().view(-1, 1, 1, 1)\n",
    "                    pred_latents = (noisy_latents - sqrt_one_minus_alpha_prod_t * noise_pred) / sqrt_alpha_prod_t\n",
    "                    generated_images = vae.decode(pred_latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                    loss_mse = mse_loss_fn(noise_pred, noise)\n",
    "                    loss_perceptual = perceptual_loss_fn(generated_images, real_images)\n",
    "                    loss_clip = clip_loss_fn(clip_embedding, real_images)\n",
    "\n",
    "                    loss = (loss_mse * config.MSE_LOSS_WEIGHT) + (loss_perceptual * config.PERCEPTUAL_LOSS_WEIGHT) + (loss_clip * config.CLIP_LOSS_WEIGHT)\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                val_bar.set_postfix({'loss_mse': f'{loss_mse:.3f}', 'loss_percep': f'{loss_perceptual:.3f}', 'loss_clip': f'{loss_clip:.3f}', 'loss_total': f'{loss:.3f}'})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} -> Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"âœ¨ New best validation loss. Saving EEG Encoder to {checkpoint_path}\")\n",
    "            torch.save({'eeg_encoder_state_dict': eeg_encoder.state_dict()}, checkpoint_path)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        if (epoch + 1) % config.VISUALIZATION_INTERVAL == 0:\n",
    "            print(f\"--- Generating reconstructions for epoch {epoch+1} ---\")\n",
    "            eeg_encoder.eval()\n",
    "            with torch.no_grad():\n",
    "                # For visualization, we only need the UNet embedding\n",
    "                unet_embedding, _ = eeg_encoder(fixed_spectrograms)\n",
    "                eeg_cond = unet_embedding.unsqueeze(1)\n",
    "\n",
    "                latents = torch.randn((fixed_spectrograms.shape[0], 4, 64, 64), device=device, dtype=torch.float16)\n",
    "                noise_scheduler.set_timesteps(50)\n",
    "\n",
    "                for t in tqdm(noise_scheduler.timesteps, desc=\"Generating Images\"):\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        noise_pred = unet(latents, t, encoder_hidden_states=eeg_cond.to(latents.dtype)).sample\n",
    "                    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        generated_images = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "                comp_grid = torch.cat([fixed_real_images, generated_images.to(torch.float32)])\n",
    "                comp_grid = (comp_grid * 0.5 + 0.5).clamp(0, 1)\n",
    "                save_path = output_dir / f'reconstructions_epoch_{epoch+1:03d}.png'\n",
    "                save_image(comp_grid, save_path, nrow=config.BATCH_SIZE)\n",
    "                print(f\"Saved reconstructions to {save_path}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_end_to_end_clip(TRAIN_CONFIG())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
