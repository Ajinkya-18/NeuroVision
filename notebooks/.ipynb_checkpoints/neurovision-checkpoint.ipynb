{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f68d0c-a42d-4ede-a02d-d2ea6dbf9275",
   "metadata": {},
   "source": [
    "# NeuroVision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162949c-6fe9-43c2-a775-7dbf886ab75a",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fce1e7a-a9dd-47e5-970b-682938b29ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbc4228-6ab9-4fb4-8dbc-13d2b07e5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the dir contents of dataset folder and segregate them \n",
    "# into n separate classes.\n",
    "def create_dataset_folders(metadata_file:str, csv_dir:str, output_dir:str):\n",
    "    class_id_to_folder = {}\n",
    "\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            label_str, _, class_id = parts\n",
    "            # print(label_str, class_id)\n",
    "            first_label = label_str.split(',')[0].strip()\n",
    "            # print(first_label)\n",
    "            class_id_to_folder[class_id] = first_label\n",
    "\n",
    "        count = 0\n",
    "        for filename in os.listdir(csv_dir):\n",
    "            if not filename.endswith('.csv'):\n",
    "                continue\n",
    "\n",
    "            class_id = filename.split('_')[3]\n",
    "\n",
    "            folder_name = class_id_to_folder.get(class_id)\n",
    "            print(folder_name)\n",
    "\n",
    "            if not folder_name:\n",
    "                print(f'Unknown class id: {class_id}')\n",
    "                continue\n",
    "\n",
    "            safe_folder = folder_name.replace('/', '_').replace('\\\\', '_').strip()\n",
    "\n",
    "            dest_folder = os.path.join(output_dir, safe_folder)\n",
    "            os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "            src_path = os.path.join(csv_dir, filename)\n",
    "            dst_path = os.path.join(dest_folder, filename)\n",
    "\n",
    "            # print(f\"Move: {src_path} to {dst_path}\")\n",
    "            count+=1\n",
    "            print(count)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee0ee6e-277d-43c4-8269-88ef54a61c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create_dataset_folders('../data/WordReport-v1.04.txt', \n",
    "#                        '../data/MindBigData-Imagenet', \n",
    "#                        '../data/Segregated_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc0b3cc-e26c-4823-8bd0-d3904f696b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "\n",
    "def reorganize_dataset(mapping_file, src_root, dst_root, move=False):\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "\n",
    "    os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "    for super_class, sub_classes in mapping.items():\n",
    "        super_cls_dir = os.path.join(dst_root, super_class)\n",
    "        os.makedirs(super_cls_dir, exist_ok=True)\n",
    "\n",
    "        for sub_class in sub_classes:\n",
    "            sub_cls_dir = os.path.join(src_root, sub_class)\n",
    "            if not os.path.exists(sub_cls_dir):\n",
    "                print(f\"[Warning] Sub-class folder not found: {sub_cls_dir}\")\n",
    "                continue\n",
    "\n",
    "            for file_name in os.listdir(sub_cls_dir):\n",
    "                src_file = os.path.join(sub_cls_dir, file_name)\n",
    "                dst_file = os.path.join(super_cls_dir, file_name)\n",
    "\n",
    "                if move:\n",
    "                    shutil.move(src_file, dst_file)\n",
    "\n",
    "                else: \n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "\n",
    "            print(f\"[OK] {'Moved' if move else 'Copied'} {sub_class} -> {super_class}\")\n",
    "    print(\"Dataset reorganization complete!\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a4f18-0cbf-451e-be8c-1aa156eff8a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset Processing for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f7ba77-0e7b-46c4-9823-85ed74ec2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027fe9b8-8ee9-4f76-b0b0-20f528f20261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, samples, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)          \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "\n",
    "        df = pd.read_csv(file_path, header=None, index_col=0)\n",
    "        eeg_data = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "        if eeg_data.shape[0] < eeg_data.shape[1]:\n",
    "            eeg_data = eeg_data.T\n",
    "\n",
    "        if self.transform:\n",
    "            eeg_data = self.transform(eeg_data)\n",
    "\n",
    "        return eeg_data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4342db-dfec-4a91-bd75-848017a33149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(root_dir, val_ratio=0.2, random_state=42): \n",
    "    class_names = sorted(os.listdir(root_dir))\n",
    "    class_to_idx = {cls:idx for idx, cls in enumerate(class_names)}\n",
    "\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "\n",
    "    for cls in class_names:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        \n",
    "        for fname in os.listdir(cls_dir): \n",
    "            if fname.endswith('.csv'):\n",
    "                path = os.path.join(cls_dir, fname)\n",
    "                all_samples.append((path, class_to_idx[cls]))\n",
    "                all_labels.append(class_to_idx[cls])\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(all_samples))), \n",
    "        test_size=val_ratio, \n",
    "        random_state=random_state, \n",
    "        stratify=all_labels\n",
    "    )\n",
    "\n",
    "    train_samples = [all_samples[i] for i in train_idx]\n",
    "    val_samples = [all_samples[i] for i in val_idx]\n",
    "\n",
    "    train_dataset = EEGDataset(root_dir, train_samples)\n",
    "    val_dataset = EEGDataset(root_dir, val_samples)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0696ba6-64c0-427e-895e-0dead7a43cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor([seq.size(0) for seq in sequences], dtype=torch.long)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return padded_seqs, torch.tensor(labels), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8180c7-8721-41be-9a88-1dfd1e25a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../data/Segregated_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3aa364a-238c-4491-b180-38f91b9e998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = make_datasets(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a21f8-2bf8-43e3-a97e-921a0309a3ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2419cd3-599a-433b-a84e-8a69bf5f8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6a7229-ac9e-4bed-bd54-97f1e0cbc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegLstm(nn.Module):\n",
    "    def __init__(self, input_dims=5, hidden_dims=256, num_layers=4, dropout=0.3 , num_classes=len(os.listdir(root_dir))): \n",
    "        super(EegLstm, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dims, \n",
    "            hidden_size=hidden_dims, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 2 else 0\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dims, hidden_dims//2), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(hidden_dims//2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths=None): \n",
    "        if lengths is not None: \n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "\n",
    "            packed_out, (h_n, c_n) = self.lstm(packed)\n",
    "\n",
    "        else:\n",
    "            out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        last_hidden = h_n[-1]\n",
    "        logits=self.fc(last_hidden)\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf8cb365-9915-4f3a-afd4-6946979a0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89fb3cb9-1dec-4a6a-9c4e-7a445954daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, model, save_path='../models/eeg_classifier.pt', patience=5, tol=1e-3):\n",
    "        self.model = model\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.tol = tol\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, batch_val_loss):\n",
    "        if batch_val_loss < self.best_val_loss - self.tol:\n",
    "            torch.save(self.model.state_dict(), self.save_path)\n",
    "            self.best_val_loss = batch_val_loss\n",
    "            self.counter = 0\n",
    "            print(f'Validation Loss improved -> model saved to {self.save_path}')\n",
    "            \n",
    "        else:\n",
    "            if self.counter < self.patience: \n",
    "                self.counter += 1\n",
    "                print(f'No improvement in Val Loss. Counter: {self.counter}/{self.patience}')\n",
    "                \n",
    "            else: \n",
    "                self.early_stop = True\n",
    "                print(f\"Early Stopping triggered!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0401a423-49b1-40f1-b535-4db7af34c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, val_loader, epochs=20, lr=1e-2, device='cpu'): \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter(log_dir=f'../reports/runs/{model_name}')\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    early_stopping = EarlyStopping(model, save_path=f'../models/{model_name}_v1_best.pth', patience=6)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train Pass]', leave=True)\n",
    "\n",
    "        for batch_x, batch_y, lengths in train_bar: \n",
    "            batch_x, batch_y, lengths = batch_x.to(device), batch_y.to(device), lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_preds = model(batch_x, lengths)\n",
    "\n",
    "            loss = criterion(y_preds, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "            _, preds = torch.max(y_preds, 1)\n",
    "            train_correct += (preds == batch_y).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        train_loss /= train_total\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch{epoch+1}/{epochs} [Val Pass]\", leave=True)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for batch_x, batch_y, lengths in val_bar:\n",
    "                batch_x, batch_y, lengths = batch_x.to(device), batch_y.to(device), lengths.to(device)\n",
    "\n",
    "                y_preds = model(batch_x, lengths)\n",
    "                loss = criterion(y_preds, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "                _, preds = torch.max(y_preds, 1)\n",
    "                val_correct += (preds == batch_y).sum().item()\n",
    "                val_total += batch_y.size(0)\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "            \n",
    "\n",
    "        # logging\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\\nTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} %\\nVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc98eb7-1f94-448f-9631-a983d33d4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print('========================================= Model Summary ==============================================\\n')\n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"{'| Parameter Name':31}|| Number of Parameters|\")\n",
    "    print(f\"{'='*55}\")\n",
    "    \n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'| {name:30}|{param.numel():20} |')\n",
    "        print(f\"{'-'*55}\")\n",
    "        total_params += param.numel()\n",
    "        \n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9cb8d-ac5b-4ba4-a4ec-8b2d0877b6f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "052a5da9-938e-460f-b34f-22d9d004458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, \n",
    "                          num_workers=4, pin_memory=False, persistent_workers=True, prefetch_factor=2)\n",
    "        \n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, \n",
    "                        num_workers=4, pin_memory=False, persistent_workers=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "450a101a-3a4c-4e5b-85f0-4189cae1651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = EegLstm(input_dims=5, hidden_dims=64, num_layers=2, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3ffcf2-4219-4dd0-b84a-57ca835d803a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================= Model Summary ==============================================\n",
      "\n",
      "\n",
      "=======================================================\n",
      "| Parameter Name               || Number of Parameters|\n",
      "=======================================================\n",
      "| lstm.weight_ih_l0             |                1280 |\n",
      "-------------------------------------------------------\n",
      "| lstm.weight_hh_l0             |               16384 |\n",
      "-------------------------------------------------------\n",
      "| lstm.bias_ih_l0               |                 256 |\n",
      "-------------------------------------------------------\n",
      "| lstm.bias_hh_l0               |                 256 |\n",
      "-------------------------------------------------------\n",
      "| lstm.weight_ih_l1             |               16384 |\n",
      "-------------------------------------------------------\n",
      "| lstm.weight_hh_l1             |               16384 |\n",
      "-------------------------------------------------------\n",
      "| lstm.bias_ih_l1               |                 256 |\n",
      "-------------------------------------------------------\n",
      "| lstm.bias_hh_l1               |                 256 |\n",
      "-------------------------------------------------------\n",
      "| fc.0.weight                   |                2048 |\n",
      "-------------------------------------------------------\n",
      "| fc.0.bias                     |                  32 |\n",
      "-------------------------------------------------------\n",
      "| fc.3.weight                   |               18144 |\n",
      "-------------------------------------------------------\n",
      "| fc.3.bias                     |                 567 |\n",
      "-------------------------------------------------------\n",
      "\n",
      "Total Parameters: 72,247\n"
     ]
    }
   ],
   "source": [
    "model_summary(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b77474-4ff5-4ad0-8f17-2cb024182b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train Pass]:   0%|                                                                 | 0/351 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'        \n",
    "train_model(lstm_model, 'EEG_LSTM', train_loader, val_loader, 20, 1e-2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc284f5f-0ff3-4928-9c02-e89f81e00640",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd95c4-6134-4a72-a305-7896d14f00ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurovision",
   "language": "python",
   "name": "neurovision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
