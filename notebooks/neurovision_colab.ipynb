{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajinkya-18/NeuroVision/blob/main/notebooks/neurovision_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f68d0c-a42d-4ede-a02d-d2ea6dbf9275",
      "metadata": {
        "id": "58f68d0c-a42d-4ede-a02d-d2ea6dbf9275"
      },
      "source": [
        "# NeuroVision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7162949c-6fe9-43c2-a775-7dbf886ab75a",
      "metadata": {
        "id": "7162949c-6fe9-43c2-a775-7dbf886ab75a"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your Google Drive is mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- This is the key step ---\n",
        "# Use 'cp -r' to recursively copy the entire folder.\n",
        "# This will be SLOW, as it's copying thousands of individual files over the network.\n",
        "# Let it run until it's finished.\n",
        "\n",
        "print(\"Starting to copy dataset folder from Drive to local storage...\")\n",
        "print(\"This may take a significant amount of time, please be patient.\")\n",
        "\n",
        "# !cp -r \"/content/drive/MyDrive/NeuroVision/data/Classes_Regrouped_Dataset\" \"/content/eeg_dataset\"\n",
        "\n",
        "print(\"Copying complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDgv-pwx8Zvr",
        "outputId": "73c8a932-9eb1-4800-eb63-23983efbe2d2"
      },
      "id": "ZDgv-pwx8Zvr",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting to copy dataset folder from Drive to local storage...\n",
            "This may take a significant amount of time, please be patient.\n",
            "Copying complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6fce1e7a-a9dd-47e5-970b-682938b29ae3",
      "metadata": {
        "id": "6fce1e7a-a9dd-47e5-970b-682938b29ae3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9fbc4228-6ab9-4fb4-8dbc-13d2b07e5d17",
      "metadata": {
        "id": "9fbc4228-6ab9-4fb4-8dbc-13d2b07e5d17"
      },
      "outputs": [],
      "source": [
        "# function to read the dir contents of dataset folder and segregate them\n",
        "# into n separate classes.\n",
        "def create_dataset_folders(metadata_file:str, csv_dir:str, output_dir:str):\n",
        "    class_id_to_folder = {}\n",
        "\n",
        "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "\n",
        "            label_str, _, class_id = parts\n",
        "            # print(label_str, class_id)\n",
        "            first_label = label_str.split(',')[0].strip()\n",
        "            # print(first_label)\n",
        "            class_id_to_folder[class_id] = first_label\n",
        "\n",
        "        count = 0\n",
        "        for filename in os.listdir(csv_dir):\n",
        "            if not filename.endswith('.csv'):\n",
        "                continue\n",
        "\n",
        "            class_id = filename.split('_')[3]\n",
        "\n",
        "            folder_name = class_id_to_folder.get(class_id)\n",
        "            print(folder_name)\n",
        "\n",
        "            if not folder_name:\n",
        "                print(f'Unknown class id: {class_id}')\n",
        "                continue\n",
        "\n",
        "            safe_folder = folder_name.replace('/', '_').replace('\\\\', '_').strip()\n",
        "\n",
        "            dest_folder = os.path.join(output_dir, safe_folder)\n",
        "            os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "            src_path = os.path.join(csv_dir, filename)\n",
        "            dst_path = os.path.join(dest_folder, filename)\n",
        "\n",
        "            # print(f\"Move: {src_path} to {dst_path}\")\n",
        "            count+=1\n",
        "            print(count)\n",
        "            shutil.copy(src_path, dst_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4ee0ee6e-277d-43c4-8269-88ef54a61c04",
      "metadata": {
        "id": "4ee0ee6e-277d-43c4-8269-88ef54a61c04",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# create_dataset_folders('../data/WordReport-v1.04.txt',\n",
        "#                        '../data/MindBigData-Imagenet',\n",
        "#                        '../data/Segregated_Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import json\n",
        "import os\n",
        "\n",
        "def reorganize_dataset(mapping_file, src_root, dst_root, move=False):\n",
        "    with open(mapping_file, 'r') as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    os.makedirs(os.path.dirname(dst_root), exist_ok=True)\n",
        "    # src_root = os.path.dirname(src_root)\n",
        "\n",
        "    for super_class, sub_classes in mapping.items():\n",
        "        super_cls_dir = os.path.join(dst_root, super_class)\n",
        "        os.makedirs(super_cls_dir, exist_ok=True)\n",
        "\n",
        "        for sub_class in sub_classes:\n",
        "            sub_cls_dir = os.path.join(src_root, sub_class)\n",
        "            if not os.path.exists(sub_cls_dir):\n",
        "                print(f\"[Warning] Sub-class folder not found: {sub_cls_dir}\")\n",
        "                continue\n",
        "\n",
        "            for file_name in os.listdir(sub_cls_dir):\n",
        "                src_file = os.path.join(sub_cls_dir, file_name)\n",
        "                dst_file = os.path.join(super_cls_dir, file_name)\n",
        "\n",
        "                if move:\n",
        "                    shutil.move(src_file, dst_file)\n",
        "\n",
        "                else:\n",
        "                    shutil.copy2(src_file, dst_file)\n",
        "\n",
        "            print(f\"[OK] {'Moved' if move else 'Copied'} {sub_class} -> {super_class}\")\n",
        "    print(\"Dataset reorganization complete!\")"
      ],
      "metadata": {
        "id": "vbgSZe26XQUQ"
      },
      "id": "vbgSZe26XQUQ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f93d8148-91b6-47b6-890f-46ed86a39094",
      "metadata": {
        "id": "f93d8148-91b6-47b6-890f-46ed86a39094",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# reorganize_dataset(mapping_file='../content/drive/MyDrive/NeuroVision/class_mapping_v4.json',\n",
        "#                    src_root='../content/drive/MyDrive/NeuroVision/Segregated_Dataset',\n",
        "#                    dst_root='../content/drive/MyDrive/NeuroVision/data/Classes_Regrouped_Dataset',\n",
        "#                    move=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0a4f18-0cbf-451e-be8c-1aa156eff8a7",
      "metadata": {
        "id": "ff0a4f18-0cbf-451e-be8c-1aa156eff8a7"
      },
      "source": [
        "## Dataset Processing for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a6f7ba77-0e7b-46c4-9823-85ed74ec2bda",
      "metadata": {
        "id": "a6f7ba77-0e7b-46c4-9823-85ed74ec2bda"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "027fe9b8-8ee9-4f76-b0b0-20f528f20261",
      "metadata": {
        "id": "027fe9b8-8ee9-4f76-b0b0-20f528f20261"
      },
      "outputs": [],
      "source": [
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, root_dir, samples, num_channels=5, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "        self.num_channels = num_channels\n",
        "        self.class_to_idx = list(set([label for _, label in self.samples]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path, label = self.samples[idx]\n",
        "\n",
        "        df = pd.read_csv(file_path, header=None, index_col=0)\n",
        "        eeg_data = torch.tensor(df.values, dtype=torch.float32)\n",
        "\n",
        "        if eeg_data.shape[1] != self.num_channels:\n",
        "          if eeg_data.shape[0] == self.num_channels:\n",
        "            eeg_data = eeg_data.T\n",
        "\n",
        "          else:\n",
        "            raise ValueError(f\"File {file_path} has invalid shape: {eeg_data.shape}\")\n",
        "\n",
        "        if self.transform:\n",
        "            eeg_data = self.transform(eeg_data)\n",
        "\n",
        "        return eeg_data, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "cd4342db-dfec-4a91-bd75-848017a33149",
      "metadata": {
        "id": "cd4342db-dfec-4a91-bd75-848017a33149"
      },
      "outputs": [],
      "source": [
        "def make_datasets(root_dir, val_ratio=0.25, random_state=42):\n",
        "    class_names = os.listdir(root_dir)\n",
        "    class_to_idx = {cls:idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "    all_samples = []\n",
        "    all_labels = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(root_dir, cls)\n",
        "\n",
        "        for fname in os.listdir(cls_dir):\n",
        "            if fname.endswith('.csv'):\n",
        "                path = os.path.join(cls_dir, fname)\n",
        "                all_samples.append((path, class_to_idx[cls]))\n",
        "                all_labels.append(class_to_idx[cls])\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(len(all_samples))),\n",
        "        test_size=val_ratio,\n",
        "        random_state=random_state,\n",
        "        stratify=all_labels\n",
        "    )\n",
        "\n",
        "    train_samples = [all_samples[i] for i in train_idx]\n",
        "    val_samples = [all_samples[i] for i in val_idx]\n",
        "\n",
        "    train_dataset = EEGDataset(root_dir, train_samples)\n",
        "    val_dataset = EEGDataset(root_dir, val_samples)\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "d0696ba6-64c0-427e-895e-0dead7a43cea",
      "metadata": {
        "id": "d0696ba6-64c0-427e-895e-0dead7a43cea"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    lengths = torch.tensor([seq.size(0) for seq in sequences], dtype=torch.long)\n",
        "    padded_seqs = pad_sequence(sequences, batch_first=True)\n",
        "\n",
        "    return padded_seqs, torch.tensor(labels), lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "44d72aba-d8c1-46e8-a055-52cf6ba2cae1",
      "metadata": {
        "id": "44d72aba-d8c1-46e8-a055-52cf6ba2cae1"
      },
      "outputs": [],
      "source": [
        "def create_sampler(dataset):\n",
        "  from collections import Counter\n",
        "\n",
        "  all_labels = [label for _, label in dataset.samples]\n",
        "\n",
        "  class_counts = Counter(all_labels)\n",
        "\n",
        "  num_classes = len(dataset.class_to_idx)\n",
        "  class_weights = torch.zeros(num_classes)\n",
        "\n",
        "  for class_idx, count in class_counts.items():\n",
        "    if count > 0:\n",
        "      class_weights[class_idx] = 1.0 / count\n",
        "\n",
        "  sample_weights = [class_weights[label] for label in all_labels]\n",
        "\n",
        "  sampler = WeightedRandomSampler(\n",
        "      weights=sample_weights,\n",
        "      num_samples=len(dataset.samples),\n",
        "      replacement=True\n",
        "  )\n",
        "\n",
        "  return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "cc8180c7-8721-41be-9a88-1dfd1e25a3ca",
      "metadata": {
        "id": "cc8180c7-8721-41be-9a88-1dfd1e25a3ca"
      },
      "outputs": [],
      "source": [
        "root_dir = '../content/drive/MyDrive/NeuroVision/data/Classes_Regrouped_Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "d3aa364a-238c-4491-b180-38f91b9e998f",
      "metadata": {
        "id": "d3aa364a-238c-4491-b180-38f91b9e998f"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = make_datasets(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_ZvO9_x5KTD",
        "outputId": "e9c20725-6987-4cb9-fccc-77f725af121a"
      },
      "id": "0_ZvO9_x5KTD",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10288, 3430)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = create_sampler(train_dataset)"
      ],
      "metadata": {
        "id": "bjw3UR-DaF0A"
      },
      "id": "bjw3UR-DaF0A",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "1b9db34b-54f7-4a6d-8e85-0a7f7d0ba655",
      "metadata": {
        "id": "1b9db34b-54f7-4a6d-8e85-0a7f7d0ba655"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, sampler=train_sampler,\n",
        "                          collate_fn=collate_fn, num_workers=2, pin_memory=False,\n",
        "                          persistent_workers=False, prefetch_factor=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn,\n",
        "                        num_workers=2, pin_memory=False, persistent_workers=False, prefetch_factor=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3a21f8-2bf8-43e3-a97e-921a0309a3ea",
      "metadata": {
        "id": "7f3a21f8-2bf8-43e3-a97e-921a0309a3ea"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "e2419cd3-599a-433b-a84e-8a69bf5f8f46",
      "metadata": {
        "id": "e2419cd3-599a-433b-a84e-8a69bf5f8f46"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "1e6a7229-ac9e-4bed-bd54-97f1e0cbc70c",
      "metadata": {
        "id": "1e6a7229-ac9e-4bed-bd54-97f1e0cbc70c"
      },
      "outputs": [],
      "source": [
        "class EegLstm(nn.Module):\n",
        "    def __init__(self, input_dims=5, hidden_dims=128, num_layers=3, dropout=0.3 , num_classes=len(os.listdir(root_dir))):\n",
        "        super(EegLstm, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dims,\n",
        "            hidden_size=hidden_dims,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers >= 2 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.conv_stack = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dims*2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dims*2, hidden_dims),\n",
        "            nn.BatchNorm1d(hidden_dims),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dims, hidden_dims),\n",
        "            nn.BatchNorm1d(hidden_dims),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dims, hidden_dims//2),\n",
        "            nn.BatchNorm1d(hidden_dims//2),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(hidden_dims, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        if lengths is not None:\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "            )\n",
        "\n",
        "            packed_out, (h_n, c_n) = self.lstm(packed)\n",
        "\n",
        "        else:\n",
        "            out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        last_hidden_backward, last_hidden_forward = h_n[-1], h_n[-2]\n",
        "        logits=self.fc(torch.cat((last_hidden_backward, last_hidden_forward), dim=1))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridExtractor(nn.Module):\n",
        "    def __init__(self, input_dims=5, cnn_out_channels=64, kernel_size=50, lstm_hidden_dims=128, num_layers=3, dropout=0.3 , num_classes=len(os.listdir(root_dir))):\n",
        "        super(HybridExtractor, self).__init__()\n",
        "\n",
        "        # CNN block\n",
        "        self.cnn_stack = nn.Sequential(\n",
        "            nn.Conv1d(input_dims, 32, kernel_size=kernel_size, stride=1, padding='same'),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv1d(32, cnn_out_channels, kernel_size=kernel_size//2, stride=1, padding='same'),\n",
        "            nn.BatchNorm1d(cnn_out_channels),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_out_channels,\n",
        "            hidden_size=lstm_hidden_dims,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden_dims*2, lstm_hidden_dims),\n",
        "            nn.BatchNorm1d(lstm_hidden_dims),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_hidden_dims, lstm_hidden_dims),\n",
        "            nn.BatchNorm1d(lstm_hidden_dims),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_hidden_dims, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "      x = x.permute(0, 2, 1)\n",
        "\n",
        "      cnn_out = self.cnn_stack(x)\n",
        "\n",
        "      lstm_input = cnn_out.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "      if lengths is not None:\n",
        "        new_lengths = (lengths//4).long()\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                lstm_input, new_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "            )\n",
        "\n",
        "        packed_out, (h_n, c_n) = self.lstm(packed)\n",
        "\n",
        "      else:\n",
        "          out, (h_n, c_n) = self.lstm(lstm_input)\n",
        "\n",
        "      last_hidden_backward, last_hidden_forward = h_n[-1, :, :], h_n[-2, :, :]\n",
        "      logits=self.fc(torch.cat((last_hidden_backward, last_hidden_forward), dim=1))\n",
        "\n",
        "      return logits"
      ],
      "metadata": {
        "id": "rQRrJAdUPJqj"
      },
      "id": "rQRrJAdUPJqj",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "  if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='selu')\n",
        "\n",
        "    if m.bias is not None:\n",
        "      nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "zNsDXuvBSzsx"
      },
      "id": "zNsDXuvBSzsx",
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "bf8cb365-9915-4f3a-afd4-6946979a0590",
      "metadata": {
        "id": "bf8cb365-9915-4f3a-afd4-6946979a0590"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "89fb3cb9-1dec-4a6a-9c4e-7a445954daaf",
      "metadata": {
        "id": "89fb3cb9-1dec-4a6a-9c4e-7a445954daaf"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping(object):\n",
        "    def __init__(self, model, save_path='../content/drive/MyDrive/eeg_classifier.pt', patience=4, tol=1e-3):\n",
        "        self.model = model\n",
        "        self.save_path = save_path\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.tol = tol\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, batch_val_loss):\n",
        "        if batch_val_loss < self.best_val_loss - self.tol:\n",
        "            torch.save(self.model.state_dict(), self.save_path)\n",
        "            self.best_val_loss = batch_val_loss\n",
        "            self.counter = 0\n",
        "            print(f'Validation Loss improved -> model saved to {self.save_path}')\n",
        "\n",
        "        else:\n",
        "            if self.counter < self.patience:\n",
        "                self.counter += 1\n",
        "                print(f'No improvement in Val Loss. Counter: {self.counter}/{self.patience}')\n",
        "\n",
        "            else:\n",
        "                self.early_stop = True\n",
        "                print(f\"Early Stopping triggered!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "0401a423-49b1-40f1-b535-4db7af34c7d6",
      "metadata": {
        "id": "0401a423-49b1-40f1-b535-4db7af34c7d6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, model_name, train_loader, val_loader, epochs=15, lr=1e-3, device='cpu'):\n",
        "    log_dir = f'../content/drive/MyDrive/NeuroVision/runs/{model_name}'\n",
        "    save_path = f'../content/drive/MyDrive/NeuroVision/models/{model_name}_v2_best.pth'\n",
        "    os.makedirs(os.path.dirname(log_dir), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(train_cls_wts).to(device))\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n",
        "                                              total_steps=len(train_loader)*epochs,\n",
        "                                              pct_start=0.2)\n",
        "    early_stopping = EarlyStopping(model, save_path=save_path, patience=6)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train Pass]', leave=True)\n",
        "\n",
        "        for batch_x, batch_y, lengths in train_bar:\n",
        "            batch_x, batch_y, lengths = batch_x.to(device), batch_y.to(device), lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_preds = model(batch_x, lengths)\n",
        "\n",
        "            loss = criterion(y_preds, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, preds = torch.max(y_preds, 1)\n",
        "            train_correct += (preds == batch_y).sum().item()\n",
        "            train_total += batch_y.size(0)\n",
        "\n",
        "            train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss /= train_total\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch{epoch+1}/{epochs} [Val Pass]\", leave=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y, lengths in val_bar:\n",
        "                batch_x, batch_y, lengths = batch_x.to(device), batch_y.to(device), lengths.to(device)\n",
        "\n",
        "                y_preds = model(batch_x, lengths)\n",
        "                loss = criterion(y_preds, batch_y)\n",
        "\n",
        "                val_loss += loss.item() * batch_x.size(0)\n",
        "                _, preds = torch.max(y_preds, 1)\n",
        "                print(f\"Sample Predictions: {preds.cpu().numpy()}\")\n",
        "                val_correct += (preds == batch_y).sum().item()\n",
        "                val_total += batch_y.size(0)\n",
        "\n",
        "                val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "\n",
        "        # scheduler.step(val_loss)\n",
        "\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            break\n",
        "\n",
        "\n",
        "        # logging\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\\nTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} %\\nVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f} %\")\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "bdc98eb7-1f94-448f-9631-a983d33d4c14",
      "metadata": {
        "id": "bdc98eb7-1f94-448f-9631-a983d33d4c14"
      },
      "outputs": [],
      "source": [
        "def model_summary(model):\n",
        "    print('========================================= Model Summary ==============================================\\n')\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"{'| Parameter Name':31}|| Number of Parameters|\")\n",
        "    print(f\"{'='*55}\")\n",
        "\n",
        "    total_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f'| {name:30}|{param.numel():20} |')\n",
        "        print(f\"{'-'*55}\")\n",
        "        total_params += param.numel()\n",
        "\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df9cb8d-ac5b-4ba4-a4ec-8b2d0877b6f0",
      "metadata": {
        "id": "9df9cb8d-ac5b-4ba4-a4ec-8b2d0877b6f0"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "450a101a-3a4c-4e5b-85f0-4189cae1651b",
      "metadata": {
        "id": "450a101a-3a4c-4e5b-85f0-4189cae1651b"
      },
      "outputs": [],
      "source": [
        "# lstm_model = EegLstm(input_dims=5, hidden_dims=256, num_layers=4, dropout=0.4)\n",
        "# hybrid_model = HybridExtractor(input_dims=5, cnn_out_channels=64, lstm_hidden_dims=256, num_layers=3, dropout=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mesonet = EEG_MesoNet(5, len(os.listdir(root_dir)), 0.5)"
      ],
      "metadata": {
        "id": "VuQVgc3FTn1f"
      },
      "id": "VuQVgc3FTn1f",
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mesonet.apply(weights_init)\n",
        "# hybrid_model.load_state_dict(torch.load('../content/drive/MyDrive/NeuroVision/models/EEG_LSTM_v1_best.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR96jD3ZUJqN",
        "outputId": "4b8c3e7e-4653-4206-c9ec-d4d072f7112a"
      },
      "id": "sR96jD3ZUJqN",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EEG_MesoNet(\n",
              "  (branch_fine): ConvBranch(\n",
              "    (conv): Conv1d(5, 16, kernel_size=(10,), stride=(1,), padding=same, bias=False)\n",
              "    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool): AvgPool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
              "  )\n",
              "  (branch_medium): ConvBranch(\n",
              "    (conv): Conv1d(5, 16, kernel_size=(50,), stride=(1,), padding=same, bias=False)\n",
              "    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool): AvgPool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
              "  )\n",
              "  (branch_coarse): ConvBranch(\n",
              "    (conv): Conv1d(5, 16, kernel_size=(150,), stride=(1,), padding=same, bias=False)\n",
              "    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool): AvgPool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=48, out_features=256, bias=True)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=118, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "4b3ffcf2-4219-4dd0-b84a-57ca835d803a",
      "metadata": {
        "id": "4b3ffcf2-4219-4dd0-b84a-57ca835d803a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd4d381-22bc-4220-e0e7-2a6a9322a8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================= Model Summary ==============================================\n",
            "\n",
            "\n",
            "=======================================================\n",
            "| Parameter Name               || Number of Parameters|\n",
            "=======================================================\n",
            "| branch_fine.conv.weight       |                 800 |\n",
            "-------------------------------------------------------\n",
            "| branch_fine.bn.weight         |                  16 |\n",
            "-------------------------------------------------------\n",
            "| branch_fine.bn.bias           |                  16 |\n",
            "-------------------------------------------------------\n",
            "| branch_medium.conv.weight     |                4000 |\n",
            "-------------------------------------------------------\n",
            "| branch_medium.bn.weight       |                  16 |\n",
            "-------------------------------------------------------\n",
            "| branch_medium.bn.bias         |                  16 |\n",
            "-------------------------------------------------------\n",
            "| branch_coarse.conv.weight     |               12000 |\n",
            "-------------------------------------------------------\n",
            "| branch_coarse.bn.weight       |                  16 |\n",
            "-------------------------------------------------------\n",
            "| branch_coarse.bn.bias         |                  16 |\n",
            "-------------------------------------------------------\n",
            "| fc.0.weight                   |               12288 |\n",
            "-------------------------------------------------------\n",
            "| fc.0.bias                     |                 256 |\n",
            "-------------------------------------------------------\n",
            "| fc.1.weight                   |                 256 |\n",
            "-------------------------------------------------------\n",
            "| fc.1.bias                     |                 256 |\n",
            "-------------------------------------------------------\n",
            "| fc.4.weight                   |               30208 |\n",
            "-------------------------------------------------------\n",
            "| fc.4.bias                     |                 118 |\n",
            "-------------------------------------------------------\n",
            "\n",
            "Total Parameters: 60,278\n"
          ]
        }
      ],
      "source": [
        "model_summary(mesonet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_4ofmjqdN-6",
        "outputId": "6a1636b7-6eac-4bef-a777-3adc661a57cf"
      },
      "id": "A_4ofmjqdN-6",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 28 20:53:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0             34W /   70W |    7556MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "a1b77474-4ff5-4ad0-8f17-2cb024182b02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "a1b77474-4ff5-4ad0-8f17-2cb024182b02",
        "outputId": "db0de0cd-f8e4-4a09-d316-9cd309a3fbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30 [Train Pass]:   5%|â–         | 4/81 [03:40<1:10:44, 55.13s/it, loss=5.07]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3477905871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesonet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mesonet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2420961492.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs} [Train Pass]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_model(mesonet, 'mesonet', train_loader, val_loader, 30, 1e-4, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc284f5f-0ff3-4928-9c02-e89f81e00640",
      "metadata": {
        "id": "cc284f5f-0ff3-4928-9c02-e89f81e00640"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43cd95c4-6134-4a72-a305-7896d14f00ba",
      "metadata": {
        "id": "43cd95c4-6134-4a72-a305-7896d14f00ba"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "7162949c-6fe9-43c2-a775-7dbf886ab75a",
        "cc284f5f-0ff3-4928-9c02-e89f81e00640"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}