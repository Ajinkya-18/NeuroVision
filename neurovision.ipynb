{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajinkya-18/NeuroVision/blob/main/neurovision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f68d0c-a42d-4ede-a02d-d2ea6dbf9275",
      "metadata": {
        "id": "58f68d0c-a42d-4ede-a02d-d2ea6dbf9275"
      },
      "source": [
        "# NeuroVision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7162949c-6fe9-43c2-a775-7dbf886ab75a",
      "metadata": {
        "id": "7162949c-6fe9-43c2-a775-7dbf886ab75a"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDgv-pwx8Zvr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDgv-pwx8Zvr",
        "outputId": "f2162593-b12c-45b0-9bfa-b624ecc98d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting to copy dataset folder from Drive to local storage...\n",
            "This may take a significant amount of time, please be patient.\n",
            "Copying complete!\n"
          ]
        }
      ],
      "source": [
        "# Make sure your Google Drive is mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- This is the key step ---\n",
        "# Use 'cp -r' to recursively copy the entire folder.\n",
        "# This will be SLOW, as it's copying thousands of individual files over the network.\n",
        "# Let it run until it's finished.\n",
        "\n",
        "print(\"Starting to copy dataset folder from Drive to local storage...\")\n",
        "print(\"This may take a significant amount of time, please be patient.\")\n",
        "\n",
        "# !cp -r \"/content/drive/MyDrive/NeuroVision/Segregated_Dataset\" \"/content/eeg_dataset\"\n",
        "\n",
        "print(\"Copying complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fce1e7a-a9dd-47e5-970b-682938b29ae3",
      "metadata": {
        "id": "6fce1e7a-a9dd-47e5-970b-682938b29ae3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbc4228-6ab9-4fb4-8dbc-13d2b07e5d17",
      "metadata": {
        "id": "9fbc4228-6ab9-4fb4-8dbc-13d2b07e5d17"
      },
      "outputs": [],
      "source": [
        "# function to read the dir contents of dataset folder and segregate them\n",
        "# into n separate classes.\n",
        "def create_dataset_folders(metadata_file:str, csv_dir:str, output_dir:str):\n",
        "    class_id_to_folder = {}\n",
        "\n",
        "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "\n",
        "            label_str, _, class_id = parts\n",
        "            # print(label_str, class_id)\n",
        "            first_label = label_str.split(',')[0].strip()\n",
        "            # print(first_label)\n",
        "            class_id_to_folder[class_id] = first_label\n",
        "\n",
        "        count = 0\n",
        "        for filename in os.listdir(csv_dir):\n",
        "            if not filename.endswith('.csv'):\n",
        "                continue\n",
        "\n",
        "            class_id = filename.split('_')[3]\n",
        "\n",
        "            folder_name = class_id_to_folder.get(class_id)\n",
        "            print(folder_name)\n",
        "\n",
        "            if not folder_name:\n",
        "                print(f'Unknown class id: {class_id}')\n",
        "                continue\n",
        "\n",
        "            safe_folder = folder_name.replace('/', '_').replace('\\\\', '_').strip()\n",
        "\n",
        "            dest_folder = os.path.join(output_dir, safe_folder)\n",
        "            os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "            src_path = os.path.join(csv_dir, filename)\n",
        "            dst_path = os.path.join(dest_folder, filename)\n",
        "\n",
        "            # print(f\"Move: {src_path} to {dst_path}\")\n",
        "            count+=1\n",
        "            print(count)\n",
        "            shutil.copy(src_path, dst_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee0ee6e-277d-43c4-8269-88ef54a61c04",
      "metadata": {
        "id": "4ee0ee6e-277d-43c4-8269-88ef54a61c04",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# create_dataset_folders('../data/WordReport-v1.04.txt',\n",
        "#                        '../data/MindBigData-Imagenet',\n",
        "#                        '../data/Segregated_Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vbgSZe26XQUQ",
      "metadata": {
        "id": "vbgSZe26XQUQ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import json\n",
        "import os\n",
        "\n",
        "def reorganize_dataset(mapping_file, src_root, dst_root, move=False):\n",
        "    with open(mapping_file, 'r') as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    os.makedirs(os.path.dirname(dst_root), exist_ok=True)\n",
        "    # src_root = os.path.dirname(src_root)\n",
        "\n",
        "    for super_class, sub_classes in mapping.items():\n",
        "        super_cls_dir = os.path.join(dst_root, super_class)\n",
        "        os.makedirs(super_cls_dir, exist_ok=True)\n",
        "\n",
        "        for sub_class in sub_classes:\n",
        "            sub_cls_dir = os.path.join(src_root, sub_class)\n",
        "            if not os.path.exists(sub_cls_dir):\n",
        "                print(f\"[Warning] Sub-class folder not found: {sub_cls_dir}\")\n",
        "                continue\n",
        "\n",
        "            for file_name in os.listdir(sub_cls_dir):\n",
        "                src_file = os.path.join(sub_cls_dir, file_name)\n",
        "                dst_file = os.path.join(super_cls_dir, file_name)\n",
        "\n",
        "                if move:\n",
        "                    shutil.move(src_file, dst_file)\n",
        "\n",
        "                else:\n",
        "                    shutil.copy2(src_file, dst_file)\n",
        "\n",
        "            print(f\"[OK] {'Moved' if move else 'Copied'} {sub_class} -> {super_class}\")\n",
        "    print(\"Dataset reorganization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93d8148-91b6-47b6-890f-46ed86a39094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f93d8148-91b6-47b6-890f-46ed86a39094",
        "outputId": "78bf57f8-2f71-46ba-d709-ef399f9b14f0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Copied affenpinscher -> dogs_n_cats\n",
            "[OK] Copied Afghan hound -> dogs_n_cats\n",
            "[OK] Copied Airedale -> dogs_n_cats\n",
            "[OK] Copied American Staffordshire terrier -> dogs_n_cats\n",
            "[OK] Copied Appenzeller -> dogs_n_cats\n",
            "[OK] Copied Australian terrier -> dogs_n_cats\n",
            "[OK] Copied basenji -> dogs_n_cats\n",
            "[OK] Copied basset -> dogs_n_cats\n",
            "[OK] Copied beagle -> dogs_n_cats\n",
            "[OK] Copied Bedlington terrier -> dogs_n_cats\n",
            "[OK] Copied Bernese mountain dog -> dogs_n_cats\n",
            "[OK] Copied black-and-tan coonhound -> dogs_n_cats\n",
            "[OK] Copied Blenheim spaniel -> dogs_n_cats\n",
            "[OK] Copied bloodhound -> dogs_n_cats\n",
            "[OK] Copied bluetick -> dogs_n_cats\n",
            "[OK] Copied Border collie -> dogs_n_cats\n",
            "[OK] Copied Border terrier -> dogs_n_cats\n",
            "[OK] Copied borzoi -> dogs_n_cats\n",
            "[OK] Copied Boston bull -> dogs_n_cats\n",
            "[OK] Copied Bouvier des Flandres -> dogs_n_cats\n",
            "[OK] Copied boxer -> dogs_n_cats\n",
            "[OK] Copied Brabancon griffon -> dogs_n_cats\n",
            "[OK] Copied briard -> dogs_n_cats\n",
            "[OK] Copied Brittany spaniel -> dogs_n_cats\n",
            "[OK] Copied bull mastiff -> dogs_n_cats\n",
            "[OK] Copied cairn -> dogs_n_cats\n",
            "[OK] Copied Cardigan -> dogs_n_cats\n",
            "[OK] Copied Chesapeake Bay retriever -> dogs_n_cats\n",
            "[OK] Copied Chihuahua -> dogs_n_cats\n",
            "[OK] Copied chow -> dogs_n_cats\n",
            "[OK] Copied clumber -> dogs_n_cats\n",
            "[OK] Copied cocker spaniel -> dogs_n_cats\n",
            "[OK] Copied collie -> dogs_n_cats\n",
            "[OK] Copied curly-coated retriever -> dogs_n_cats\n",
            "[OK] Copied dalmatian -> dogs_n_cats\n",
            "[OK] Copied Dandie Dinmont -> dogs_n_cats\n",
            "[OK] Copied Doberman -> dogs_n_cats\n",
            "[OK] Copied dog -> dogs_n_cats\n",
            "[OK] Copied English foxhound -> dogs_n_cats\n",
            "[OK] Copied English setter -> dogs_n_cats\n",
            "[OK] Copied English springer -> dogs_n_cats\n",
            "[OK] Copied EntleBucher -> dogs_n_cats\n",
            "[OK] Copied Eskimo dog -> dogs_n_cats\n",
            "[OK] Copied flat-coated retriever -> dogs_n_cats\n",
            "[OK] Copied French bulldog -> dogs_n_cats\n",
            "[OK] Copied German shepherd -> dogs_n_cats\n",
            "[OK] Copied German short-haired pointer -> dogs_n_cats\n",
            "[OK] Copied giant schnauzer -> dogs_n_cats\n",
            "[OK] Copied golden retriever -> dogs_n_cats\n",
            "[OK] Copied Gordon setter -> dogs_n_cats\n",
            "[OK] Copied tabby -> dogs_n_cats\n",
            "[OK] Copied Great Dane -> dogs_n_cats\n",
            "[OK] Copied Great Pyrenees -> dogs_n_cats\n",
            "[OK] Copied Greater Swiss Mountain dog -> dogs_n_cats\n",
            "[OK] Copied groenendael -> dogs_n_cats\n",
            "[OK] Copied Ibizan hound -> dogs_n_cats\n",
            "[OK] Copied Irish setter -> dogs_n_cats\n",
            "[OK] Copied Irish terrier -> dogs_n_cats\n",
            "[OK] Copied Irish water spaniel -> dogs_n_cats\n",
            "[OK] Copied Irish wolfhound -> dogs_n_cats\n",
            "[OK] Copied Italian greyhound -> dogs_n_cats\n",
            "[OK] Copied Japanese spaniel -> dogs_n_cats\n",
            "[OK] Copied keeshond -> dogs_n_cats\n",
            "[OK] Copied kelpie -> dogs_n_cats\n",
            "[OK] Copied Kerry blue terrier -> dogs_n_cats\n",
            "[OK] Copied komondor -> dogs_n_cats\n",
            "[OK] Copied kuvasz -> dogs_n_cats\n",
            "[OK] Copied Siamese cat -> dogs_n_cats\n",
            "[OK] Copied Labrador retriever -> dogs_n_cats\n",
            "[OK] Copied Lakeland terrier -> dogs_n_cats\n",
            "[OK] Copied Leonberg -> dogs_n_cats\n",
            "[OK] Copied Lhasa -> dogs_n_cats\n",
            "[OK] Copied malamute -> dogs_n_cats\n",
            "[OK] Copied malinois -> dogs_n_cats\n",
            "[OK] Copied Maltese dog -> dogs_n_cats\n",
            "[OK] Copied Mexican hairless -> dogs_n_cats\n",
            "[OK] Copied miniature pinscher -> dogs_n_cats\n",
            "[OK] Copied miniature poodle -> dogs_n_cats\n",
            "[OK] Copied miniature schnauzer -> dogs_n_cats\n",
            "[OK] Copied Newfoundland -> dogs_n_cats\n",
            "[OK] Copied Norfolk terrier -> dogs_n_cats\n",
            "[OK] Copied Norwegian elkhound -> dogs_n_cats\n",
            "[OK] Copied Norwich terrier -> dogs_n_cats\n",
            "[OK] Copied Old English sheepdog -> dogs_n_cats\n",
            "[OK] Copied otterhound -> dogs_n_cats\n",
            "[OK] Copied papillon -> dogs_n_cats\n",
            "[OK] Copied Pekinese -> dogs_n_cats\n",
            "[OK] Copied Pembroke -> dogs_n_cats\n",
            "[OK] Copied Pomeranian -> dogs_n_cats\n",
            "[OK] Copied pug -> dogs_n_cats\n",
            "[OK] Copied redbone -> dogs_n_cats\n",
            "[OK] Copied Rhodesian ridgeback -> dogs_n_cats\n",
            "[OK] Copied Rottweiler -> dogs_n_cats\n",
            "[OK] Copied Saint Bernard -> dogs_n_cats\n",
            "[OK] Copied Saluki -> dogs_n_cats\n",
            "[OK] Copied Samoyed -> dogs_n_cats\n",
            "[OK] Copied schipperke -> dogs_n_cats\n",
            "[OK] Copied Scotch terrier -> dogs_n_cats\n",
            "[OK] Copied Scottish deerhound -> dogs_n_cats\n",
            "[OK] Copied Sealyham terrier -> dogs_n_cats\n",
            "[OK] Copied Shetland sheepdog -> dogs_n_cats\n",
            "[OK] Copied Shih-Tzu -> dogs_n_cats\n",
            "[OK] Copied Siberian husky -> dogs_n_cats\n",
            "[OK] Copied silky terrier -> dogs_n_cats\n",
            "[OK] Copied soft-coated wheaten terrier -> dogs_n_cats\n",
            "[OK] Copied Staffordshire bullterrier -> dogs_n_cats\n",
            "[OK] Copied standard poodle -> dogs_n_cats\n",
            "[OK] Copied standard schnauzer -> dogs_n_cats\n",
            "[OK] Copied Sussex spaniel -> dogs_n_cats\n",
            "[OK] Copied Tibetan mastiff -> dogs_n_cats\n",
            "[OK] Copied Tibetan terrier -> dogs_n_cats\n",
            "[OK] Copied toy poodle -> dogs_n_cats\n",
            "[OK] Copied toy terrier -> dogs_n_cats\n",
            "[OK] Copied vizsla -> dogs_n_cats\n",
            "[OK] Copied Walker hound -> dogs_n_cats\n",
            "[OK] Copied Weimaraner -> dogs_n_cats\n",
            "[OK] Copied Welsh springer spaniel -> dogs_n_cats\n",
            "[OK] Copied West Highland white terrier -> dogs_n_cats\n",
            "[OK] Copied whippet -> dogs_n_cats\n",
            "[OK] Copied wire-haired fox terrier -> dogs_n_cats\n",
            "[OK] Copied Yorkshire terrier -> dogs_n_cats\n",
            "[OK] Copied domestic cat -> dogs_n_cats\n",
            "[OK] Copied Egyptian cat -> dogs_n_cats\n",
            "[OK] Copied Persian cat -> dogs_n_cats\n",
            "[OK] Copied antelope -> animals\n",
            "[OK] Copied Arabian camel -> animals\n",
            "[OK] Copied bighorn -> animals\n",
            "[OK] Copied camel -> animals\n",
            "[OK] Copied cattle -> animals\n",
            "[OK] Copied elephant -> animals\n",
            "[OK] Copied gazelle -> animals\n",
            "[OK] Copied hartebeest -> animals\n",
            "[OK] Copied hippopotamus -> animals\n",
            "[OK] Copied hog -> animals\n",
            "[OK] Copied horse -> animals\n",
            "[OK] Copied impala -> animals\n",
            "[OK] Copied ox -> animals\n",
            "[OK] Copied ram -> animals\n",
            "[OK] Copied sheep -> animals\n",
            "[OK] Copied sorrel -> animals\n",
            "[OK] Copied swine -> animals\n",
            "[OK] Copied warthog -> animals\n",
            "[OK] Copied wild boar -> animals\n",
            "[OK] Copied zebra -> animals\n",
            "[OK] Copied Angora -> animals\n",
            "[OK] Copied Arctic fox -> animals\n",
            "[OK] Copied armadillo -> animals\n",
            "[OK] Copied fox -> animals\n",
            "[OK] Copied fox squirrel -> animals\n",
            "[OK] Copied grey fox -> animals\n",
            "[OK] Copied hamster -> animals\n",
            "[OK] Copied hare -> animals\n",
            "[OK] Copied kit fox -> animals\n",
            "[OK] Copied mouse -> animals\n",
            "[OK] Copied otter -> animals\n",
            "[OK] Copied porcupine -> animals\n",
            "[OK] Copied rabbit -> animals\n",
            "[OK] Copied red fox -> animals\n",
            "[OK] Copied skunk -> animals\n",
            "[OK] Copied squirrel -> animals\n",
            "[OK] Copied wood rabbit -> animals\n",
            "[OK] Copied baboon -> animals\n",
            "[OK] Copied capuchin -> animals\n",
            "[OK] Copied colobus -> animals\n",
            "[OK] Copied guenon -> animals\n",
            "[OK] Copied howler monkey -> animals\n",
            "[OK] Copied langur -> animals\n",
            "[OK] Copied macaque -> animals\n",
            "[OK] Copied marmoset -> animals\n",
            "[OK] Copied monkey -> animals\n",
            "[OK] Copied patas -> animals\n",
            "[OK] Copied proboscis monkey -> animals\n",
            "[OK] Copied spider monkey -> animals\n",
            "[OK] Copied squirrel monkey -> animals\n",
            "[OK] Copied titi -> animals\n",
            "[OK] Copied American black bear -> animals\n",
            "[OK] Copied bear -> animals\n",
            "[OK] Copied brown bear -> animals\n",
            "[OK] Copied giant panda -> animals\n",
            "[OK] Copied ice bear -> animals\n",
            "[OK] Copied koala -> animals\n",
            "[OK] Copied lesser panda -> animals\n",
            "[OK] Copied sloth bear -> animals\n",
            "[OK] Copied tiger -> animals\n",
            "[OK] Copied lion -> animals\n",
            "[OK] Copied African grey -> birds\n",
            "[OK] Copied albatross -> birds\n",
            "[OK] Copied American coot -> birds\n",
            "[OK] Copied American egret -> birds\n",
            "[OK] Copied bee eater -> birds\n",
            "[OK] Copied bird -> birds\n",
            "[OK] Copied bittern -> birds\n",
            "[OK] Copied black grouse -> birds\n",
            "[OK] Copied black stork -> birds\n",
            "[OK] Copied black swan -> birds\n",
            "[OK] Copied brambling -> birds\n",
            "[OK] Copied bulbul -> birds\n",
            "[OK] Copied bustard -> birds\n",
            "[OK] Copied chickadee -> birds\n",
            "[OK] Copied cock -> birds\n",
            "[OK] Copied coucal -> birds\n",
            "[OK] Copied drake -> birds\n",
            "[OK] Copied flamingo -> birds\n",
            "[OK] Copied goldfinch -> birds\n",
            "[OK] Copied goose -> birds\n",
            "[OK] Copied great grey owl -> birds\n",
            "[OK] Copied hen -> birds\n",
            "[OK] Copied hornbill -> birds\n",
            "[OK] Copied house finch -> birds\n",
            "[OK] Copied hummingbird -> birds\n",
            "[OK] Copied indigo bunting -> birds\n",
            "[OK] Copied jacamar -> birds\n",
            "[OK] Copied jay -> birds\n",
            "[OK] Copied junco -> birds\n",
            "[OK] Copied king penguin -> birds\n",
            "[OK] Copied kite -> birds\n",
            "[OK] Copied limpkin -> birds\n",
            "[OK] Copied little blue heron -> birds\n",
            "[OK] Copied lorikeet -> birds\n",
            "[OK] Copied macaw -> birds\n",
            "[OK] Copied magpie -> birds\n",
            "[OK] Copied ostrich -> birds\n",
            "[OK] Copied oystercatcher -> birds\n",
            "[OK] Copied partridge -> birds\n",
            "[OK] Copied peacock -> birds\n",
            "[OK] Copied pelican -> birds\n",
            "[OK] Copied prairie chicken -> birds\n",
            "[OK] Copied ptarmigan -> birds\n",
            "[OK] Copied quail -> birds\n",
            "[OK] Copied red-backed sandpiper -> birds\n",
            "[OK] Copied red-breasted merganser -> birds\n",
            "[OK] Copied redshank -> birds\n",
            "[OK] Copied robin -> birds\n",
            "[OK] Copied ruffed grouse -> birds\n",
            "[OK] Copied ruddy turnstone -> birds\n",
            "[OK] Copied spoonbill -> birds\n",
            "[OK] Copied sulphur-crested cockatoo -> birds\n",
            "[OK] Copied toucan -> birds\n",
            "[OK] Copied vulture -> birds\n",
            "[OK] Copied water ouzel -> birds\n",
            "[OK] Copied white stork -> birds\n",
            "[OK] Copied African chameleon -> reptiles_aquatic_and_insects\n",
            "[OK] Copied agama -> reptiles_aquatic_and_insects\n",
            "[OK] Copied alligator lizard -> reptiles_aquatic_and_insects\n",
            "[OK] Copied American chameleon -> reptiles_aquatic_and_insects\n",
            "[OK] Copied American lobster -> reptiles_aquatic_and_insects\n",
            "[OK] Copied ant -> reptiles_aquatic_and_insects\n",
            "[OK] Copied banded gecko -> reptiles_aquatic_and_insects\n",
            "[OK] Copied bee -> reptiles_aquatic_and_insects\n",
            "[OK] Copied boa constrictor -> reptiles_aquatic_and_insects\n",
            "[OK] Copied box turtle -> reptiles_aquatic_and_insects\n",
            "[OK] Copied bullfrog -> reptiles_aquatic_and_insects\n",
            "[OK] Copied butterfly -> reptiles_aquatic_and_insects\n",
            "[OK] Copied cabbage butterfly -> reptiles_aquatic_and_insects\n",
            "[OK] Copied centipede -> reptiles_aquatic_and_insects\n",
            "[OK] Copied common iguana -> reptiles_aquatic_and_insects\n",
            "[OK] Copied diamondback -> reptiles_aquatic_and_insects\n",
            "[OK] Copied dowitcher -> reptiles_aquatic_and_insects\n",
            "[OK] Copied dragonfly -> reptiles_aquatic_and_insects\n",
            "[OK] Copied electric ray -> reptiles_aquatic_and_insects\n",
            "[OK] Copied isopod -> reptiles_aquatic_and_insects\n",
            "[OK] Copied European gallinule -> reptiles_aquatic_and_insects\n",
            "[OK] Copied frilled lizard -> reptiles_aquatic_and_insects\n",
            "[OK] Copied frog -> reptiles_aquatic_and_insects\n",
            "[OK] Copied garter snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied Gila monster -> reptiles_aquatic_and_insects\n",
            "[OK] Copied goldfish -> reptiles_aquatic_and_insects\n",
            "[OK] Copied snail -> reptiles_aquatic_and_insects\n",
            "[OK] Copied green lizard -> reptiles_aquatic_and_insects\n",
            "[OK] Copied green mamba -> reptiles_aquatic_and_insects\n",
            "[OK] Copied grey whale -> reptiles_aquatic_and_insects\n",
            "[OK] Copied hognose snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied horned viper -> reptiles_aquatic_and_insects\n",
            "[OK] Copied Indian cobra -> reptiles_aquatic_and_insects\n",
            "[OK] Copied jellyfish -> reptiles_aquatic_and_insects\n",
            "[OK] Copied killer whale -> reptiles_aquatic_and_insects\n",
            "[OK] Copied king snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied Komodo dragon -> reptiles_aquatic_and_insects\n",
            "[OK] Copied ladybug -> reptiles_aquatic_and_insects\n",
            "[OK] Copied leatherback turtle -> reptiles_aquatic_and_insects\n",
            "[OK] Copied lizard -> reptiles_aquatic_and_insects\n",
            "[OK] Copied lobster -> reptiles_aquatic_and_insects\n",
            "[OK] Copied loggerhead -> reptiles_aquatic_and_insects\n",
            "[OK] Copied lycaenid -> reptiles_aquatic_and_insects\n",
            "[OK] Copied monarch -> reptiles_aquatic_and_insects\n",
            "[OK] Copied mud turtle -> reptiles_aquatic_and_insects\n",
            "[OK] Copied night snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied ray -> reptiles_aquatic_and_insects\n",
            "[OK] Copied ringlet -> reptiles_aquatic_and_insects\n",
            "[OK] Copied ringneck snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied rock python -> reptiles_aquatic_and_insects\n",
            "[OK] Copied scorpion -> reptiles_aquatic_and_insects\n",
            "[OK] Copied sea lion -> reptiles_aquatic_and_insects\n",
            "[OK] Copied sea snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied seal -> reptiles_aquatic_and_insects\n",
            "[OK] Copied sidewinder -> reptiles_aquatic_and_insects\n",
            "[OK] Copied snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied spiny lobster -> reptiles_aquatic_and_insects\n",
            "[OK] Copied starfish -> reptiles_aquatic_and_insects\n",
            "[OK] Copied stingray -> reptiles_aquatic_and_insects\n",
            "[OK] Copied sulphur butterfly -> reptiles_aquatic_and_insects\n",
            "[OK] Copied tailed frog -> reptiles_aquatic_and_insects\n",
            "[OK] Copied terrapin -> reptiles_aquatic_and_insects\n",
            "[OK] Copied thunder snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied tick -> reptiles_aquatic_and_insects\n",
            "[OK] Copied tree frog -> reptiles_aquatic_and_insects\n",
            "[OK] Copied turtle -> reptiles_aquatic_and_insects\n",
            "[OK] Copied vine snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied water snake -> reptiles_aquatic_and_insects\n",
            "[OK] Copied whale -> reptiles_aquatic_and_insects\n",
            "[OK] Copied whiptail -> reptiles_aquatic_and_insects\n",
            "[OK] Copied admiral -> people\n",
            "[OK] Copied ballplayer -> people\n",
            "[OK] Copied groom -> people\n",
            "[OK] Copied person -> people\n",
            "[OK] Copied pirate -> people\n",
            "[OK] Copied racer -> people\n",
            "[OK] Copied scuba diver -> people\n",
            "[OK] Copied aircraft carrier -> vehicles\n",
            "[OK] Copied airliner -> vehicles\n",
            "[OK] Copied airplane -> vehicles\n",
            "[OK] Copied ambulance -> vehicles\n",
            "[OK] Copied beach wagon -> vehicles\n",
            "[OK] Copied bicycle -> vehicles\n",
            "[OK] Copied bicycle-built-for-two -> vehicles\n",
            "[OK] Copied bullet train -> vehicles\n",
            "[OK] Copied bus -> vehicles\n",
            "[OK] Copied cab -> vehicles\n",
            "[OK] Copied canoe -> vehicles\n",
            "[OK] Copied car -> vehicles\n",
            "[OK] Copied cart -> vehicles\n",
            "[OK] Copied catamaran -> vehicles\n",
            "[OK] Copied container ship -> vehicles\n",
            "[OK] Copied convertible -> vehicles\n",
            "[OK] Copied fireboat -> vehicles\n",
            "[OK] Copied gondola -> vehicles\n",
            "[OK] Copied golfcart -> vehicles\n",
            "[OK] Copied horse cart -> vehicles\n",
            "[OK] Copied jeep -> vehicles\n",
            "[OK] Copied jinrikisha -> vehicles\n",
            "[OK] Copied lifeboat -> vehicles\n",
            "[OK] Copied limousine -> vehicles\n",
            "[OK] Copied liner -> vehicles\n",
            "[OK] Copied minibus -> vehicles\n",
            "[OK] Copied minivan -> vehicles\n",
            "[OK] Copied Model T -> vehicles\n",
            "[OK] Copied moped -> vehicles\n",
            "[OK] Copied motor scooter -> vehicles\n",
            "[OK] Copied motorcycle -> vehicles\n",
            "[OK] Copied mountain bike -> vehicles\n",
            "[OK] Copied oxcart -> vehicles\n",
            "[OK] Copied schooner -> vehicles\n",
            "[OK] Copied school bus -> vehicles\n",
            "[OK] Copied snowmobile -> vehicles\n",
            "[OK] Copied snowplow -> vehicles\n",
            "[OK] Copied speedboat -> vehicles\n",
            "[OK] Copied sports car -> vehicles\n",
            "[OK] Copied submarine -> vehicles\n",
            "[OK] Copied train -> vehicles\n",
            "[OK] Copied trimaran -> vehicles\n",
            "[OK] Copied trolleybus -> vehicles\n",
            "[OK] Copied unicycle -> vehicles\n",
            "[OK] Copied vessel -> vehicles\n",
            "[OK] Copied yawl -> vehicles\n",
            "[OK] Copied apple -> food_and_drink\n",
            "[OK] Copied artichoke -> food_and_drink\n",
            "[OK] Copied bagel -> food_and_drink\n",
            "[OK] Copied banana -> food_and_drink\n",
            "[OK] Copied bell pepper -> food_and_drink\n",
            "[OK] Copied burrito -> food_and_drink\n",
            "[OK] Copied cheeseburger -> food_and_drink\n",
            "[OK] Copied cocktail shaker -> food_and_drink\n",
            "[OK] Copied coffee mug -> food_and_drink\n",
            "[OK] Copied cream -> food_and_drink\n",
            "[OK] Copied cucumber -> food_and_drink\n",
            "[OK] Copied cup -> food_and_drink\n",
            "[OK] Copied fig -> food_and_drink\n",
            "[OK] Copied Granny Smith -> food_and_drink\n",
            "[OK] Copied guacamole -> food_and_drink\n",
            "[OK] Copied hamburger -> food_and_drink\n",
            "[OK] Copied head cabbage -> food_and_drink\n",
            "[OK] Copied hotdog -> food_and_drink\n",
            "[OK] Copied ice lolly -> food_and_drink\n",
            "[OK] Copied lemon -> food_and_drink\n",
            "[OK] Copied milk can -> food_and_drink\n",
            "[OK] Copied mixing bowl -> food_and_drink\n",
            "[OK] Copied mushroom -> food_and_drink\n",
            "[OK] Copied orange -> food_and_drink\n",
            "[OK] Copied pineapple -> food_and_drink\n",
            "[OK] Copied pitcher -> food_and_drink\n",
            "[OK] Copied pizza -> food_and_drink\n",
            "[OK] Copied pomegranate -> food_and_drink\n",
            "[OK] Copied pot -> food_and_drink\n",
            "[OK] Copied pretzel -> food_and_drink\n",
            "[OK] Copied soup bowl -> food_and_drink\n",
            "[OK] Copied strawberry -> food_and_drink\n",
            "[OK] Copied water bottle -> food_and_drink\n",
            "[OK] Copied wine bottle -> food_and_drink\n",
            "[OK] Copied baby bed -> household_and_furniture\n",
            "[OK] Copied barber chair -> household_and_furniture\n",
            "[OK] Copied bassinet -> household_and_furniture\n",
            "[OK] Copied beaker -> household_and_furniture\n",
            "[OK] Copied bench -> household_and_furniture\n",
            "[OK] Copied bedroom -> household_and_furniture\n",
            "[OK] Copied billiard room -> household_and_furniture\n",
            "[OK] Copied bookcase -> household_and_furniture\n",
            "[OK] Copied bowl -> household_and_furniture\n",
            "[OK] Copied can opener -> household_and_furniture\n",
            "[OK] Copied chair -> household_and_furniture\n",
            "[OK] Copied chime -> household_and_furniture\n",
            "[OK] Copied classroom -> household_and_furniture\n",
            "[OK] Copied coffee maker -> household_and_furniture\n",
            "[OK] Copied corkscrew -> household_and_furniture\n",
            "[OK] Copied cradle -> household_and_furniture\n",
            "[OK] Copied crib -> household_and_furniture\n",
            "[OK] Copied desk -> household_and_furniture\n",
            "[OK] Copied dining table -> household_and_furniture\n",
            "[OK] Copied dishwasher -> household_and_furniture\n",
            "[OK] Copied electric fan -> household_and_furniture\n",
            "[OK] Copied espresso maker -> household_and_furniture\n",
            "[OK] Copied file -> household_and_furniture\n",
            "[OK] Copied folding chair -> household_and_furniture\n",
            "[OK] Copied frying pan -> household_and_furniture\n",
            "[OK] Copied garage -> household_and_furniture\n",
            "[OK] Copied hand blower -> household_and_furniture\n",
            "[OK] Copied home theater -> household_and_furniture\n",
            "[OK] Copied kitchen -> household_and_furniture\n",
            "[OK] Copied ladle -> household_and_furniture\n",
            "[OK] Copied lamp -> household_and_furniture\n",
            "[OK] Copied microwave -> household_and_furniture\n",
            "[OK] Copied mug -> household_and_furniture\n",
            "[OK] Copied office -> household_and_furniture\n",
            "[OK] Copied park bench -> household_and_furniture\n",
            "[OK] Copied plate rack -> household_and_furniture\n",
            "[OK] Copied pool table -> household_and_furniture\n",
            "[OK] Copied recreation room -> household_and_furniture\n",
            "[OK] Copied refrigerator -> household_and_furniture\n",
            "[OK] Copied restaurant -> household_and_furniture\n",
            "[OK] Copied rocking chair -> household_and_furniture\n",
            "[OK] Copied saltshaker -> household_and_furniture\n",
            "[OK] Copied soap dispenser -> household_and_furniture\n",
            "[OK] Copied sofa -> household_and_furniture\n",
            "[OK] Copied spatula -> household_and_furniture\n",
            "[OK] Copied stove -> household_and_furniture\n",
            "[OK] Copied strainer -> household_and_furniture\n",
            "[OK] Copied studio couch -> household_and_furniture\n",
            "[OK] Copied table -> household_and_furniture\n",
            "[OK] Copied table lamp -> household_and_furniture\n",
            "[OK] Copied throne -> household_and_furniture\n",
            "[OK] Copied toaster -> household_and_furniture\n",
            "[OK] Copied vacuum -> household_and_furniture\n",
            "[OK] Copied waffle iron -> household_and_furniture\n",
            "[OK] Copied washer -> household_and_furniture\n",
            "[OK] Copied accordion -> tools_instruments_electronics\n",
            "[OK] Copied acoustic guitar -> tools_instruments_electronics\n",
            "[OK] Copied ax -> tools_instruments_electronics\n",
            "[OK] Copied balance beam -> tools_instruments_electronics\n",
            "[OK] Copied Band Aid -> tools_instruments_electronics\n",
            "[OK] Copied banjo -> tools_instruments_electronics\n",
            "[OK] Copied baseball -> tools_instruments_electronics\n",
            "[OK] Copied basketball -> tools_instruments_electronics\n",
            "[OK] Copied binder -> tools_instruments_electronics\n",
            "[OK] Copied bow -> tools_instruments_electronics\n",
            "[OK] Copied cassette player -> tools_instruments_electronics\n",
            "[OK] Copied cello -> tools_instruments_electronics\n",
            "[OK] Copied chain saw -> tools_instruments_electronics\n",
            "[OK] Copied computer keyboard -> tools_instruments_electronics\n",
            "[OK] Copied cornet -> tools_instruments_electronics\n",
            "[OK] Copied crutch -> tools_instruments_electronics\n",
            "[OK] Copied digital clock -> tools_instruments_electronics\n",
            "[OK] Copied display -> tools_instruments_electronics\n",
            "[OK] Copied drum -> tools_instruments_electronics\n",
            "[OK] Copied dumbbell -> tools_instruments_electronics\n",
            "[OK] Copied electric guitar -> tools_instruments_electronics\n",
            "[OK] Copied flute -> tools_instruments_electronics\n",
            "[OK] Copied football helmet -> tools_instruments_electronics\n",
            "[OK] Copied French horn -> tools_instruments_electronics\n",
            "[OK] Copied golf ball -> tools_instruments_electronics\n",
            "[OK] Copied grand piano -> tools_instruments_electronics\n",
            "[OK] Copied guitar -> tools_instruments_electronics\n",
            "[OK] Copied hammer -> tools_instruments_electronics\n",
            "[OK] Copied harmonica -> tools_instruments_electronics\n",
            "[OK] Copied harp -> tools_instruments_electronics\n",
            "[OK] Copied hatchet -> tools_instruments_electronics\n",
            "[OK] Copied horizontal bar -> tools_instruments_electronics\n",
            "[OK] Copied iPod -> tools_instruments_electronics\n",
            "[OK] Copied laptop -> tools_instruments_electronics\n",
            "[OK] Copied maraca -> tools_instruments_electronics\n",
            "[OK] Copied microphone -> tools_instruments_electronics\n",
            "[OK] Copied monitor -> tools_instruments_electronics\n",
            "[OK] Copied nail -> tools_instruments_electronics\n",
            "[OK] Copied neck brace -> tools_instruments_electronics\n",
            "[OK] Copied notebook -> tools_instruments_electronics\n",
            "[OK] Copied oboe -> tools_instruments_electronics\n",
            "[OK] Copied pencil box -> tools_instruments_electronics\n",
            "[OK] Copied pencil sharpener -> tools_instruments_electronics\n",
            "[OK] Copied piano -> tools_instruments_electronics\n",
            "[OK] Copied ping-pong ball -> tools_instruments_electronics\n",
            "[OK] Copied polo -> tools_instruments_electronics\n",
            "[OK] Copied power drill -> tools_instruments_electronics\n",
            "[OK] Copied printer -> tools_instruments_electronics\n",
            "[OK] Copied puck -> tools_instruments_electronics\n",
            "[OK] Copied punching bag -> tools_instruments_electronics\n",
            "[OK] Copied racket -> tools_instruments_electronics\n",
            "[OK] Copied remote control -> tools_instruments_electronics\n",
            "[OK] Copied rubber eraser -> tools_instruments_electronics\n",
            "[OK] Copied rugby ball -> tools_instruments_electronics\n",
            "[OK] Copied rule -> tools_instruments_electronics\n",
            "[OK] Copied sax -> tools_instruments_electronics\n",
            "[OK] Copied screen -> tools_instruments_electronics\n",
            "[OK] Copied screwdriver -> tools_instruments_electronics\n",
            "[OK] Copied ski -> tools_instruments_electronics\n",
            "[OK] Copied soccer ball -> tools_instruments_electronics\n",
            "[OK] Copied steel drum -> tools_instruments_electronics\n",
            "[OK] Copied stethoscope -> tools_instruments_electronics\n",
            "[OK] Copied stretcher -> tools_instruments_electronics\n",
            "[OK] Copied syringe -> tools_instruments_electronics\n",
            "[OK] Copied tape player -> tools_instruments_electronics\n",
            "[OK] Copied tennis ball -> tools_instruments_electronics\n",
            "[OK] Copied trombone -> tools_instruments_electronics\n",
            "[OK] Copied upright -> tools_instruments_electronics\n",
            "[OK] Copied violin -> tools_instruments_electronics\n",
            "[OK] Copied volleyball -> tools_instruments_electronics\n",
            "[OK] Copied backpack -> clothing_and_personal_items\n",
            "[OK] Copied bathing cap -> clothing_and_personal_items\n",
            "[OK] Copied bow tie -> clothing_and_personal_items\n",
            "[OK] Copied brassiere -> clothing_and_personal_items\n",
            "[OK] Copied cowboy hat -> clothing_and_personal_items\n",
            "[OK] Copied crash helmet -> clothing_and_personal_items\n",
            "[OK] Copied diaper -> clothing_and_personal_items\n",
            "[OK] Copied face powder -> clothing_and_personal_items\n",
            "[OK] Copied hair spray -> clothing_and_personal_items\n",
            "[OK] Copied helmet -> clothing_and_personal_items\n",
            "[OK] Copied lipstick -> clothing_and_personal_items\n",
            "[OK] Copied lotion -> clothing_and_personal_items\n",
            "[OK] Copied maillot -> clothing_and_personal_items\n",
            "[OK] Copied miniskirt -> clothing_and_personal_items\n",
            "[OK] Copied perfume -> clothing_and_personal_items\n",
            "[OK] Copied plastic bag -> clothing_and_personal_items\n",
            "[OK] Copied purse -> clothing_and_personal_items\n",
            "[OK] Copied shower cap -> clothing_and_personal_items\n",
            "[OK] Copied sombrero -> clothing_and_personal_items\n",
            "[OK] Copied sunglasses -> clothing_and_personal_items\n",
            "[OK] Copied sunscreen -> clothing_and_personal_items\n",
            "[OK] Copied swimming trunks -> clothing_and_personal_items\n",
            "[OK] Copied Windsor tie -> clothing_and_personal_items\n",
            "Dataset reorganization complete!\n"
          ]
        }
      ],
      "source": [
        "reorganize_dataset(mapping_file='/content/drive/MyDrive/NeuroVision/meta-learner-class-mapping-v2.json',\n",
        "                   src_root='/content/drive/MyDrive/NeuroVision/Segregated_Dataset',\n",
        "                   dst_root='/content/meta_learner_dataset',\n",
        "                   move=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0a4f18-0cbf-451e-be8c-1aa156eff8a7",
      "metadata": {
        "id": "ff0a4f18-0cbf-451e-be8c-1aa156eff8a7"
      },
      "source": [
        "## Dataset Processing for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f7ba77-0e7b-46c4-9823-85ed74ec2bda",
      "metadata": {
        "id": "a6f7ba77-0e7b-46c4-9823-85ed74ec2bda"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HnvdOhuonfeY",
      "metadata": {
        "id": "HnvdOhuonfeY"
      },
      "outputs": [],
      "source": [
        "def simple_collate_fn(batch):\n",
        "  sequences, _ = zip(*batch)\n",
        "  padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "\n",
        "  return padded_seqs, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MCA5zBFxaqh_",
      "metadata": {
        "id": "MCA5zBFxaqh_"
      },
      "outputs": [],
      "source": [
        "def get_dataset_stats(dataset):\n",
        "  from tqdm import tqdm\n",
        "\n",
        "  num_channels = dataset.num_channels\n",
        "  sum_ = torch.zeros(num_channels)\n",
        "  sum_sq = torch.zeros(num_channels)\n",
        "  count = torch.zeros(num_channels)\n",
        "\n",
        "  # loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "  # all_channels_data = [[] for _ in range(5)]\n",
        "\n",
        "  print(\"Calculating dataset stats...\")\n",
        "\n",
        "  for spectrogram, _ in tqdm(dataset):\n",
        "    sum_ += torch.sum(spectrogram, dim=[1,2])\n",
        "    sum_sq += torch.sum(spectrogram **2, dim=[1,2])\n",
        "\n",
        "    count += spectrogram.shape[1] * spectrogram.shape[2]\n",
        "\n",
        "  mean = sum_/count\n",
        "  std = torch.sqrt((sum_sq/count) - (mean**2))\n",
        "  #   for i in range(5):\n",
        "  #     all_channels_data[i].append(data[:, :, i].flatten())\n",
        "\n",
        "  #   channel_means = [torch.cat(ch_data).mean() for ch_data in all_channels_data]\n",
        "  #   channel_stds = [torch.cat(ch_data).std() for ch_data in all_channels_data]\n",
        "\n",
        "  # return torch.tensor(channel_means), torch.tensor(channel_stds)\n",
        "  return mean, std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027fe9b8-8ee9-4f76-b0b0-20f528f20261",
      "metadata": {
        "id": "027fe9b8-8ee9-4f76-b0b0-20f528f20261"
      },
      "outputs": [],
      "source": [
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, root_dir, samples, num_channels=5, mean=None, std=None, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "        self.num_channels = num_channels\n",
        "        self.class_to_idx = list(set([label for _, label in self.samples]))\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path, label = self.samples[idx]\n",
        "\n",
        "        df = pd.read_csv(file_path, header=None, index_col=0)\n",
        "        eeg_data = torch.tensor(df.values, dtype=torch.float32)\n",
        "\n",
        "        if eeg_data.shape[1] != self.num_channels:\n",
        "          if eeg_data.shape[0] == self.num_channels:\n",
        "            eeg_data = eeg_data.T\n",
        "\n",
        "          else:\n",
        "            raise ValueError(f\"File {file_path} has invalid shape: {eeg_data.shape}\")\n",
        "\n",
        "        if self.mean is not None and self.std is not None:\n",
        "          eeg_data = (eeg_data - self.mean) / self.std\n",
        "\n",
        "        if self.transform:\n",
        "            eeg_data = self.transform(eeg_data)\n",
        "\n",
        "        return eeg_data, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4342db-dfec-4a91-bd75-848017a33149",
      "metadata": {
        "id": "cd4342db-dfec-4a91-bd75-848017a33149"
      },
      "outputs": [],
      "source": [
        "def make_datasets(root_dir, val_ratio=0.3, random_state=42):\n",
        "    class_names = os.listdir(root_dir)\n",
        "    class_to_idx = {cls:idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "    all_samples = []\n",
        "    all_labels = []\n",
        "\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(root_dir, cls)\n",
        "\n",
        "        for fname in os.listdir(cls_dir):\n",
        "            if fname.endswith('.csv'):\n",
        "                path = os.path.join(cls_dir, fname)\n",
        "                all_samples.append((path, class_to_idx[cls]))\n",
        "                all_labels.append(class_to_idx[cls])\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(len(all_samples))),\n",
        "        test_size=val_ratio,\n",
        "        random_state=random_state,\n",
        "        stratify=all_labels\n",
        "    )\n",
        "\n",
        "    train_samples = [all_samples[i] for i in train_idx]\n",
        "    val_samples = [all_samples[i] for i in val_idx]\n",
        "\n",
        "    # train_dataset = EEGDataset(root_dir, train_samples)\n",
        "    # val_dataset = EEGDataset(root_dir, val_samples)\n",
        "\n",
        "    return train_samples, val_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ystIJ3GLNI7b",
      "metadata": {
        "id": "ystIJ3GLNI7b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qd6ZuxeuNHYq",
      "metadata": {
        "id": "qd6ZuxeuNHYq"
      },
      "outputs": [],
      "source": [
        "class EEGSpectrogramDataset(Dataset):\n",
        "  def __init__(self, root_dir, samples, is_train=False, fs=128, nperseg=64, noverlap=32, mean=None, std=None, transforms=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.samples = samples\n",
        "    self.is_train = is_train\n",
        "    self.class_to_idx = list(set([label for _, label in self.samples]))\n",
        "    self.transform_params = {'fs': fs, 'nperseg': nperseg, 'noverlap': noverlap}\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.num_channels = 5\n",
        "    self.target_freq_bins = 33\n",
        "    self.target_time_bins = 10\n",
        "    self.transforms=transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_path, label = self.samples[idx]\n",
        "\n",
        "    eeg_data_1d = pd.read_csv(file_path, header=None, index_col=0).values\n",
        "\n",
        "    if eeg_data_1d.shape[1] != self.num_channels:\n",
        "      if eeg_data_1d.shape[0] == self.num_channels:\n",
        "        eeg_data_1d = eeg_data_1d.T\n",
        "      else:\n",
        "        raise ValueError(f\"File {file_path} has an invalid shape: {eeg_data_1d.shape}\")\n",
        "\n",
        "\n",
        "    channel_spectrograms = []\n",
        "\n",
        "    for i in range(self.num_channels):\n",
        "      channel_signal = eeg_data_1d[:, i]\n",
        "\n",
        "      f, t, Sxx = signal.spectrogram(channel_signal, **self.transform_params)\n",
        "\n",
        "      Sxx = np.log1p(Sxx)\n",
        "\n",
        "      channel_spectrograms.append(Sxx)\n",
        "\n",
        "    spectrogram = torch.tensor(np.array(channel_spectrograms), dtype=torch.float32)\n",
        "\n",
        "    # _, current_freqs, current_times = spectrogram.shape\n",
        "    # resized_spectrogram = torch.zeros((self.num_channels, self.target_freq_bins, self.target_time_bins))\n",
        "\n",
        "    # copy_freqs = min(current_freqs, self.target_freq_bins)\n",
        "    # copy_times = min(current_times, self.target_time_bins)\n",
        "\n",
        "    # resized_spectrogram[:, :copy_freqs, :copy_times] = spectrogram[:, :copy_freqs, :copy_times]\n",
        "\n",
        "    # spectrogram = resized_spectrogram\n",
        "\n",
        "    if self.mean is not None and self.std is not None:\n",
        "      mean = self.mean.view(self.num_channels, 1, 1)\n",
        "      std = self.std.view(self.num_channels, 1, 1)\n",
        "\n",
        "      spectrogram = (spectrogram - mean) / (std + 1e-9)\n",
        "\n",
        "    if self.is_train and self.transforms:\n",
        "      transformed_channels = []\n",
        "\n",
        "      for i in range(self.num_channels):\n",
        "        channel_spectrogram = spectrogram[i, :, :]\n",
        "        transformed_channel = self.transforms(channel_spectrogram)\n",
        "        transformed_channels.append(transformed_channel)\n",
        "\n",
        "      spectrogram = torch.cat(transformed_channels, dim=0)\n",
        "\n",
        "    elif not self.is_train and self.transforms:\n",
        "      transformed_channels = []\n",
        "\n",
        "      for i in range(self.num_channels):\n",
        "        channel_spectrogram = spectrogram[i, :, :]\n",
        "        transformed_channel = self.transforms(channel_spectrogram)\n",
        "        transformed_channels.append(transformed_channel)\n",
        "\n",
        "      spectrogram = torch.cat(transformed_channels, dim=0)\n",
        "\n",
        "\n",
        "    return spectrogram, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0696ba6-64c0-427e-895e-0dead7a43cea",
      "metadata": {
        "id": "d0696ba6-64c0-427e-895e-0dead7a43cea"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    MIN_LENGTH = 4\n",
        "    valid_batch = [item for item in batch if item[0].shape[0] >= MIN_LENGTH]\n",
        "\n",
        "    if not valid_batch:\n",
        "      return None, None, None, None\n",
        "\n",
        "    sequences, labels = zip(*valid_batch)\n",
        "    lengths = torch.tensor([seq.shape[0] for seq in sequences], dtype=torch.long)\n",
        "\n",
        "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    mask = torch.arange(padded_seqs.shape[1])[None, :] < lengths[:, None]\n",
        "\n",
        "    return padded_seqs, torch.tensor(labels, dtype=torch.long), lengths, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d72aba-d8c1-46e8-a055-52cf6ba2cae1",
      "metadata": {
        "id": "44d72aba-d8c1-46e8-a055-52cf6ba2cae1"
      },
      "outputs": [],
      "source": [
        "def create_sampler(dataset):\n",
        "  from collections import Counter\n",
        "  from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "  all_labels = [label for _, label in dataset.samples]\n",
        "\n",
        "  class_counts = Counter(all_labels)\n",
        "\n",
        "  num_classes = len(dataset.class_to_idx)\n",
        "  class_weights = torch.zeros(num_classes)\n",
        "\n",
        "  for class_idx, count in class_counts.items():\n",
        "    if count > 0:\n",
        "      class_weights[class_idx] = 1.0 / count\n",
        "\n",
        "  sample_weights = [class_weights[label] for label in all_labels]\n",
        "\n",
        "  sampler = WeightedRandomSampler(\n",
        "      weights=sample_weights,\n",
        "      num_samples=len(dataset.samples),\n",
        "      replacement=True\n",
        "  )\n",
        "\n",
        "  return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8180c7-8721-41be-9a88-1dfd1e25a3ca",
      "metadata": {
        "id": "cc8180c7-8721-41be-9a88-1dfd1e25a3ca"
      },
      "outputs": [],
      "source": [
        "# root_dir = '../content/drive/MyDrive/NeuroVision/data/Classes_Regrouped_Dataset'\n",
        "root_dir = '/content/meta_learner_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOd0_MiidE94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOd0_MiidE94",
        "outputId": "fdf6c2d6-5f46-4da5-960a-80b1a3b71e5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir(root_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3aa364a-238c-4491-b180-38f91b9e998f",
      "metadata": {
        "id": "d3aa364a-238c-4491-b180-38f91b9e998f"
      },
      "outputs": [],
      "source": [
        "train_samples, val_samples = make_datasets(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5XCiiIVMkLiy",
      "metadata": {
        "id": "5XCiiIVMkLiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = EEGSpectrogramDataset(root_dir, train_samples, 5)\n",
        "val_dataset = EEGSpectrogramDataset(root_dir, val_samples, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_ZvO9_x5KTD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_ZvO9_x5KTD",
        "outputId": "a8fab933-b9d7-44d0-ff93-d3b2d8973d75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9424, 4039)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oV9ii4uDc1oi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV9ii4uDc1oi",
        "outputId": "48d1f92e-f5d6-4a9f-c788-2220eb42ad19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating dataset stats...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9424/9424 [01:32<00:00, 101.80it/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "train_means, train_stds = get_dataset_stats(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QKkZtYDZZ_mh",
      "metadata": {
        "id": "QKkZtYDZZ_mh"
      },
      "outputs": [],
      "source": [
        "train_sampler = create_sampler(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XqxvU5HPwMmp",
      "metadata": {
        "id": "XqxvU5HPwMmp"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.v2 import GaussianNoise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ufMYyijpZuxy",
      "metadata": {
        "id": "ufMYyijpZuxy"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((128, 128), antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    GaussianNoise(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.1, 10.0), value=0)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((128, 128), antialias=True),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nMfb5iffZUoN",
      "metadata": {
        "id": "nMfb5iffZUoN"
      },
      "outputs": [],
      "source": [
        "train_dataset_scaled = EEGSpectrogramDataset(root_dir, train_samples, is_train=True, mean=train_means,\n",
        "                                             std=train_stds, transforms=train_transforms)\n",
        "\n",
        "val_dataset_scaled = EEGSpectrogramDataset(root_dir, val_samples, is_train=False, mean=train_means,\n",
        "                                             std=train_stds, transforms=val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bjw3UR-DaF0A",
      "metadata": {
        "id": "bjw3UR-DaF0A"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset_scaled, batch_size=128, shuffle=False, sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=3)\n",
        "\n",
        "val_loader = DataLoader(val_dataset_scaled, batch_size=128, shuffle=False, num_workers=2, pin_memory=True,\n",
        "                        persistent_workers=True, prefetch_factor=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1NTPqRswu6E_",
      "metadata": {
        "id": "1NTPqRswu6E_"
      },
      "source": [
        "## Past Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6a7229-ac9e-4bed-bd54-97f1e0cbc70c",
      "metadata": {
        "id": "1e6a7229-ac9e-4bed-bd54-97f1e0cbc70c"
      },
      "outputs": [],
      "source": [
        "# class EegLstm(nn.Module):\n",
        "#     def __init__(self, input_dims=5, hidden_dims=128, num_layers=3, dropout=0.3 , num_classes=len(os.listdir(root_dir))):\n",
        "#         super(EegLstm, self).__init__()\n",
        "\n",
        "#         self.lstm = nn.LSTM(\n",
        "#             input_size=input_dims,\n",
        "#             hidden_size=hidden_dims,\n",
        "#             num_layers=num_layers,\n",
        "#             batch_first=True,\n",
        "#             dropout=dropout if num_layers >= 2 else 0,\n",
        "#             bidirectional=True\n",
        "#         )\n",
        "\n",
        "#         self.conv_stack = nn.Sequential(\n",
        "#             nn.Conv2d(hidden_dims*2)\n",
        "#         )\n",
        "\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(hidden_dims*2, hidden_dims),\n",
        "#             nn.BatchNorm1d(hidden_dims),\n",
        "#             nn.SELU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(hidden_dims, hidden_dims),\n",
        "#             nn.BatchNorm1d(hidden_dims),\n",
        "#             nn.SELU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(hidden_dims, hidden_dims//2),\n",
        "#             nn.BatchNorm1d(hidden_dims//2),\n",
        "#             nn.SELU(),\n",
        "#             nn.Linear(hidden_dims, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, lengths=None):\n",
        "#         if lengths is not None:\n",
        "#             packed = nn.utils.rnn.pack_padded_sequence(\n",
        "#                 x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "#             )\n",
        "\n",
        "#             packed_out, (h_n, c_n) = self.lstm(packed)\n",
        "\n",
        "#         else:\n",
        "#             out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "#         last_hidden_backward, last_hidden_forward = h_n[-1], h_n[-2]\n",
        "#         logits=self.fc(torch.cat((last_hidden_backward, last_hidden_forward), dim=1))\n",
        "\n",
        "#         return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rQRrJAdUPJqj",
      "metadata": {
        "id": "rQRrJAdUPJqj"
      },
      "outputs": [],
      "source": [
        "# class HybridExtractor(nn.Module):\n",
        "#     def __init__(self, input_dims=5, cnn_out_channels=64, kernel_size=50, lstm_hidden_dims=128, num_layers=3, dropout=0.3 , num_classes=len(os.listdir(root_dir))):\n",
        "#         super(HybridExtractor, self).__init__()\n",
        "\n",
        "#         # CNN block\n",
        "#         self.cnn_stack = nn.Sequential(\n",
        "#             nn.Conv1d(input_dims, 32, kernel_size=kernel_size, stride=1, padding='same'),\n",
        "#             nn.BatchNorm1d(32),\n",
        "#             nn.ELU(),\n",
        "#             nn.AvgPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "#             nn.Conv1d(32, cnn_out_channels, kernel_size=kernel_size//2, stride=1, padding='same'),\n",
        "#             nn.BatchNorm1d(cnn_out_channels),\n",
        "#             nn.ELU(),\n",
        "#             nn.AvgPool1d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         self.lstm = nn.LSTM(\n",
        "#             input_size=cnn_out_channels,\n",
        "#             hidden_size=lstm_hidden_dims,\n",
        "#             num_layers=num_layers,\n",
        "#             batch_first=True,\n",
        "#             dropout=dropout if num_layers > 1 else 0,\n",
        "#             bidirectional=True\n",
        "#         )\n",
        "\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(lstm_hidden_dims*2, lstm_hidden_dims),\n",
        "#             nn.BatchNorm1d(lstm_hidden_dims),\n",
        "#             nn.SELU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(lstm_hidden_dims, lstm_hidden_dims),\n",
        "#             nn.BatchNorm1d(lstm_hidden_dims),\n",
        "#             nn.SELU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(lstm_hidden_dims, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, lengths=None):\n",
        "#       x = x.permute(0, 2, 1)\n",
        "\n",
        "#       cnn_out = self.cnn_stack(x)\n",
        "\n",
        "#       lstm_input = cnn_out.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "#       if lengths is not None:\n",
        "#         new_lengths = (lengths//4).long()\n",
        "\n",
        "#         packed = nn.utils.rnn.pack_padded_sequence(\n",
        "#                 lstm_input, new_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "#             )\n",
        "\n",
        "#         packed_out, (h_n, c_n) = self.lstm(packed)\n",
        "\n",
        "#       else:\n",
        "#           out, (h_n, c_n) = self.lstm(lstm_input)\n",
        "\n",
        "#       last_hidden_backward, last_hidden_forward = h_n[-1, :, :], h_n[-2, :, :]\n",
        "#       logits=self.fc(torch.cat((last_hidden_backward, last_hidden_forward), dim=1))\n",
        "\n",
        "#       return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UJccjfqsvB18",
      "metadata": {
        "id": "UJccjfqsvB18"
      },
      "source": [
        "## Newer Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NjL80aWJu16j",
      "metadata": {
        "id": "NjL80aWJu16j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "di-ptzIovFh_",
      "metadata": {
        "id": "di-ptzIovFh_"
      },
      "outputs": [],
      "source": [
        "class DepthwiseSeparableConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "    super(DepthwiseSeparableConv, self).__init__()\n",
        "    self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, padding=padding, groups=in_channels, bias=False)\n",
        "    self.pointwise = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n",
        "    self.bn = nn.BatchNorm1d(out_channels)\n",
        "    self.elu = nn.ELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.depthwise(x)\n",
        "    x = self.pointwise(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return self.elu(x)\n",
        "\n",
        "class MesoHybridNet(nn.Module):\n",
        "  def __init__(self, input_dims=5, num_classes=118, gru_hidden_dims=128, gru_num_layers=2, dropout=0.4):\n",
        "    super(MesoHybridNet, self).__init__()\n",
        "\n",
        "    channels=24\n",
        "    self.branch_fine = DepthwiseSeparableConv(input_dims, channels, kernel_size=10, padding='same')\n",
        "    self.branch_medium = DepthwiseSeparableConv(input_dims, channels, kernel_size=50, padding='same')\n",
        "    self.branch_coarse = DepthwiseSeparableConv(input_dims, channels, kernel_size=150, padding='same')\n",
        "\n",
        "    combined_channels = 3 * channels\n",
        "    self.pool = nn.AvgPool1d(4)\n",
        "\n",
        "    self.gru = nn.GRU(\n",
        "        input_size=combined_channels,\n",
        "        hidden_size=gru_hidden_dims,\n",
        "        num_layers=gru_num_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True,\n",
        "        dropout=dropout if gru_num_layers > 1 else 0\n",
        "    )\n",
        "\n",
        "    gru_output_dim = gru_hidden_dims * 2\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(gru_output_dim, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, lengths=None, mask=None):\n",
        "    x = x.permute(0,2,1)\n",
        "\n",
        "    out_fine = self.branch_fine(x)\n",
        "    out_medium = self.branch_medium(x)\n",
        "    out_coarse = self.branch_coarse(x)\n",
        "\n",
        "    combined_features = torch.cat([out_fine, out_medium, out_coarse], dim=1)\n",
        "    pooled_features = self.pool(combined_features)\n",
        "\n",
        "    gru_input = pooled_features.permute(0,2,1)\n",
        "\n",
        "    if lengths is not None:\n",
        "      new_lengths = (lengths//4).clamp(min=1).long()\n",
        "      packed = nn.utils.rnn.pack_padded_sequence(\n",
        "          gru_input, new_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "      )\n",
        "      _, h_n = self.gru(packed)\n",
        "\n",
        "    else:\n",
        "      _, h_n = self.gru(gru_input)\n",
        "\n",
        "    features = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
        "\n",
        "    # if mask is not None:\n",
        "    #   mask = mask[:, ::4]\n",
        "    #   mask = mask.unsqueeze(1)\n",
        "\n",
        "    #   combined_features = combined_features * mask\n",
        "    #   summed_features = torch.sum(combined_features, dim=2)\n",
        "\n",
        "    #   true_lengths = torch.sum(mask, dim=2) + 1e-9\n",
        "    #   pooled_features = summed_features / true_lengths\n",
        "\n",
        "    # else:\n",
        "    #   pooled_features = torch.mean(combined_features, dim=2)\n",
        "\n",
        "    logits = self.fc(features)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6tX04uvJJqIH",
      "metadata": {
        "id": "6tX04uvJJqIH"
      },
      "outputs": [],
      "source": [
        "class ConvBranch(nn.Module):\n",
        "  def __init__(self, input_dims, output_channels, kernel_size, stride, padding, pooling_type, pool_size=4):\n",
        "    super(ConvBranch, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(input_dims, output_channels, kernel_size, stride, padding, bias=False)\n",
        "    # self.bn = nn.BatchNorm2d(output_channels)\n",
        "\n",
        "    # if pooling_type.lower() == 'max':\n",
        "    #   self.pool = nn.MaxPool2d(pool_size)\n",
        "    # else:\n",
        "    #   self.pool = nn.AvgPool2d(pool_size)\n",
        "\n",
        "    # self.elu = nn.ELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class EEGMesoNet(nn.Module):\n",
        "  def __init__(self, input_dims=5, num_classes=10, dropout=0.5):\n",
        "    super(EEGMesoNet, self).__init__()\n",
        "\n",
        "    channels = 8\n",
        "    self.branch_fine = ConvBranch(input_dims, channels*3, kernel_size=4, stride=1, padding=2, pooling_type='max')\n",
        "    self.branch_medium = ConvBranch(input_dims, channels*2, kernel_size=16, stride=4, padding=4, pooling_type='avg')\n",
        "    self.branch_coarse = ConvBranch(input_dims, channels, kernel_size=64, stride=16, padding=8, pooling_type='avg')\n",
        "\n",
        "    combined_channels = (channels*2) + channels + (channels*3)\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(combined_channels, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.BatchNorm1d(64),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(64, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = x.permute(0, 2, 1)\n",
        "\n",
        "    out_fine = self.branch_fine(x)\n",
        "    out_medium = self.branch_medium(x)\n",
        "    out_coarse = self.branch_coarse(x)\n",
        "\n",
        "    out_fine_pooled_flattened = torch.flatten(nn.functional.adaptive_max_pool2d(out_fine, (1,1)), 1)\n",
        "    out_medium_pooled_flattened = torch.flatten(nn.functional.adaptive_avg_pool2d(out_medium, (1,1)), 1)\n",
        "    out_coarse_pooled_flattened = torch.flatten(nn.functional.adaptive_avg_pool2d(out_coarse, (1,1)), 1)\n",
        "\n",
        "    combined_features = torch.cat([out_fine_pooled_flattened, out_medium_pooled_flattened, out_coarse_pooled_flattened], dim=1)\n",
        "\n",
        "    # if mask is not None:\n",
        "    #   mask = mask[:, ::4]\n",
        "    #   mask = mask.unsqueeze(1)\n",
        "\n",
        "    #   combined_features = combined_features * mask\n",
        "    #   summed_features = torch.sum(combined_features, dim=2)\n",
        "    #   true_lengths = torch.sum(mask, dim=2) + 1e-9\n",
        "    #   pooled_features = summed_features / true_lengths\n",
        "\n",
        "    # else:\n",
        "      # pooled_features = torch.mean(combined_features, dim=2)\n",
        "\n",
        "    logits = self.fc(combined_features)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GKhnfwXcd8V4",
      "metadata": {
        "id": "GKhnfwXcd8V4"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zNsDXuvBSzsx",
      "metadata": {
        "id": "zNsDXuvBSzsx"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "  if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    if m.bias is not None:\n",
        "      nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8cb365-9915-4f3a-afd4-6946979a0590",
      "metadata": {
        "id": "bf8cb365-9915-4f3a-afd4-6946979a0590"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fb3cb9-1dec-4a6a-9c4e-7a445954daaf",
      "metadata": {
        "id": "89fb3cb9-1dec-4a6a-9c4e-7a445954daaf"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping(object):\n",
        "    def __init__(self, model, save_path='../content/drive/MyDrive/eeg_classifier.pt', patience=4, tol=1e-3):\n",
        "        self.model = model\n",
        "        self.save_path = save_path\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.tol = tol\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, batch_val_loss):\n",
        "        if batch_val_loss < self.best_val_loss - self.tol:\n",
        "            torch.save(self.model.state_dict(), self.save_path)\n",
        "            self.best_val_loss = batch_val_loss\n",
        "            self.counter = 0\n",
        "            print(f'Validation Loss improved -> model saved to {self.save_path}')\n",
        "\n",
        "        else:\n",
        "            if self.counter < self.patience:\n",
        "                self.counter += 1\n",
        "                print(f'No improvement in Val Loss. Counter: {self.counter}/{self.patience}')\n",
        "\n",
        "            else:\n",
        "                self.early_stop = True\n",
        "                print(f\"Early Stopping triggered!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0401a423-49b1-40f1-b535-4db7af34c7d6",
      "metadata": {
        "id": "0401a423-49b1-40f1-b535-4db7af34c7d6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, model_name, train_loader, val_loader, epochs=50, lr=1e-3, device='cpu'):\n",
        "    log_dir = f'../content/drive/MyDrive/NeuroVision/runs/{model_name}_v1'\n",
        "    save_path = f'../content/drive/MyDrive/NeuroVision/models/{model_name}_v2_best.pth'\n",
        "    os.makedirs(os.path.dirname(log_dir), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    # params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.AdamW([\n",
        "    #     {'params': resnet18.conv1.parameters(), 'lr': 1e-4},\n",
        "    #     {'params': resnet18.fc.parameters(), 'lr': lr},\n",
        "    #     {'params': resnet18.layer1.parameters(), 'lr':5e-5},\n",
        "    #     {'params': resnet18.layer2.parameters(), 'lr': 5e-5},\n",
        "    #     {'params': resnet18.layer3.parameters(), 'lr': 5e-5},\n",
        "    #     {'params': resnet18.layer4.parameters(), 'lr': 5e-5}\n",
        "    # ], weight_decay=0.01)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), weight_decay=0.05)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-4,\n",
        "    #                                           total_steps=epochs * len(train_loader),\n",
        "    #                                           pct_start=0.05)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer,\n",
        "                                                               T_0=5,\n",
        "                                                               T_mult=1,\n",
        "                                                               eta_min=5e-5)\n",
        "\n",
        "    early_stopping = EarlyStopping(model, save_path=save_path, patience=10)\n",
        "    model.to(device)\n",
        "\n",
        "    iters = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train Pass]', leave=True)\n",
        "\n",
        "        for i, (batch_x, batch_y) in enumerate(train_bar):\n",
        "            batch_x, batch_y= batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_preds = model(batch_x)\n",
        "\n",
        "            loss = criterion(y_preds, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step(epoch + i / iters)\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, preds = torch.max(y_preds, 1)\n",
        "            train_correct += (preds == batch_y).sum().item()\n",
        "            train_total += batch_y.size(0)\n",
        "\n",
        "            train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss /= train_total\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch{epoch+1}/{epochs} [Val Pass]\", leave=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_bar:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "                y_preds = model(batch_x)\n",
        "                loss = criterion(y_preds, batch_y)\n",
        "\n",
        "                val_loss += loss.item() * batch_x.size(0)\n",
        "                _, preds = torch.max(y_preds, 1)\n",
        "                # print(f\"Sample Predictions: {preds.cpu().numpy()}\")\n",
        "                val_correct += (preds == batch_y).sum().item()\n",
        "                val_total += batch_y.size(0)\n",
        "\n",
        "                val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            break\n",
        "\n",
        "\n",
        "        # logging\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\\nTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} %\\nVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f} %\")\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc98eb7-1f94-448f-9631-a983d33d4c14",
      "metadata": {
        "id": "bdc98eb7-1f94-448f-9631-a983d33d4c14"
      },
      "outputs": [],
      "source": [
        "def model_summary(model):\n",
        "    print('========================================= Model Summary ==============================================\\n')\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"{'| Parameter Name':31}|| Number of Parameters|\")\n",
        "    print(f\"{'='*55}\")\n",
        "\n",
        "    total_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f'| {name:30}|{param.numel():20} |')\n",
        "        print(f\"{'-'*55}\")\n",
        "        total_params += param.numel()\n",
        "\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SYQ-8v8RH6VB",
      "metadata": {
        "id": "SYQ-8v8RH6VB"
      },
      "source": [
        "## Spectrogram Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kUq6vn2ckKX-",
      "metadata": {
        "id": "kUq6vn2ckKX-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cV13LWJvnozS",
      "metadata": {
        "id": "cV13LWJvnozS"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.cbam = CBAM(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*planes)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "      out = self.bn2(self.conv2(out))\n",
        "\n",
        "      out = self.cbam(out)\n",
        "\n",
        "      out += self.shortcut(x)\n",
        "      out = nn.functional.relu(out)\n",
        "\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bonFtsSChk1f",
      "metadata": {
        "id": "bonFtsSChk1f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "  def __init__(self, in_planes, ratio=16):\n",
        "    super(ChannelAttention, self).__init__()\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "    self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Conv2d(in_planes//ratio, in_planes, 1, bias=False)\n",
        "                            )\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    avg_out = self.fc(self.avg_pool(x))\n",
        "    max_out = self.fc(self.max_pool(x))\n",
        "    out = avg_out + max_out\n",
        "    return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "  def __init__(self, kernel_size=7):\n",
        "    super(SpatialAttention, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "    max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "    x = torch.cat([avg_out, max_out], dim=1)\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "  def __init__(self, in_planes, ratio=16,  kernel_size=7):\n",
        "    super(CBAM, self).__init__()\n",
        "    self.ca = ChannelAttention(in_planes, ratio)\n",
        "    self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    channel_attention_map = self.ca(x)\n",
        "    x = x * channel_attention_map\n",
        "\n",
        "    spatial_attention_map = self.sa(x)\n",
        "    x = x * spatial_attention_map\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AhYKXo07QjXI",
      "metadata": {
        "id": "AhYKXo07QjXI"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT, progress=True)\n",
        "\n",
        "for param in resnet18.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LKX0ZbDKyHuJ",
      "metadata": {
        "id": "LKX0ZbDKyHuJ"
      },
      "outputs": [],
      "source": [
        "old_conv1 = resnet18.conv1\n",
        "old_fc = resnet18.fc\n",
        "\n",
        "new_conv1 = nn.Conv2d(5, old_conv1.out_channels, old_conv1.kernel_size,\n",
        "                      old_conv1.stride, old_conv1.padding, bias=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  new_conv1.weight[:, :3, :, :] = old_conv1.weight.clone()\n",
        "  mean_weights = torch.mean(old_conv1.weight, dim=1, keepdim=True)\n",
        "  new_conv1.weight[:, 3:5, :, :] = mean_weights.repeat(1, 2, 1, 1)\n",
        "\n",
        "new_fc = nn.Sequential(nn.Linear(old_fc.in_features, 256),\n",
        "                       nn.ReLU(inplace=True),\n",
        "                       nn.BatchNorm1d(256),\n",
        "                       nn.Dropout(0.5),\n",
        "                       nn.Linear(in_features=256, out_features=118))\n",
        "new_fc.apply(weights_init)\n",
        "\n",
        "resnet18.fc = new_fc\n",
        "resnet18.conv1 = new_conv1\n",
        "\n",
        "resnet18.fc.requires_grad = True\n",
        "resnet18.conv1.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PiVLzKoMruzq",
      "metadata": {
        "id": "PiVLzKoMruzq"
      },
      "outputs": [],
      "source": [
        "import types\n",
        "from torchvision.models import resnet\n",
        "\n",
        "def new_forward(self, x):\n",
        "  identity = x\n",
        "\n",
        "  out = self.conv1(x)\n",
        "  out = self.bn1(out)\n",
        "  out = self.relu(out)\n",
        "\n",
        "  out = self.conv2(out)\n",
        "  out = self.bn2(out)\n",
        "\n",
        "  if hasattr(self, 'cbam'):\n",
        "    out = self.cbam(out)\n",
        "\n",
        "  if self.downsample is not None:\n",
        "    identity = self.downsample(x)\n",
        "\n",
        "  out += identity\n",
        "  out = self.relu(out)\n",
        "\n",
        "  return out\n",
        "\n",
        "for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
        "  layer = getattr(resnet18, layer_name)\n",
        "\n",
        "  for i in range(len(layer)):\n",
        "    block = layer[i]\n",
        "\n",
        "    if isinstance(block, resnet.BasicBlock):\n",
        "      num_channels = block.conv2.out_channels\n",
        "\n",
        "      block.cbam = CBAM(num_channels)\n",
        "      block.forward = types.MethodType(new_forward, block)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SWaHSOg5zXQJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWaHSOg5zXQJ",
        "outputId": "cc539ae1-07d2-448d-b63d-821db603af3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 118)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet18.conv1.in_channels, resnet18.fc[-1].out_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xcybqP8SSfOZ",
      "metadata": {
        "collapsed": true,
        "id": "xcybqP8SSfOZ"
      },
      "outputs": [],
      "source": [
        "model_summary(resnet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df9cb8d-ac5b-4ba4-a4ec-8b2d0877b6f0",
      "metadata": {
        "id": "9df9cb8d-ac5b-4ba4-a4ec-8b2d0877b6f0"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q6o-wJ5oJQdT",
      "metadata": {
        "id": "Q6o-wJ5oJQdT"
      },
      "outputs": [],
      "source": [
        "mesonet = EEGMesoNet(5, 15, 0.6)\n",
        "# mesonet.load_state_dict(torch.load('../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v1_best.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R2ZWFLZrNYGt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ZWFLZrNYGt",
        "outputId": "4e21f8a1-2a3c-4e00-9ad3-161fde2c8808"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EEGMesoNet(\n",
              "  (branch_fine): ConvBranch(\n",
              "    (conv): Conv2d(5, 24, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "  )\n",
              "  (branch_medium): ConvBranch(\n",
              "    (conv): Conv2d(5, 16, kernel_size=(16, 16), stride=(4, 4), padding=(4, 4), bias=False)\n",
              "  )\n",
              "  (branch_coarse): ConvBranch(\n",
              "    (conv): Conv2d(5, 8, kernel_size=(64, 64), stride=(16, 16), padding=(8, 8), bias=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=48, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Dropout(p=0.6, inplace=False)\n",
              "    (7): Linear(in_features=64, out_features=15, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mesonet.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmFcO0wy3-_L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmFcO0wy3-_L",
        "outputId": "94d7bb08-129c-4bf4-f16d-60e76e37b0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= Model Summary ==============================================\n",
            "\n",
            "\n",
            "=======================================================\n",
            "| Parameter Name               || Number of Parameters|\n",
            "=======================================================\n",
            "| branch_fine.conv.weight       |                1920 |\n",
            "-------------------------------------------------------\n",
            "| branch_medium.conv.weight     |               20480 |\n",
            "-------------------------------------------------------\n",
            "| branch_coarse.conv.weight     |              163840 |\n",
            "-------------------------------------------------------\n",
            "| fc.0.weight                   |                6144 |\n",
            "-------------------------------------------------------\n",
            "| fc.0.bias                     |                 128 |\n",
            "-------------------------------------------------------\n",
            "| fc.1.weight                   |                 128 |\n",
            "-------------------------------------------------------\n",
            "| fc.1.bias                     |                 128 |\n",
            "-------------------------------------------------------\n",
            "| fc.3.weight                   |                8192 |\n",
            "-------------------------------------------------------\n",
            "| fc.3.bias                     |                  64 |\n",
            "-------------------------------------------------------\n",
            "| fc.4.weight                   |                  64 |\n",
            "-------------------------------------------------------\n",
            "| fc.4.bias                     |                  64 |\n",
            "-------------------------------------------------------\n",
            "| fc.7.weight                   |                 960 |\n",
            "-------------------------------------------------------\n",
            "| fc.7.bias                     |                  15 |\n",
            "-------------------------------------------------------\n",
            "\n",
            "Total Parameters: 202,127\n"
          ]
        }
      ],
      "source": [
        "model_summary(mesonet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "miusOtIue83c",
      "metadata": {
        "id": "miusOtIue83c"
      },
      "outputs": [],
      "source": [
        "# Do not forget to load the best meta learner saved on drive last night !!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b77474-4ff5-4ad0-8f17-2cb024182b02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1b77474-4ff5-4ad0-8f17-2cb024182b02",
        "outputId": "90fa9672-e1b7-48ff-d84c-45c4f00ae9ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.62s/it, loss=2.7]\n",
            "Epoch1/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it, loss=3.91]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss improved -> model saved to ../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v2_best.pth\n",
            "Epoch 1/200:\n",
            "Train Loss: 3.038 | Train Acc: 9.52 %\n",
            "Val Loss: 3.615 | Val Acc: 5.05 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.61s/it, loss=2.52]\n",
            "Epoch2/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.39s/it, loss=2.31]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss improved -> model saved to ../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v2_best.pth\n",
            "Epoch 2/200:\n",
            "Train Loss: 2.637 | Train Acc: 10.66 %\n",
            "Val Loss: 2.340 | Val Acc: 13.54 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.62s/it, loss=2.58]\n",
            "Epoch3/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it, loss=2.34]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 1/10\n",
            "Epoch 3/200:\n",
            "Train Loss: 2.516 | Train Acc: 10.79 %\n",
            "Val Loss: 2.342 | Val Acc: 10.89 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.61s/it, loss=2.47]\n",
            "Epoch4/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.38s/it, loss=2.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss improved -> model saved to ../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v2_best.pth\n",
            "Epoch 4/200:\n",
            "Train Loss: 2.464 | Train Acc: 10.80 %\n",
            "Val Loss: 2.326 | Val Acc: 10.35 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/200 [Train Pass]:  46%|████▌     | 34/74 [00:55<00:51,  1.28s/it, loss=2.42]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 5/200 [Train Pass]:  50%|█████     | 37/74 [01:05<01:24,  2.28s/it, loss=2.4]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f48691ad260>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 5/200 [Train Pass]: 100%|██████████| 74/74 [02:06<00:00,  1.71s/it, loss=2.47]\n",
            "Epoch5/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it, loss=2.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss improved -> model saved to ../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v2_best.pth\n",
            "Epoch 5/200:\n",
            "Train Loss: 2.455 | Train Acc: 10.91 %\n",
            "Val Loss: 2.303 | Val Acc: 10.75 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.62s/it, loss=2.37]\n",
            "Epoch6/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.39s/it, loss=2.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 1/10\n",
            "Epoch 6/200:\n",
            "Train Loss: 2.419 | Train Acc: 11.01 %\n",
            "Val Loss: 2.440 | Val Acc: 4.26 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/200 [Train Pass]: 100%|██████████| 74/74 [02:01<00:00,  1.64s/it, loss=2.4]\n",
            "Epoch7/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it, loss=2.32]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 2/10\n",
            "Epoch 7/200:\n",
            "Train Loss: 2.382 | Train Acc: 11.69 %\n",
            "Val Loss: 2.323 | Val Acc: 9.63 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/200 [Train Pass]: 100%|██████████| 74/74 [02:00<00:00,  1.63s/it, loss=2.37]\n",
            "Epoch8/200 [Val Pass]: 100%|██████████| 32/32 [00:45<00:00,  1.41s/it, loss=2.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss improved -> model saved to ../content/drive/MyDrive/NeuroVision/models/mesonet-meta-learner_v2_best.pth\n",
            "Epoch 8/200:\n",
            "Train Loss: 2.355 | Train Acc: 12.19 %\n",
            "Val Loss: 2.297 | Val Acc: 9.28 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/200 [Train Pass]: 100%|██████████| 74/74 [02:00<00:00,  1.62s/it, loss=2.39]\n",
            "Epoch9/200 [Val Pass]: 100%|██████████| 32/32 [00:45<00:00,  1.41s/it, loss=2.32]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 1/10\n",
            "Epoch 9/200:\n",
            "Train Loss: 2.351 | Train Acc: 12.19 %\n",
            "Val Loss: 2.331 | Val Acc: 6.76 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/200 [Train Pass]: 100%|██████████| 74/74 [01:58<00:00,  1.60s/it, loss=2.41]\n",
            "Epoch10/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.35s/it, loss=2.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 2/10\n",
            "Epoch 10/200:\n",
            "Train Loss: 2.342 | Train Acc: 12.63 %\n",
            "Val Loss: 2.299 | Val Acc: 9.75 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/200 [Train Pass]: 100%|██████████| 74/74 [01:57<00:00,  1.59s/it, loss=2.27]\n",
            "Epoch11/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it, loss=2.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 3/10\n",
            "Epoch 11/200:\n",
            "Train Loss: 2.326 | Train Acc: 12.73 %\n",
            "Val Loss: 2.460 | Val Acc: 5.08 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/200 [Train Pass]: 100%|██████████| 74/74 [01:58<00:00,  1.60s/it, loss=2.24]\n",
            "Epoch12/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it, loss=2.94]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 4/10\n",
            "Epoch 12/200:\n",
            "Train Loss: 2.317 | Train Acc: 13.04 %\n",
            "Val Loss: 2.862 | Val Acc: 1.36 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.61s/it, loss=2.36]\n",
            "Epoch13/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.38s/it, loss=3.57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 5/10\n",
            "Epoch 13/200:\n",
            "Train Loss: 2.298 | Train Acc: 13.82 %\n",
            "Val Loss: 3.626 | Val Acc: 1.26 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/200 [Train Pass]: 100%|██████████| 74/74 [02:00<00:00,  1.63s/it, loss=2.22]\n",
            "Epoch14/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it, loss=2.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 6/10\n",
            "Epoch 14/200:\n",
            "Train Loss: 2.281 | Train Acc: 14.90 %\n",
            "Val Loss: 2.844 | Val Acc: 1.51 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/200 [Train Pass]: 100%|██████████| 74/74 [01:58<00:00,  1.60s/it, loss=2.27]\n",
            "Epoch15/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it, loss=2.28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 7/10\n",
            "Epoch 15/200:\n",
            "Train Loss: 2.278 | Train Acc: 14.66 %\n",
            "Val Loss: 2.300 | Val Acc: 9.16 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/200 [Train Pass]: 100%|██████████| 74/74 [01:58<00:00,  1.60s/it, loss=2.34]\n",
            "Epoch16/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.38s/it, loss=2.25]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 8/10\n",
            "Epoch 16/200:\n",
            "Train Loss: 2.279 | Train Acc: 14.98 %\n",
            "Val Loss: 2.337 | Val Acc: 9.21 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.61s/it, loss=2.24]\n",
            "Epoch17/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.35s/it, loss=2.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 9/10\n",
            "Epoch 17/200:\n",
            "Train Loss: 2.261 | Train Acc: 15.46 %\n",
            "Val Loss: 2.682 | Val Acc: 2.15 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/200 [Train Pass]: 100%|██████████| 74/74 [01:58<00:00,  1.61s/it, loss=2.31]\n",
            "Epoch18/200 [Val Pass]: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it, loss=2.52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement in Val Loss. Counter: 10/10\n",
            "Epoch 18/200:\n",
            "Train Loss: 2.243 | Train Acc: 16.78 %\n",
            "Val Loss: 2.517 | Val Acc: 5.03 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/200 [Train Pass]: 100%|██████████| 74/74 [01:59<00:00,  1.61s/it, loss=2.18]\n",
            "Epoch19/200 [Val Pass]: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it, loss=3.92]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early Stopping triggered!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_model(mesonet, 'mesonet-meta-learner', train_loader, val_loader, 200, 5e-3, device)\n",
        "# train_model(resnet18, 'resnet-18', train_loader, val_loader, 128, 200, 1e-3, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc284f5f-0ff3-4928-9c02-e89f81e00640",
      "metadata": {
        "id": "cc284f5f-0ff3-4928-9c02-e89f81e00640"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usN-ZMu72tYZ",
      "metadata": {
        "id": "usN-ZMu72tYZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1NTPqRswu6E_",
        "SYQ-8v8RH6VB"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
